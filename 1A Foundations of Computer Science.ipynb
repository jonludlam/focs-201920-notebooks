{"metadata":{"kernelspec":{"display_name":"OCaml 4.07.1","language":"OCaml","name":"ocaml-jupyter"},"language_info":{"name":"OCaml","version":"4.07.1","codemirror_mode":"text/x-ocaml","file_extension":".ml","mimetype":"text/x-ocaml","nbconverter_exporter":null,"pygments_lexer":"OCaml"}},"nbformat":4,"nbformat_minor":2,"cells":[{"cell_type":"markdown","metadata":{},"source":"\nThis course has two aims. The first is to teach programming. The second is to\npresent some fundamental principles of computer science, especially algorithm\ndesign. Most students will have some programming experience already, but there\nare few people whose programming cannot be improved through greater knowledge\nof basic principles. Please bear this point in mind if you have extensive\nexperience and find parts of the course rather slow.\n\nThe programming in this course is based on the language [OCaml](https://ocaml.org)\nand mostly concerns the functional programming style. Functional programs tend\nto be shorter and easier to understand than their counterparts in conventional\nlanguages such as C. In the space of a few weeks, we shall cover many\nfundamental data structures and learn basic methods for estimating efficiency.\n\n**this is a work-in-progress port of Lawrence C. Paulson's 1819 Cambridge\ncourse notes**\n"},{"cell_type":"markdown","metadata":{},"source":"\n- Computers: a child can use them; **nobody** can fully understand them!\n- We can master complexity through levels of abstraction.\n- Focus on 2 or 3 levels at most!\n\n**Recurring issues:**\n- what services to provide at each level\n- how to implement them using lower-level services\n- the interface: how the two levels should communicate\n\nA basic concept in computer science is that large systems can only be\nunderstood in levels, with each level further subdivided into functions or\nservices of some sort. The interface to the higher level should supply the\nadvertised services. Just as important, it should block access to the means by\nwhich those services are implemented. This _abstraction barrier_ allows one\nlevel to be changed without affecting levels above. For example, when a\nmanufacturer designs a faster version of a processor, it is essential that\nexisting programs continue to run on it. Any differences between the old and\nnew processors should be invisible to the program.\n\nModern processors have elaborate specifications, which still sometimes leave\nout important details. In the old days, you then had to consult the circuit\ndiagrams.\n"},{"cell_type":"markdown","metadata":{},"source":"\n- Abstract level: dates over a certain interval\n- Concrete level: typically 6 characters: `YYMMDD` (where each character is represented by 8 bits)\n- Date crises caused by __inadequate__ internal formats:\n  * Digital’s PDP-10: using 12-bit dates (good for at most 11 years)\n  * 2000 crisis: 48 bits could be good for lifetime of universe!\n\nDigital Equipment Corporation’s date crisis occurred in 1975.  The\nPDP-10 was a 36-bit mainframe computer. It represented dates using a 12-bit\nformat designed for the tiny PDP-8. With 12 bits, one can distinguish\n$2^{12} = 4096$ days or 11 years.\n\nThe most common industry format for dates uses six characters: two for the\nyear, two for the month and two for the day. The most common \"solution\" to the\nyear 2000 crisis is to add two further characters, thereby altering file sizes.\nOthers have noticed that the existing six characters consist of 48 bits,\nalready sufficient to represent all dates over the projected lifetime of the\nuniverse: $2^{48}$ = $2.8 * 1014$ days = $7.7 * 1011$ years!\n\nMathematicians think in terms of unbounded ranges, but the representation we\nchoose for the computer usually imposes hard limits. A good programming\nlanguage like OCaml lets one easily change the representation used in the\nprogram.  But if files in the old representation exist all over the place,\nthere will still be conversion problems. The need for compatibility with older\nsystems causes problems across the computer industry.\n"},{"cell_type":"markdown","metadata":{},"source":"\nComputers have integers like `1066` and floats like $1.066 x 10^3$.\nA floating-point number is represented by two integers.\nThe concept of _data type_ involves:\n* how a value is represented inside the computer\n* the suite of operations given to programmers\n* valid and invalid (or exceptional) results, such as “infinity”\nComputer arithmetic can yield _incorrect answers_!\n\nIn science, numbers written with finite precision and a decimal exponent are\nsaid to be in _standard form_. The computational equivalent is the _floating\npoint number_. These are familiar to anybody who has used a scientific\ncalculator.  Internally, a float consists of two integers.\n\nBecause of its finite precision, floating-point computations are potentially\ninaccurate. To see an example, use your nearest electronic calculator to\ncompute $(2^{1/10000})10000$. I get $1.99999959$! With certain computations,\nthe errors spiral out of control. Many programming languages fail to check\nwhether even integer computations fall within the allowed range: you can add\ntwo positive integers and get a negative one!\n\nMost computers give us a choice of precisions. In 32-bit precision, integers\ntypically range from $2^{31} − 1$ (namely $2,147,483,647$) to $−2^{31}$; reals\nare accurate to about six decimal places and can get as large as 1035 or so.\nFor reals, 64-bit precision is often preferred. Early languages like Fortran\nrequired variables to be declared as `INTEGER`, `REAL` or `COMPLEX` and barred\nprogrammers from mixing numbers in a computation. Nowadays, programs handle\nmany different kinds of data, including text and symbols. The concept of a\n_data type_ can ensure that different types of data are not combined in a\nsenseless way.\n\nInside the computer, all data are stored as bits. In most programming\nlanguages, the compiler uses types to generate correct machine code, and types\nare not stored during program execution. In this course, we focus almost\nentirely on programming in a high-level language: OCaml.\n"},{"cell_type":"markdown","metadata":{},"source":"\n- to describe a computation so that it can be done _mechanically_:\n  * Expressions compute values.\n  * Commands cause effects.\n- to do so efficiently and **correctly**, giving the right answers quickly\n- to allow easy modification as needs change\n  * Through an orderly _structure_ based on abstraction principles\n  * Such as modules or (Java) classes\n\nProgramming _in-the-small_ concerns the writing of code to do simple, clearly\ndefined tasks. Programs provide expressions for describing mathematical\nformulae and so forth. (This was the original contribution of FORTRAN, the\nFORmula TRANslator.) Commands describe how control should flow from one part of\nthe program to the next.\n\nAs we code layer upon layer, we eventually find ourselves programming\n_in the large_ : joining large modules to solve some messy task. Programming\nlanguages have used various mechanisms to allow one part of the program to\nprovide interfaces to other parts. Modules encapsulate a body of code, allowing\noutside access only through a programmer-defined interface. _Abstract Data\nTypes_ are a simpler version of this concept, which implement a single concept\nsuch as dates or floating-point numbers.\n\n_Object-oriented programming_ is the most complicated approach to modularity.\n_Classes_ define concepts, and they can be built upon other classes. Operations\ncan be defined that work in appropriately specialized ways on a family of\nrelated classes. _Objects_ are instances of classes and hold the data that is\nbeing manipulated.\n\nThis course does not cover OCaml's sophisticated module system, which can do\nmany of the same things as classes. You will learn all about objects when you\nstudy Java.\n"},{"cell_type":"markdown","metadata":{},"source":"\n* Why Program in ML?\n* It is interactive.\n* It has a flexible notion of _data type_.\n* It hides the underlying hardware: _no crashes_.\n* Programs can easily be understood mathematically.\n* It distinguishes naming something from _updating memory_.\n* It manages storage for us.\n\nStandard ML is the outcome of years of research into\nprogramming languages. It is unique, defined using a mathematical formalism (an\noperational semantics) that is both precise and comprehensible. Several\nsupported compilers are available, and thanks to the formal definition, there\nare remarkably few incompatibilities among them. _(TODO edit)_\n\nBecause of its connection to mathematics, ML programs can be designed and\nunderstood without thinking in detail about how the computer will run them.\nAlthough a program can abort, it cannot crash: it remains under the control of\nthe OCaml system. It still achieves respectable efficiency and provides\nlower-level primitives for those who need them. Most other languages allow\ndirect access to the underlying machine and even try to execute illegal\noperations, causing crashes.\n\nThe only way to learn programming is by writing and running programs. This web\nnotebook provides an interactive environment where you can modify the example\nfragments and see the results for yourself.  You should also consider\ninstalling OCaml on your own computer so that you try more advanced programs\nlocally.\n"},{"cell_type":"code","metadata":{},"source":"let pi = 3.14159265358979","outputs":[],"execution_count":1},{"cell_type":"markdown","metadata":{},"source":"\nThe first line of this simple session is a _value declaration_. It makes the\nname `pi` stand for the floating point number `3.14159`. (Such names are called\n_identifiers_.)  OCaml echoes the name (`pi`) and type (`float`) of the\ndeclared identifier.\n"},{"cell_type":"code","metadata":{},"source":"pi *. 1.5 *. 1.5","outputs":[],"execution_count":2},{"cell_type":"markdown","metadata":{},"source":"\nThe second line computes the area of the circle with radius `1.5` using the\nformula $A = \\pi r^2$. We use `pi` as an abbreviation for `3.14159`.\nMultiplication is expressed using `*.`, which is called an _infix operator_\nbecause it is written between its two operands.\n\nOCaml replies with the computed value (about `7.07`) and its type (again `float`).\n"},{"cell_type":"code","metadata":{},"source":"let area r = pi *. r *. r","outputs":[],"execution_count":3},{"cell_type":"markdown","metadata":{},"source":"\nTo work abstractly, we should provide the service \"compute the area of a\ncircle,\" so that we no longer need to remember the formula. This sort of\nencapsulated computation is called a _function_. The third line declares the\nfunction `area`. Given any floating point number `r`, it returns another\nfloating point number computed using the `area` formula; note that the function\nhas type `float -> float`.\n"},{"cell_type":"code","metadata":{},"source":"area 2.0","outputs":[],"execution_count":4},{"cell_type":"markdown","metadata":{},"source":"\nThe fourth line calls the function `area` supplying `2.0` as the argument. A\ncircle of radius `2` has an area of about `12.6`. Note that brackets around a\nfunction argument are not necessary.\n\nThe function uses `pi` to stand for `3.14159`. Unlike what you may have seen in\nother programming languages, `pi` cannot be \"assigned to\" or otherwise updated.\nIts meaning within `area` will persist even if we issue a new `let` declaration\nfor `pi` afterwards.\n"},{"cell_type":"code","metadata":{},"source":"let rec npower x n =\n  if n = 0 then 1.0\n  else x *. npower x (n-1)","outputs":[],"execution_count":5},{"cell_type":"markdown","metadata":{},"source":"\nThe function `npower` raises its real argument `x` to the power `n`, a\nnon-negative integer. The function is _recursive_: it calls itself. This concept\nshould be familiar from mathematics, since exponentiation is defined by the\nrules shown above. You may also have seen recursion in the product rule for\ndifferentiation: $(u · v)′ = u · v′ + u′ · v.$.\n\nIn finding the derivative of $u.v$, we recursively find the derivatives of $u$\nand $v$, combining them to obtain the desired result. The recursion is\nmeaningful because it terminates: we reduce the problem to two smaller\nproblems, and this cannot go on forever. The ML programmer uses recursion\nheavily.  For $n>=0$, the equation $x^{n+1} = x * x^n$ yields an obvious\ncomputation:\n\n$$ x^3 = x\\times x^2 = x\\times x\\times x^1 = x\\times x\\times x\\times x^0 = x\\times x\\times x $$\n\nThe equation clearly holds even for negative $n$. However, the corresponding\ncomputation runs forever:\n\n$$ x^{-1} = x\\times x^{-2} = x\\times x\\times x^{-3}=\\cdots $$\n\nNote that the function `npower` contains both an integer constant (0) and a\nfloating point constant (1.0). The decimal point makes all the difference. The\nML system will notice and ascribe different meaning to each type of constant.\n"},{"cell_type":"code","metadata":{},"source":"let square x = x *. x;","outputs":[],"execution_count":6},{"cell_type":"markdown","metadata":{},"source":"\nNow for a tiresome but necessary aside. In most languages, the types of\narguments and results must always be specified. ML is unusual that it normally\ninfers the types itself. However, sometimes ML could use a hint; function\n`square` above has a type constraint to say its result is a float.\n\nML can still infer the type even if you don't specify them, but in some cases\nit will use a more inefficient function than a specialised one.  Some languages\nhave just one type of number, converting automatically between different\nformats; this is slow and could lead to unexpected rounding errors.  Type\nconstraints are allowed almost anywhere. We can put one on any occurrence of x\nin the function. We can constrain the function’s result:\n"},{"cell_type":"code","metadata":{},"source":"let square (x:float) = x *. x","outputs":[],"execution_count":7},{"cell_type":"code","metadata":{},"source":"let square x : float = x *. x","outputs":[],"execution_count":8},{"cell_type":"markdown","metadata":{},"source":"\nML treats the equality and comparison test specially. Expressions like `if x = y then ...`\nare fine provided `x` and `y` have the same type and equality testing is\npossible for that type. (We discuss equality further in a later lecture.)\nNote that `x <> y` is ML for `x  ̸= y`.\n\nA characteristic feature of the computer is its ability to test for conditions\nand act accordingly.  In the early days, a program might jump to a given\naddress depending on the sign of some number.  Later, John McCarthy defined\nthe _conditional expression_ to satisfy:\n\n$$if true then x else y = x$$\n$$if false then x else y = y$$\n\nML evaluates the expression $if B then E_1 else E_2$ by first evaluating $B$.\nIf the result is `true` then ML evaluates $E_1$ and otherwise $E_2$.  Only one\nof the two expressions $E_1$ and $E_2$ is evaluated!  If both were evaluated,\nthen recursive functions like `npower` above would run forever.\n\nThe _if-expression_ is governed by an expression of type `bool`, whose two\nvalues are `true` and `false`.  In modern programming languages, tests are not\nbuilt into \"conditional branch\" constructs but have an independent status.\nTests, or _Boolean expressions_, can be expressed using relational operators\nsuch as `<` and `=`. They can be combined using the Boolean operators for\nnegation (`not`), `and` (written as `&&`) and `or` (written as `||`).  New\nproperties can be declared as functions: here, to test whether an integer is\neven.\n\nFor large `n`, computing powers using $x^{n+1} = x\\times x^n$ is too slow to\nbe practical.  The equations above are much faster. Example:\n\n$$ 2^{12} = 4^6 = 16^3 = 16\\times 256^1 = 16\\times 256 = 4096. $$\n\nInstead of `n` multiplications, we need at most $2 lg n$ multiplications,\nwhere $lg n$ is the logarithm of $n$ to the base $2$.\n\nWe use the function `even`, declared previously, to test whether the\nexponent is even.  Integer division (`div`) truncates its result to an\ninteger: dividing $2n+1$ by 2 yields $n$.\n\nA recurrence is a useful computation rule only if it is bound to terminate.\nIf $n>0$ then $n$ is smaller than both $2n$ and $2n+1$.  After enough\nrecursive calls, the exponent will be reduced to $1$.  The equations also hold\nif $n\\leq0$, but the corresponding computation runs forever.\n\nOur reasoning assumes arithmetic to be _exact_. Fortunately, the calculation is\nwell-behaved using floating-point.\n\nTODO edit for OCaml. The negation of `x` is written `~x` rather than `-x`\nplease note.  Most languages use the same symbol for minus and subtraction,\nbut ML regards all operators, whether infix or not, as functions.  Subtraction\ntakes a pair of numbers, but minus takes a single number; they are distinct\nfunctions and must have distinct names.\n\nTODO edit for OCaml. Computer numbers have a finite range, which if exceeded gives rise to an\nOverflow error.  Some ML systems can represent integers of arbitrary size.\n\nIf integers and reals must be combined in a calculation, ML provides functions\nto convert between them:\n"},{"cell_type":"code","metadata":{},"source":"int_of_float ;;","outputs":[],"execution_count":9},{"cell_type":"code","metadata":{},"source":"int_of_float 3.14159 ;;","outputs":[],"execution_count":10},{"cell_type":"code","metadata":{},"source":"float_of_int ;;","outputs":[],"execution_count":11},{"cell_type":"code","metadata":{},"source":"float_of_int 3 ;;","outputs":[],"execution_count":12},{"cell_type":"markdown","metadata":{},"source":"\nML's libraries are organized using _modules_, so we many use compound\nidentifiers such as `Float.of_int` to refer to library functions.  In OCaml,\nlibrary units can also be loaded by commands such as `#require \"num\"`.  There\nare many thousands of library functions available in the OCaml ecosystem,\nincluding text-processing and operating systems functions in addition to the\nusual numerical ones.\n\nTODO summarise OCaml syntax.\n\n"},{"cell_type":"markdown","metadata":{},"source":"\nExpression evaluation concerns expressions and the values they return. This\nview of computation may seem to be too narrow. It is certainly far removed from\ncomputer hardware, but that can be seen as an advantage. For the traditional\nconcept of computing solutions to problems, expression evaluation is entirely\nadequate.\n\nStarting with $E_0$, the expression $E_i$ is reduced to $E_{i+1}$ until this\nprocess concludes with a value~$v$.  A _value_ is something like a number\nthat cannot be further reduced.\n\nWe write $E E'$ to say that $E$ is _reduced_ to $E'$.\nMathematically, they are equal: $E=E'$, but the computation goes from $E$ to\n$E'$ and never the other way around.\n\nComputers also interact with the outside world.  For a start, they need some\nmeans of accepting problems and delivering solutions.  Many computer systems\nmonitor and control industrial processes.  This role of computers is familiar\nnow, but was never envisaged in the early days. Computer pioneers focused on\nmathematical calculations.  Modelling interaction and control requires a notion\nof _states_ that can be observed and changed.  Then we can consider\nupdating the state by assigning to variables or performing input/output,\nfinally arriving at conventional programs as coded in C, for instance.\n\nFor now, we remain at the level of expressions, which is usually termed\n_functional programming_.\n\n"},{"cell_type":"code","metadata":{},"source":"let rec nsum n =\n  if n = 0 then 0\n           else n + nsum (n-1)","outputs":[],"execution_count":13},{"cell_type":"markdown","metadata":{},"source":"\nThe function call `nsum n` computes the sum `1+...+nz` rather naively, hence the\ninitial `n` in its name.  The nesting of parentheses is not just an artifact of\nour notation; it indicates a real problem.  The function gathers up a\ncollection of numbers, but none of the additions can be performed until `nsum\n0` is reached.  Meanwhile, the computer must store the numbers in an internal\ndata structure, typically the _stack_.  For large `n`, say `nsum 10000`, the\ncomputation might fail due to stack overflow.\n\nWe all know that the additions can be performed as we go along.  How do we\nmake the computer do that?\n"},{"cell_type":"code","metadata":{},"source":"let rec summing n total =\n  if n = 0 then total\n           else summing (n-1) (n + total)","outputs":[],"execution_count":14},{"cell_type":"markdown","metadata":{},"source":"\nFunction `summing` takes an additional argument: a running total.  If\n`n` is zero then it returns the running total; otherwise, `summing`\nadds to it and continues.  The recursive calls do not nest; the additions are\ndone immediately.\n\nA recursive function whose computation does not nest is called\n_iterative_ or _tail-recursive_. Many functions can be made iterative by\nintroducing an argument analogous to _total_, which is often called an\n_accumulator_.\n\nThe gain in efficiency is sometimes worthwhile and sometimes not.  The function\n`power` is not iterative because nesting occurs whenever the exponent is odd.\nAdding a third argument makes it iterative, but the change complicates the\nfunction and the gain in efficiency is minute; for 32-bit integers, the maximum\npossible nesting is 30 for the exponent $2^{31}-1$.\n\n\nTODO slide\n\nA [classic\nbook](https://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs)\nby Abelson and Sussman used _iterative_ to mean _tail-recursive_. It describes\nthe Lisp dialect known as Scheme. Iterative functions produce computations\nresembling those that can be done using while-loops in conventional languages.\n\nMany algorithms can be expressed naturally using recursion, but only awkwardly\nusing iteration. There is a story that Dijkstra sneaked recursion into Algol-60\nby inserting the words \"any other occurrence of the procedure name denotes\nexecution of the procedure\". By not using the word \"recursion\", he managed to\nslip this amendment past sceptical colleagues.\n\nObsession with tail recursion leads to a coding style in which functions\nhave many more arguments than necessary.  Write straightforward code first,\navoiding only gross inefficiency.  If the program turns out to be too slow,\ntools are available for pinpointing the cause.  Always remember KISS (Keep\nIt Simple, Stupid).\n\nI hope you have all noticed by now that the summation can be done even more\nefficiently using the arithmetic progression formula:\n\n$$ 1+\\cdots+n = n(n+1)/2 $$\n"},{"cell_type":"code","metadata":{},"source":"let rec stupidSum n =\n  if n = 0 then 0\n           else n + (stupidSum (n-1) + stupidSum (n-1)) / 2","outputs":[],"execution_count":15},{"cell_type":"markdown","metadata":{},"source":"\nThe function calls itself $2^n$ times!  Bigger inputs mean higher costs---but\nwhat's the _growth rate_?\n\nNow let us consider how to estimate various costs associated with a program.\n_Asymptotic complexity_ refers to how costs---usually time or space---grow with\nincreasing inputs. Space complexity can never exceed time complexity, for it\ntakes time to do anything with the space.  Time complexity often greatly\nexceeds space complexity.\n\nThe function `stupidSum` calls itself twice in each recursive step.  This\nfunction is contrived, but many mathematical formulas refer to a particular\nquantity more than once.  In OCaml, we can create a local binding to a computed\nvalue using the _local declaration_ syntax:\n\nTODO make power a real function\n"},{"cell_type":"markdown","metadata":{},"source":"\nFast hardware does not make good algorithms unnecessary.  On the contrary,\nfaster hardware magnifies the superiority of better algorithms.  Typically, we\nwant to handle the largest inputs possible.  If we double our processing power,\nwhat do we gain?  How much can we increase $n$, the input to our function?\nWith `stupidSum`, we can only go from $n$ to $n+1$.  We are limited to this\nmodest increase because the function's running time is proportional to $2^n$.\nWith the function `npower` defined in the previous lecture, we can go from $n$\nto $2n$: we can handle problems twice as big.  With `power` we can do much\nbetter still, going from $n$ to $n^2$.\n\nTODO table\n\nThis table (excerpted from a 40-year-old book! TODO cite aho74) illustrates the\neffect of various time complexities.  The left-hand column indicates how many\nmilliseconds are required to process an input of size $n$.  The other entries\nshow the maximum size of $n$ that can be processed in the given time (one\nsecond, minute or hour).\n\nThe table illustrates how large an input can be processed as a function\nof time.  As we increase the computer time per input from one second to one\nminute and then to one hour, the size of the input increases accordingly.\n\nThe top two rows (complexities $n$ and $n \\lg n$) increase rapidly: for $n$, by\na factor of 60.  The bottom two start out close together, but $n^3$ (which\ngrows by a factor of 3.9) pulls well away from $2^n$ (whose growth is only\nadditive).  If an algorithm's complexity is exponential then it can never\nhandle large inputs, even if it is given huge resources.  On the other hand,\nsuppose the complexity has the form $n^c$, where $c$ is a constant.  (We say\nthe complexity is _polynomial_.)  Doubling the argument then increases the\ncost by a constant factor.  That is much better, though if $c>3$ the algorithm\nmay not be considered practical.\n\nThe cost of a program is usually a complicated formula.  Often we should\nconsider only the most significant term.  If the cost is $n^2 + 99n + 900$\nfor an input of size $n$, then the $n^2$ term will eventually dominate,\neven though $99n$ is bigger for $n<99$.\nThe constant term $900$ may look big, but it is soon dominated by $n^2$.\n\nConstant factors in costs can be ignored unless they are large.  For one thing,\nthey seldom make a difference: $100n^2$ will be better than $n^3$ in the long\nrun: or _asymptotically_ to use the jargon.  Moreover, constant factors are\nseldom stable.  They depend upon details such as which hardware, operating\nsystem or programming language is being used.  By ignoring constant factors, we\ncan make comparisons between algorithms that remain valid in a broad range of\ncircumstances.\n\nThe \"Big O\" notation is commonly used to describe efficiency---to be precise,\n_asymptotic complexity_.  It concerns the limit of a function as its\nargument tends to infinity.  It is an abstraction that meets the informal\ncriteria that we have just discussed.\nIn the definition, _sufficiently large_ means there is some constant $n_0$\nsuch that $|f(n)|\\leq c|g(n)|$ for all $n$ greater than $n_0$.  The\nrole of $n_0$ is to ignore finitely many exceptions to the bound, such as the\ncases when $99n$ exceeds~$n^2$.\n\nTODO onotation slide\n\n$O$ notation lets us reason about the costs of algorithms easily.\n- Constant factors such as the $2$ in $O(2g(n))$ drop out: we can use $O(g(n))$ with twice the value of~$c$ in the definition.\n- Because constant factors drop out, the base of logarithms is irrelevant.\n- Insignificant terms drop out.  To see that $O(n^2+50n+36)$ is the same as $O(n^2)$, consider that $n^2+50n+36/n^2$ converges to 1 for increasing $n$.  % In fact, $n^2+50n+36 \\le 2n^2$ for $n\\ge 51$, so can double the constant factor\n\nIf $c$ and $d$ are constants (that is, they are independent of $n$) with $0 < c < d$ then\n- $O(n^c)$ is contained in $O(n^d)$\n- $O(c^n)$ is contained in $O(d^n)$\n- $O(\\log n)$ is contained $in O(n^c)$\n\nTo say that $O(c^n)$ _is contained in_ $O(d^n)$ means that the former gives\na tighter bound than the latter.  For example, if $f(n)=O(2^n)$ then\n$f(n)=O(3^n)$ trivially, but the converse does not hold.\n"},{"cell_type":"markdown","metadata":{},"source":"\n- $O(1)$ is _constant_\n- $O(\\log n)$ is _logarithmic_\n- $O(n)$ is _linear_\n- $O(n\\log n)$ is _quasi-linear_\n- $O(n^2)$ is _quadratic_\n- $O(n^3)$ is _cubic_\n- $O(a^n)$ is _exponential_ (for fixed $a$)\n\nLogarithms grow very slowly, so $O(\\log n)$ complexity is excellent.  Because\n$O$ notation ignores constant factors, the base of the logarithm is\nirrelevant!\n\nUnder linear we might mention $O(n\\log n)$, which occasionally is called\n_quasilinear_ and which scales up well for large $n$.\n\nAn example of quadratic complexity is matrix addition: forming the sum of two\n$n\\times n$ matrices obviously takes $n^2$ additions.  Matrix\nmultiplication is of cubic complexity, which limits the size of matrices that\nwe can multiply in reasonable time.  An $O(n^{2.81})$ algorithm exists, but it\nis too complicated to be of much use, even though it is theoretically better.\n\nAn exponential growth rate such as $2^n$ restricts us to small values of~$n$.\nAlready with $n=20$ the cost exceeds one million.  However, the worst case\nmight not arise in normal practice.  OCaml type-checking is exponential in the\nworst case, but not for ordinary programs.\n\n### Sample costs in O notation\n\n| Function     | Time | Space |\n| ------------ | ---- | ----- |\n|  npower, nsum  | $O(n)$ | $O(n)$ |\n|  summing       | $O(n)$ | $O(1)$ |\n| $n(n+1)/2$ | $O(1)$ | $O(1)$ |\n|  power  | $O(\\log n)$ | $O(\\log n)$ |\n|  stupidSum  | $O(2^n)$ | $O(n)$ |\n\nRecall that `npower` computes $x^n$\nby repeated multiplication while `nsum` naively computes the sum\n$1+\\cdots+n$.  Each obviously performs $O(n)$ arithmetic operations.  Because\nthey are not tail recursive, their use of space is also $O(n)$.  The function\n`summing` is a version of `nsum` with an accumulating argument;\nits iterative behaviour lets it work in constant space.  $O$ notation spares\nus from having to specify the units used to measure space.\n\nEven ignoring constant factors, the units chosen can influence the result.\nMultiplication may be regarded as a single unit of cost.  However, the cost of\nmultiplying two $n$-digit numbers for large $n$ is itself an important\nquestion, especially now that public-key cryptography uses numbers hundreds of\ndigits long.\n\nFew things can _really_ be done in constant time or stored in constant\nspace.  Merely to store the number $n$ requires $O(\\log n)$ bits.  If a\nprogram cost is $O(1)$, then we have probably assumed that certain operations\nit performs are also $O(1)$---typically because we expect never to exceed the\ncapacity of the standard hardware arithmetic.\n\nWith `power`, the precise number of operations depends upon $n$ in a\ncomplicated way, depending on how many odd numbers arise, so it is convenient\nthat we can just write $O(\\log n)$.  An accumulating argument could reduce its\nspace cost to $O(1)$.\n\n### Some Simple Recurrence Relations\n\n\nConsider $T(n)$ has a cost we want to bound using $O$ notation.\nA typical _base case_ is $T(1)=1$.  Some _recurrences_ are:\n\n| Equation            | Complexity   |\n| ------------------- | ------------ |\n| $T(n+1) = T(n)+1$  | $O(n)$       |\n| $T(n+1) = T(n)+n$  | $O(n^2)$     |\n| $T(n) = T(n/2)+1$  | $O(\\log n)$  |\n| $T(n) = 2T(n/2)+n$ | $O(n\\log n)$ |\n\nTo analyse a function, inspect its OCaml declaration.  Recurrence equations for\nthe cost function $T(n)$ can usually be read off.  Since we ignore constant\nfactors, we can give the base case a cost of one unit.  Constant work done in\nthe recursive step can also be given unit cost; since we only need an upper\nbound, this unit represents the larger of the two actual costs.  We could use\nother constants if it simplifies the algebra.\n\nFor example, recall our function `nsum`:\n"},{"cell_type":"code","metadata":{},"source":"let rec nsum n =\n  if n = 0 then \n    0\n  else\n    n + nsum (n-1)","outputs":[],"execution_count":16},{"cell_type":"markdown","metadata":{},"source":"\nGiven $n+1$, it performs a constant amount of work (an addition and\nsubtraction) and calls itself recursively with argument $n$.  We get the\nrecurrence equations $T(0)=1$ and $T(n+1) = T(n)+1$.  The closed form is\nclearly $T(n)=n+1$, as we can easily verify by substitution.  The cost is\n_linear_.\n\nThis function, given $n+1$, calls `nsum`, performing $O(n)$ work.\nAgain ignoring constant factors, we can say that this call takes exactly $n$\nunits.\n"},{"cell_type":"code","metadata":{},"source":"let rec nsumsum n =\n  if n = 0 then\n    0\n  else\n    nsum n + nsumsum (n-1)","outputs":[],"execution_count":17},{"cell_type":"markdown","metadata":{},"source":"\nWe get the recurrence equations $T(0)=1$ and $T(n+1) = T(n)+n$.  It is easy to\nsee that $T(n)=(n-1)+\\cdots+1=n(n-1)/2=O(n^2)$.  The cost is\n_quadratic_.\n\nThe function `power` divides its input $n$ into two, with\nthe recurrence equation $T(n) = T(n/2)+1$.  Clearly $T(2^n)=n+1$, so\n$T(n)=O(\\log n)$.\n"},{"cell_type":"code","metadata":{},"source":"let x = [3; 5; 9] ;;","outputs":[],"execution_count":18},{"cell_type":"code","metadata":{},"source":"let y = [ (1,\"one\"); (2,\"two\") ] ;;","outputs":[],"execution_count":19},{"cell_type":"markdown","metadata":{},"source":"\nA _list_ is an ordered series of elements; repetitions are significant.\nSo `[3;5;9]` differs from `[5;3;9]` and from `[3;3;5;9]`.  Elements in the\nlist are separated with `;` when constructed, as opposed to the `,` syntax\nused for fixed-length tuples.\n\nAll elements of a list must have the same type.  Above we see a list of\nintegers and a list of `(integer, string)` pairs.  One can also have lists of\nlists, such as `[[3]; []; [5,6]]`, which has type `int list list`.\n\nIn the general case, if $x_1$, \\ldots, $x_n$ all have the same type (say\n$\\tau$) then the list $[x_1,\\ldots,x_n]$ has type $(\\tau)\\texttt{list}$.\n\nLists are the simplest data structure that can be used to process collections\nof items.  Conventional languages use _arrays_, whose elements are\naccessed using subscripting: for example, $A[i]$ yields the $i$th element of\nthe array~$A$.  Subscripting errors are a known cause of programmer grief,\nhowever, so arrays should be replaced by higher-level data structures whenever\npossible.\n"},{"cell_type":"code","metadata":{},"source":"x @ [2; 10] ;;","outputs":[],"execution_count":20},{"cell_type":"code","metadata":{},"source":"List.rev [ (1,\"one\"); (2,\"two\") ] ;;","outputs":[],"execution_count":21},{"cell_type":"markdown","metadata":{},"source":"\nThe infix operator `@` (also called `List.append`) concatenates two lists.\nAlso built-in is `List.rev`, which reverses a list.  These are demonstrated\nin the session above.\n"},{"cell_type":"markdown","metadata":{},"source":"\nThere are two kinds of lists:\n- `[]` represents the empty list\n- `x :: l` is the list with head $x$ and tail $l$\n"},{"cell_type":"code","metadata":{},"source":"let nil = [] ;;","outputs":[],"execution_count":22},{"cell_type":"code","metadata":{},"source":"1 :: nil ;;","outputs":[],"execution_count":23},{"cell_type":"code","metadata":{},"source":"1 :: 2 :: nil ;;","outputs":[],"execution_count":24},{"cell_type":"markdown","metadata":{},"source":"\nThe operator `::` (also called `List.cons` for \"construct\"), puts a new element on\nto the head of an existing list.  While we should not be too preoccupied with\nimplementation details, it is essential to know that `::` is an $O(1)$\noperation.  It uses constant time and space, regardless of the length of the\nresulting list.  Lists are represented internally with a linked structure;\nadding a new element to a list merely hooks the new element to the front of\nthe existing structure.  Moreover, that structure continues to denote the same\nlist as it did before; to see the new list, one must look at the new `::` node\n(or _cons cell_) just created.\n\n\nHere we see the element~1 being consed to the front of the list `[3;5;9]`:\n\n$$\n\\let\\down=\\downarrow\n\\begin{array}{*{10}{c@{\\,}}c}\n:: & \\to & \\cdots & :: & \\to &  :: & \\to &  :: & \\to & nil \\\\\n\\down &  &        & \\down &  & \\down &  & \\down  \\\\\n1     &  &        & 3     &  & 5     &  & 9\n\\end{array}\n$$\n\nGiven a list, taking its first element (its _head_) or its list of\nremaining elements (its _tail_) also takes constant time.  Each\noperation just follows a link.  In the diagram above, the first down arrow\nleads to the head and the leftmost right arrow leads to the tail.  Once we\nhave the tail, its head is the second element of the original list, etc.\n\nThe tail is _not_ the last element; it is the _list_ of all elements\nother than the head!\n"},{"cell_type":"code","metadata":{},"source":"let null = function\n  | [] -> true\n  | x :: l -> false ;;","outputs":[],"execution_count":25},{"cell_type":"code","metadata":{},"source":"null [] ;;","outputs":[],"execution_count":26},{"cell_type":"code","metadata":{},"source":"null [1;2;3] ;;","outputs":[],"execution_count":27},{"cell_type":"code","metadata":{},"source":"let hd (x::l) = x ;;","outputs":[],"execution_count":28},{"cell_type":"code","metadata":{},"source":"hd [1;2;3] ;;","outputs":[],"execution_count":29},{"cell_type":"code","metadata":{},"source":"let tl (x::l) = l ;;","outputs":[],"execution_count":30},{"cell_type":"code","metadata":{},"source":"tl [7;6;5] ;;","outputs":[],"execution_count":31},{"cell_type":"markdown","metadata":{},"source":"\nThe empty list has neither head nor tail.  Applying `List.hd` `List.tl` to `[]`\nis an error---strictly speaking, an _exception_.  The function `null` can\nbe used to check for the empty list beforehand.  Taking a list apart using\ncombinations of `hd` and `tl` is hard to get right.  Fortunately, it is seldom\nnecessary because of _pattern-matching_.\n\nThe declaration of `null`} above has two clauses: one for the empty list (for\nwhich it returns `true`) and one for non-empty lists (for which it returns\n`false`).\n\nThe declaration of `null` above has two clauses: one for the empty list\n(for which it returns `true`) and one for non-empty lists (for which it\nreturns `false`).\n\nThe declaration of `hd` above has only one clause, for non-empty lists.  They\nhave the form `x::l` and the function returns `x`, which is the head.  OCaml\nprints a warning to tell us that calling the function could raise an exception\ndue to all possible inputs not being handles, including a counter-example (in\nthis case, the empty list `[]`). The declaration of `tl` is similar to `hd`.\n\nThese three primitive functions are _polymorphic_ and allow flexibility in the\ntypes of their arguments and results. Note their types!\n"},{"cell_type":"code","metadata":{},"source":"null ;;","outputs":[],"execution_count":32},{"cell_type":"code","metadata":{},"source":"hd ;;","outputs":[],"execution_count":33},{"cell_type":"code","metadata":{},"source":"tl ;;","outputs":[],"execution_count":34},{"cell_type":"markdown","metadata":{},"source":"\nSymbols `'a` and `'b` are called _type variables_ and stand for any types. Code\nwritten using these functions is checked for type correctness at compile time.\nAnd this guarantees strong properties at run time, for example that the\nelements of any list all have the same type.\n"},{"cell_type":"code","metadata":{},"source":"let rec nlength = function\n | [] -> 0\n | x :: xs -> 1 + nlength xs ;;","outputs":[],"execution_count":35},{"cell_type":"code","metadata":{},"source":"nlength [] ;;","outputs":[],"execution_count":36},{"cell_type":"code","metadata":{},"source":"nlength [5; 6; 7] ;;","outputs":[],"execution_count":37},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nnlength[a,b,c] \\Rightarrow & 1 + nlength[b,c] \\\\\n   \\Rightarrow & 1 + (1 + nlength[c]) \\\\\n   \\Rightarrow & 1 + (1 + (1 + nlength[])) \\\\\n   \\Rightarrow & 1 + (1 + (1 + 0)) \\\\\n   \\Rightarrow & \\ldots \\;\\; 3\n\\end{align*}\n$$\n\nMost list processing involves recursion.  This is a simple example; patterns\ncan be more complex.  Observe the use of a vertical bar `|` to separate the function's\nclauses.  We have _one_ function declaration that handles two cases.\nTo understand its role, consider the following faulty code:\n"},{"cell_type":"code","metadata":{},"source":"let rec nlength [] = 0 ;;","outputs":[],"execution_count":38},{"cell_type":"code","metadata":{},"source":"let rec nlength (x::xs) = 1 + nlength xs ;;","outputs":[],"execution_count":39},{"cell_type":"markdown","metadata":{},"source":"\nThese are two declarations, not one.  First we declare `nlength` to be a\nfunction that handles only empty lists.  Then we redeclare it to be a function\nthat handles only non-empty lists; it can never deliver a result.  We see that\na second `let` declaration replaces any previous one rather than extending it\nto cover new cases.\n\nNow, let us return to the declaration shown on the slide.  The length function\nis _polymorphic_ and applies to _all_ lists regardless of element\ntype!  Most programming languages lack such flexibility.\n\nUnfortunately, this length computation is naive and wasteful.  Like\n`nsum` earlier, it is not tail-recursive.  It\nuses $O(n)$ space, where $n$ is the length of its input.  As usual, the\nsolution is to add an accumulating argument.\n"},{"cell_type":"code","metadata":{},"source":"let rec addlen = function\n  | (n, []) -> n\n  | (n, x::xs) -> addlen (n+1, xs) ;;","outputs":[],"execution_count":40},{"cell_type":"code","metadata":{},"source":"addlen (0, [5;6;7]) ;;","outputs":[],"execution_count":41},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\naddlen(0, [a,b,c]) \\Rightarrow &  addlen(1, [b,c]) \\\\\n  \\Rightarrow  & addlen(2, [c]) \\\\\n  \\Rightarrow  & addlen(3, []) \\\\\n  \\Rightarrow  & 3\n\\end{align*}\n$$\n\nPatterns can be as complicated as we like.  Here, the two patterns are\n`(n,[])` and `(n,x::xs)`.\n\nFunction `addlen` is again polymorphic.  Its type mentions the integer\naccumulator.\n\nNow we may declare an efficient length function.  It is simply a wrapper for\n`addlen`, supplying zero as the initial value of $n$.\n"},{"cell_type":"code","metadata":{},"source":"let length xs = addlen (0, xs) ;;","outputs":[],"execution_count":42},{"cell_type":"code","metadata":{},"source":"length [5;6;7;8] ;;","outputs":[],"execution_count":43},{"cell_type":"markdown","metadata":{},"source":"\nThe recursive calls do not nest: this version is iterative.  It takes $O(1)$\nspace.  Obviously its time requirement is $O(n)$ because it takes at least $n$\nsteps to find the length of an $n$-element list.\n"},{"cell_type":"code","metadata":{},"source":"let rec append = function\n  | ([], ys) -> ys\n  | (x::xs, ys) -> x :: append (xs,ys) ;;","outputs":[],"execution_count":44},{"cell_type":"code","metadata":{},"source":"append ([1;2;3], [4]) ;;","outputs":[],"execution_count":45},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nappend([1;2;3], [4]) \\Rightarrow & 1 :: append([2;3], [4]) \\\\\n  \\Rightarrow & 1 :: (2 :: append([3], [4])) \\\\\n  \\Rightarrow & 1 :: (2 :: (3 :: append([], [4]))) \\\\\n  \\Rightarrow & 1 :: (2 :: (3 :: [4])) \\;; [1;2;3;4]\n\\end{align*}\n$$\n\nHere is how append might be declared, ignoring the details of how `@` is made\nan infix operator.  This function is also not iterative.  It scans its first\nargument, sets up a string of `cons' operations (`::`) and finally does them.\n\nIt uses $O(n)$ space and time, where $n$ is the length of its first argument.\n_Its costs are independent of its second argument._\n\nAn accumulating argument could make it iterative, but with considerable\ncomplication.  The iterative version would still require $O(n)$ space and time\nbecause concatenation requires copying all the elements of the first list.\nTherefore, we cannot hope for asymptotic gains; at best we can decrease the\nconstant factor involved in $O(n)$, but complicating the code is likely to\nincrease that factor.  Never add an accumulator merely out of habit.\n\nNote append's polymorphic type. It tells us that two lists can be joined if\ntheir element types agree.\n"},{"cell_type":"markdown","metadata":{},"source":"\nnrev \n"},{"cell_type":"code","metadata":{},"source":"let rec nrev = function\n  | [] -> []\n  | x::xs -> (nrev xs) @ [x] ;;","outputs":[],"execution_count":46},{"cell_type":"code","metadata":{},"source":"nrev [1;2;3] ;;","outputs":[],"execution_count":47},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nnrev[a;b;c] \\Rightarrow & nrev[b,c] @ [a] \\\\\n  \\Rightarrow &  (nrev[c] @ [b]) @ [a] \\\\\n  \\Rightarrow &  ((nrev[] @ [c]) @ [b]) @ [a] \\\\\n  \\Rightarrow &  (([] @ [c]) @ [b]) @ [a] \\;; \\ldots \\;; [c,b,a]\n\\end{align*}\n$$\n\nThis reverse function is grossly inefficient due to poor usage of append,\nwhich copies its first argument.  If `nrev` is given a list of length\n$n>0$, then append makes $n-1$ conses to copy the reversed tail.  Constructing\nthe list `[x]` calls `cons` again, for a total of $n$ calls.  Reversing\nthe tail requires $n-1$ more conses, and so forth.  The total number of conses\nis:\n\n$$ 0 + 1 + 2 + \\cdots + n = {n(n+1)/2} $$\n\nThe time complexity is therefore $O(n^2)$.  Space complexity is only $O(n)$\nbecause the copies don't all exist at the same time.\n"},{"cell_type":"code","metadata":{},"source":"let rec revApp = function\n  | ([], ys) -> ys\n  | (x::xs, ys) -> revApp (xs, x::ys)","outputs":[],"execution_count":48},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nrevApp([a;b;c], []) \\Rightarrow & revApp([b,c], [a]) \\\\\n  \\Rightarrow & revApp([c], [b;a]) \\\\\n  \\Rightarrow & revApp([], [c;b;a]) \\\\\n  \\Rightarrow & [c;b;a]\n\\end{align*}\n$$\n\nCalling `revApp (xs,ys)` reverses the elements of `xs` and\nprepends them to `ys`.  Now we may declare\n"},{"cell_type":"code","metadata":{},"source":"let rev xs = revApp (xs, []) ;;","outputs":[],"execution_count":49},{"cell_type":"code","metadata":{},"source":"rev [1;2;3] ;;","outputs":[],"execution_count":50},{"cell_type":"markdown","metadata":{},"source":"\nIt is easy to see that this reverse function performs just $n$ conses, given\nan $n$-element list.  For both reverse functions, we could count the number of\nconses precisely---not just up to a constant factor.  $O$ notation is still\nuseful to describe the overall running time: the time taken by a cons\nvaries from one system to another.\n\nThe accumulator $y$ makes the function iterative.  But the gain in complexity\narises from the removal of `append`.  Replacing an expensive operation (append)\nby a series of cheap operations (cons) is called _reduction in strength_\nand is a common technique in computer science.  It originated when many\ncomputers did not have a hardware multiply instruction; the series of products\n$i\\times r$ for $i=0$, $\\ldots, n$ could more efficiently be computed by\nrepeated addition.  Reduction in strength can be done in various ways; we\nshall see many instances of removing append.\n\nConsing to an accumulator produces the result in reverse.  If\nthat forces the use of an extra list reversal then the iterative function\nmay be much slower than the recursive one.\n\n\n"}]}