{"metadata":{"kernelspec":{"display_name":"OCaml 4.07.1","language":"OCaml","name":"ocaml-jupyter"},"language_info":{"name":"OCaml","version":"4.07.1","codemirror_mode":"text/x-ocaml","file_extension":".ml","mimetype":"text/x-ocaml","nbconverter_exporter":null,"pygments_lexer":"OCaml"}},"nbformat":4,"nbformat_minor":2,"cells":[{"cell_type":"markdown","metadata":{},"source":"This course has two aims. The first is to teach programming. The second is to\npresent some fundamental principles of computer science, especially algorithm\ndesign. Most students will have some programming experience already, but there\nare few people whose programming cannot be improved through greater knowledge\nof basic principles. Please bear this point in mind if you have extensive\nexperience and find parts of the course rather slow.\n\nThe programming in this course is based on the language [OCaml](https://ocaml.org)\nand mostly concerns the functional programming style. Functional programs tend\nto be shorter and easier to understand than their counterparts in conventional\nlanguages such as C. In the space of a few weeks, we shall cover many\nfundamental data structures and learn basic methods for estimating efficiency.\n"},{"cell_type":"markdown","metadata":{},"source":"## Learning Guide\n\n\n\nThe first thing you will notice about this course is that there is an\n_interactive_ version hosted online at <https://hub.cl.cam.ac.uk/>, where you\ncan login with your Cambridge Raven identity and edit the code fragments in\nyour browser.  You are encouraged to do so -- such edits will only persist in\nyour session, and will help you to explore the world of functional programming.\nIf you are using the web-based version, then you need to know a few concepts:\n\n- The notebook consists of a sequence of textual and code snippets.\n- The code snippets can be executed individually, and will “remember” the\n  results of the previous snippets.\n- To begin with, click on `Cell` / `Run All` in the menu to execute the\n  entire notebook.\n- You can later double click on any cell and modify its contents, and\n  press `Shift+Enter` to reevaluate its contents.  This will only modify\n  the current cell, so you will have to `Run All` again to see the effects\n  on the whole notebook.\n- While editing longer snippets, you can also press `Shift+Tab` while\n  typing to get more documentation hints about the code you are writing. \n\nThis course is lectured by Amanda Prorok and Anil Madhavapeddy.  These notes\nare translated from Lawrence C. Paulson’s earlier course on Standard ML, which\nhad credits to David Allsopp, Stuart Becker, Gavin Bierman, Chloë Brown, Silas\nBrown, Qi Chen, David Cottingham, William Denman, Robert Harle, Daniel Hulme,\nFrank King, Jack Lawrence-Jones, Joseph Lord, Dimitrios Los, Farhan Mannan,\nJames Margetson, David Morgan, Alan Mycroft, Sridhar Prabhu, Frank Stajano,\nThomas Tuerk, Xincheng Wang, Philip Withnall and Assel Zhiyenbayeva for\npointing out errors.  The current notes have been ported to OCaml in 2019 by\nDavid Allsopp, Jon Ludlam and Anil Madhavapeddy, with feedback from Richard\nSharp.\n\nSome books that are complementary to this course are:\n\n- [*OCaml from the Very Beginning*](http://ocaml-book.com) by John Whitington.\n"},{"cell_type":"markdown","metadata":{},"source":"# Lecture 1: Introduction to Programming\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"## Basic Concepts in Computer Science\n\n\n\n- Computers: a child can use them; **nobody** can fully understand them!\n- We can master complexity through levels of abstraction.\n- Focus on 2 or 3 levels at most!\n\n**Recurring issues:**\n\n- _what services_ to provide at each level\n- _how to implement_ them using lower-level services\n- _the interface:_ how the two levels should communicate\n\nA basic concept in computer science is that large systems can only be\nunderstood in levels, with each level further subdivided into functions or\nservices of some sort. The interface to the higher level should supply the\nadvertised services. Just as important, it should block access to the means by\nwhich those services are implemented. This _abstraction barrier_ allows one\nlevel to be changed without affecting levels above. For example, when a\nmanufacturer designs a faster version of a processor, it is essential that\nexisting programs continue to run on it. Any differences between the old and\nnew processors should be invisible to the program.\n\nModern processors have elaborate specifications, which still sometimes leave\nout important details. In the old days, you then had to consult the circuit\ndiagrams.\n"},{"cell_type":"markdown","metadata":{},"source":"### Example 1: Dates\n\n\n\n- Abstract level: dates over a certain interval\n- Concrete level: typically 6 characters: `YYMMDD` (where each character is represented by 8 bits)\n- Date crises caused by __inadequate__ internal formats:\n  * Digital’s PDP-10: using 12-bit dates (good for at most 11 years)\n  * 2000 crisis: 48 bits could be good for lifetime of universe!\n\nDigital Equipment Corporation’s date crisis occurred in 1975.  The\nPDP-10 was a 36-bit mainframe computer. It represented dates using a 12-bit\nformat designed for the tiny PDP-8. With 12 bits, one can distinguish\n$2^{12} = 4096$ days or 11 years.\n\nThe most common industry format for dates uses six characters: two for the\nyear, two for the month and two for the day. The most common “solution” to the\nyear 2000 crisis is to add two further characters, thereby altering file sizes.\nOthers have noticed that the existing six characters consist of 48 bits,\nalready sufficient to represent all dates over the projected lifetime of the\nuniverse: $2^{48}$ = $2.8\\times 10^{14}$ days = $7.7\\times 10^{11}$ years!\n\nMathematicians think in terms of unbounded ranges, but the representation we\nchoose for the computer usually imposes hard limits. A good programming\nlanguage like OCaml lets one easily change the representation used in the\nprogram.  But if files in the old representation exist all over the place,\nthere will still be conversion problems. The need for compatibility with older\nsystems causes problems across the computer industry.\n"},{"cell_type":"markdown","metadata":{},"source":"### Example II: Floating Point Numbers\n\n\n\nComputers have integers like `1066` and floats like $1.066\\times 10^3$.\nA floating-point number is represented by two integers.\nThe concept of _data type_ involves:\n\n- how a value is represented inside the computer\n- the suite of operations given to programmers\n- valid and invalid (or exceptional) results, such as “infinity”\nComputer arithmetic can yield _incorrect answers!_\n\nIn science, numbers written with finite precision and a decimal exponent are\nsaid to be in _standard form_. The computational equivalent is the _floating\npoint number_. These are familiar to anybody who has used a scientific\ncalculator.  Internally, a float consists of two integers.\n\nBecause of its finite precision, floating-point computations are potentially\ninaccurate. To see an example, use your nearest electronic calculator to\ncompute $(2^{1/10000})^{10000}$. I get $1.99999959$! With certain computations,\nthe errors spiral out of control. Many programming languages fail to check\nwhether even integer computations fall within the allowed range: you can add\ntwo positive integers and get a negative one!\n\nMost computers give us a choice of precisions. In 32-bit precision, integers\ntypically range from $2^{31}-1$ (namely 2 147 483 647) to $-2^{31}$; reals\nare accurate to about six decimal places and can get as large as $10^{35}$ or so.\nFor reals, 64-bit precision is often preferred. Early languages like Fortran\nrequired variables to be declared as `INTEGER`, `REAL` or `COMPLEX` and barred\nprogrammers from mixing numbers in a computation. Nowadays, programs handle\nmany different kinds of data, including text and symbols. The concept of a\n_data type_ can ensure that different types of data are not combined in a\nsenseless way.\n\nInside the computer, all data are stored as bits. In most programming\nlanguages, the compiler uses types to generate correct machine code, and types\nare not stored during program execution. In this course, we focus almost\nentirely on programming in a high-level language: OCaml.\n"},{"cell_type":"markdown","metadata":{},"source":"## Goals of Programming\n\n\n\n- to describe a computation so that it can be done **mechanically**:\n  * Expressions compute values.\n  * Commands cause effects.\n- to do so efficiently and **correctly**, giving the right answers quickly\n- to allow easy modification as needs change\n  * Through an orderly **structure** based on abstraction principles\n  * Such as modules or classes\n\nProgramming _in-the-small_ concerns the writing of code to do simple, clearly\ndefined tasks. Programs provide expressions for describing mathematical\nformulae and so forth. This was the original contribution of FORTRAN, the\nFORmula TRANslator. Commands describe how control should flow from one part of\nthe program to the next.\n\nAs we code layer upon layer, we eventually find ourselves programming\n_in the large_ : joining large modules to solve some messy task. Programming\nlanguages have used various mechanisms to allow one part of the program to\nprovide interfaces to other parts. Modules encapsulate a body of code, allowing\noutside access only through a programmer-defined interface. _Abstract Data\nTypes_ are a simpler version of this concept, which implement a single concept\nsuch as dates or floating-point numbers.\n\n_Object-oriented programming_ is the most complicated approach to modularity.\n_Classes_ define concepts, and they can be built upon other classes. Operations\ncan be defined that work in appropriately specialised ways on a family of\nrelated classes. _Objects_ are instances of classes and hold the data that is\nbeing manipulated.\n\nThis course does not cover OCaml’s sophisticated module system, which can do\nmany of the same things as classes. You will learn all about objects when you\nstudy Java. OCaml includes a powerful object system, although this is not used\nas much as its module system.\n"},{"cell_type":"markdown","metadata":{},"source":"## Why Program in OCaml?\n\n\n\nWhy program in OCaml at all?\n\n* It is interactive.\n* It has a flexible notion of _data type_.\n* It hides the underlying hardware: _no crashes_.\n* Programs can easily be understood mathematically.\n* It distinguishes naming something from _updating memory_.\n* It manages storage for us.\n\nProgramming languages matter. They affect the reliability, security, and\nefficiency of the code you write, as well as how easy it is to read, refactor,\nand extend. The languages you know can also change how you think, influencing\nthe way you design software even when you’re not using them.\n\nWhat makes OCaml special is that it occupies a sweet spot in the space of\nprogramming language designs. It provides a combination of efficiency,\nexpressiveness and practicality that is difficult to find matched by any other language.\n“ML” was originally the meta language of the LCF (Logic for Computable Functions)\nproof assistant released by Robin Milner in 1972 (at Stanford, and later at Cambridge).\nML was turned into a compiler in order to make it easier to use LCF on different machines,\nand it was gradually turned into a full-fledged system of its own by the 1980s.\n\nThe modern OCaml emerged in 1996, and the past few decades have seen OCaml\nattract a significant user base, and language improvements have been steadily\nadded to support the growing commercial and academic codebases.\nOCaml is therefore the outcome of years of research into programming languages,\nand a good base to begin our journey into learning the foundations of computer\nscience.\n\nBecause of its connection to mathematics, OCaml programs can be designed and\nunderstood without thinking in detail about how the computer will run them.\nAlthough a program can abort, it cannot crash: it remains under the control of\nthe OCaml system. It still achieves respectable efficiency and provides\nlower-level primitives for those who need them. Most other languages allow\ndirect access to the underlying machine and even try to execute illegal\noperations, causing crashes.\n\nThe only way to learn programming is by writing and running programs. This web\nnotebook provides an interactive environment where you can modify the example\nfragments and see the results for yourself.  You should also consider\ninstalling OCaml on your own computer so that you try more advanced programs\nlocally.\n"},{"cell_type":"markdown","metadata":{},"source":"## A first session with OCaml\n\n\n"},{"cell_type":"code","metadata":{},"source":"let pi = 3.14159265358979","outputs":[],"execution_count":1},{"cell_type":"markdown","metadata":{},"source":"\nThe first line of this simple session is a _value declaration_. It makes the\nname `pi` stand for the floating point number `3.14159`. (Such names are called\n_identifiers_.)  OCaml echoes the name (`pi`) and type (`float`) of the\ndeclared identifier.\n"},{"cell_type":"code","metadata":{},"source":"pi *. 1.5 *. 1.5","outputs":[],"execution_count":2},{"cell_type":"markdown","metadata":{},"source":"\nThe second line computes the area of the circle with radius `1.5` using the\nformula $A = \\pi r^2$. We use `pi` as an abbreviation for `3.14159`.\nMultiplication is expressed using `*.`, which is called an _infix operator_\nbecause it is written between its two operands.\n\nOCaml replies with the computed value (about `7.07`) and its type (again `float`).\n"},{"cell_type":"code","metadata":{},"source":"let area r = pi *. r *. r","outputs":[],"execution_count":3},{"cell_type":"markdown","metadata":{},"source":"\nTo work abstractly, we should provide the service “compute the area of a\ncircle,” so that we no longer need to remember the formula. This sort of\nencapsulated computation is called a _function_. The third line declares the\nfunction `area`. Given any floating point number `r`, it returns another\nfloating point number computed using the `area` formula; note that the function\nhas type `float -> float`.\n"},{"cell_type":"code","metadata":{},"source":"area 2.0","outputs":[],"execution_count":4},{"cell_type":"markdown","metadata":{},"source":"\nThe fourth line calls the function `area` supplying `2.0` as the argument. A\ncircle of radius `2` has an area of about `12.6`. Note that brackets around a\nfunction argument are not necessary.\n\nThe function uses `pi` to stand for `3.14159`. Unlike what you may have seen in\nother programming languages, `pi` cannot be “assigned to” or otherwise updated.\nIts meaning within `area` will persist even if we issue a new `let` declaration\nfor `pi` afterwards.\n"},{"cell_type":"markdown","metadata":{},"source":"## Raising a Number to a Power\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec npower x n =\n  if n = 0 then 1.0\n  else x *. npower x (n - 1)","outputs":[],"execution_count":5},{"cell_type":"markdown","metadata":{},"source":"\n_Mathematical Justification_ (for $x\\not=0$):\n$$\n\\begin{align}\n           x^0 & = 1 \\\\\n           x^{n+1} & = x\\times x^n.\n\\end{align}\n$$\n\nThe function `npower` raises its real argument `x` to the power `n`, a\nnon-negative integer. The function is **recursive**: it calls itself.  You\ncan spot a recursive function due to the `rec` keyword in the definition:\nthis indicates that any invocation of the function name should call itself.\nThis concept should be familiar from mathematics, since exponentiation is defined by the\nrules shown above. You may also have seen recursion in the product rule for\ndifferentiation: $(u\\cdot v)' = u\\cdot v' + u'\\cdot v$. In finding the derivative of $u\\cdot v$,\nwe recursively find the derivatives of $u$ and $v$, combining them to obtain the desired result.\nThe recursion is meaningful because it terminates: we reduce the problem to two smaller\nproblems, and this cannot go on forever. The OCaml programmer uses recursion\nheavily.  For $n\\geq0$, the equation $x^{n+1} = x\\times x^n$ yields an obvious\ncomputation:\n\n$$ x^3 = x\\times x^2 = x\\times x\\times x^1 = x\\times x\\times x\\times x^0 = x\\times x\\times x $$\n\nThe equation clearly holds even for negative $n$. However, the corresponding\ncomputation runs forever:\n\n$$ x^{-1} = x\\times x^{-2} = x\\times x\\times x^{-3}=\\cdots $$\n\nNote that the function `npower` contains both an integer constant (`0`) and a\nfloating point constant (`1.0`). The decimal point makes all the difference.\nOCaml will notice and ascribe different meaning to each type of constant.\n"},{"cell_type":"code","metadata":{},"source":"let square x = x *. x","outputs":[],"execution_count":6},{"cell_type":"markdown","metadata":{},"source":"\nNow for a tiresome but necessary aside. In most languages, the types of\narguments and results must always be specified. OCaml is unusual that it normally\ninfers the types itself. However, sometimes it is useful to supply a hint to\nhelp you debug and develop your program. OCaml will still infer the types even if you don’t specify them, but in some cases\nit will use a more inefficient function than a specialised one. Some languages\nhave just one type of number, converting automatically between different\nformats; this is slow and could lead to unexpected rounding errors.  Type\nconstraints are allowed almost anywhere. We can put one on any occurrence of x\nin the function.\n"},{"cell_type":"code","metadata":{},"source":"let square (x : float) = x *. x","outputs":[],"execution_count":7},{"cell_type":"markdown","metadata":{},"source":"Or we can constrain the type of the function’s result:"},{"cell_type":"code","metadata":{},"source":"let square x : float = x *. x","outputs":[],"execution_count":8},{"cell_type":"markdown","metadata":{},"source":"\nOCaml treats the equality and comparison test specially. Expressions like `if x = y then` …\nare allowed provided `x` and `y` have the same type and equality testing is\npossible for that type. (We discuss equality further in a later lecture.)\nNote that `x <> y` is OCaml for $x\\not=y$.\n\nA characteristic feature of the computer is its ability to test for conditions\nand act accordingly.  In the early days, a program might jump to a given\naddress depending on the sign of some number.  Later, John McCarthy defined\nthe _conditional expression_ to satisfy:\n"},{"cell_type":"raw","metadata":{},"source":"if true then x else y = x\nif false then x else y = y"},{"cell_type":"markdown","metadata":{},"source":"\nOCaml evaluates the expression `if` $B$ `then` $E_1$ `else` $E_2$ by first evaluating $B$.\nIf the result is `true` then OCaml evaluates $E_1$ and otherwise $E_2$.  Only one\nof the two expressions $E_1$ and $E_2$ is evaluated!  If both were evaluated,\nthen recursive functions like `npower` above would run forever.\n\nThe `if`-expression is governed by an expression of type `bool`, whose two\nvalues are `true` and `false`.  In modern programming languages, tests are not\nbuilt into “conditional branch” constructs but have an independent status.\nTests, or _Boolean expressions,_ can be expressed using relational operators\nsuch as `<` and `=`. They can be combined using the Boolean operators for\nnegation (`not`), conjunction (written as `&&`) and disjunction (written as `||`).  New\nproperties can be declared as functions: here, to test whether an integer is\neven, for example:\n"},{"cell_type":"code","metadata":{},"source":"let even n = n mod 2 = 0","outputs":[],"execution_count":9},{"cell_type":"markdown","metadata":{},"source":"## _Efficiently_ Raising a Number to a Power\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec power x n =\n  if n = 1 then x\n  else if even n then\n    power (x *. x) (n / 2)\n  else\n    x *. power (x *. x) (n / 2)","outputs":[],"execution_count":10},{"cell_type":"markdown","metadata":{},"source":"\n_Mathematical Justification_\n$$\\begin{align}\n           x^1 & = x \\\\\n           x^{2n} & = (x^2)^n  \\\\\n           x^{2n+1} & = x\\times(x^2)^n.\n\\end{align}$$\n\nFor large `n`, computing powers using $x^{n+1} = x\\times x^n$ is too slow to\nbe practical.  The equations above are much faster. Example:\n\n$$ 2^{12} = 4^6 = 16^3 = 16\\times 256^1 = 16\\times 256 = 4096. $$\n\nInstead of `n` multiplications, we need at most $2\\lg n$ multiplications,\nwhere $\\lg n$ is the logarithm of $n$ to the base $2$.\n\nWe use the function `even`, declared previously, to test whether the\nexponent is even.  Integer division (`div`) truncates its result to an\ninteger: dividing $2n+1$ by 2 yields $n$.\n\nA recurrence is a useful computation rule only if it is bound to terminate.\nIf $n>0$ then $n$ is smaller than both $2n$ and $2n+1$.  After enough\nrecursive calls, the exponent will be reduced to $1$.  The equations also hold\nif $n\\leq0$, but the corresponding computation runs forever.\n\nOur reasoning assumes arithmetic to be _exact_. Fortunately, the calculation is\nwell-behaved using floating-point.\n\nComputer numbers have a finite range, which if exceeded results in the\ninteger wrapping around.  You will understand this behaviour more as you\nlearn about computer architecture and how modern systems represent\nnumbers in memory.\n\nIf integers and reals must be combined in a calculation, OCaml provides functions\nto convert between them:\n"},{"cell_type":"code","metadata":{},"source":"int_of_float 3.14159","outputs":[],"execution_count":11},{"cell_type":"code","metadata":{},"source":"float_of_int 3","outputs":[],"execution_count":12},{"cell_type":"markdown","metadata":{},"source":"\nOCaml’s libraries are organised using “modules”, so we may use compound\nidentifiers such as `Float.of_int` to refer to library functions. There\nare many thousands of library functions available in the OCaml ecosystem,\nincluding text-processing and operating systems functions in addition to the\nusual numerical ones.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 1.1\n\n\nOne solution to the year 2000 bug involves storing years as two digits, but interpreting them such\nthat 50 means 1950 and 49 means 2049. Comment on the merits and demerits of this approach.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 1.2\n\n\nUsing the date representation of the previous exercise, code OCaml functions to (a) compare two\nyears (b) add/subtract some given number of years from another year.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 1.3\n\n\n\nWhy would no experienced programmer write an expression of the form `if` … `then true else false`?\nWhat about expressions of the form `if` … `then false else true`?\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 1.4\n\n\nFunctions `npower` and `power` both return a `float`. The definition of `npower` returns the float\nvalue `1.0` in its base case. The definition of `power` does not, so how does the OCaml type checker\nknow that `power` returns a `float`?\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 1.5\n\n\nBecause computer arithmetic is based on binary numbers, simple decimals such as 0.1 often cannot be\nrepresented exactly. Write a function that performs the computation\nm$$\\underbrace{x+x+\\cdots+x}_{n}$$\nwhere $x$ has type `float`. (It is essential to use repeated addition rather than multiplication!)\n\nAn error of this type has been blamed for the failure of an American Patriot Missile battery to\nintercept an incoming Iraqi missile during the [first Gulf War](https://en.wikipedia.org/wiki/MIM-104_Patriot#Failure_at_Dhahran).\nThe missile hit an American Army barracks, killing 28.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 1.6\n\n\nAnother example of the inaccuracy of floating-point arithmetic takes the golden ratio\n$\\phi\\approx1.618\\ldots$ as its starting point:\n$$\\gamma_0 = \\frac{1+\\sqrt5}{2} \\quad\\text{and}\\quad\\gamma_{n+1} = \\frac{1}{\\gamma_n-1}.$$\nIn theory, it is easy to prove that $\\gamma_n=\\cdots = \\gamma_1 = \\gamma_0$ for all $n>0$. Code this\ncomputation in OCaml and report the value of $\\gamma_{50}$. _Hint:_ in OCaml, $\\sqrt5$ is expressed\nas `sqrt 5.0`.\n"},{"cell_type":"markdown","metadata":{},"source":"# Lecture 2: Recursion and Efficiency\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"## Expression Evaluation\n\n\n\nExpression evaluation concerns expressions and the values they return. This\nview of computation may seem to be too narrow. It is certainly far removed from\ncomputer hardware, but that can be seen as an advantage. For the traditional\nconcept of computing solutions to problems, expression evaluation is entirely\nadequate.\n\nStarting with $E_0$, the expression $E_i$ is reduced to $E_{i+1}$ until this\nprocess concludes with a value $v$.  A _value_ is something like a number\nthat cannot be further reduced.\n\nWe write $E \\rightarrow E'$ to say that $E$ is _reduced_ to $E'$.\nMathematically, they are equal: $E=E'$, but the computation goes from $E$ to\n$E'$ and never the other way around.\n\nComputers also interact with the outside world.  For a start, they need some\nmeans of accepting problems and delivering solutions.  Many computer systems\nmonitor and control industrial processes.  This role of computers is familiar\nnow, but was never envisaged in the early days. Computer pioneers focused on\nmathematical calculations.  Modelling interaction and control requires a notion\nof _states_ that can be observed and changed.  Then we can consider\nupdating the state by assigning to variables or performing input/output,\nfinally arriving at conventional programs as coded in C, for instance.\n\nFor now, we remain at the level of expressions, which is usually termed\n_functional programming_.\n\n"},{"cell_type":"markdown","metadata":{},"source":"## Summing the first _n_ integers\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec nsum n =\n  if n = 0 then\n    0\n  else\n    n + nsum (n - 1)","outputs":[],"execution_count":13},{"cell_type":"markdown","metadata":{},"source":"\nThe function call `nsum n` computes the sum `1 +` … `+ nz` rather naively, hence the\ninitial `n` in its name:\n\n$$\n\\begin{aligned}\nnsum 3 \\Rightarrow &  3 + (nsum 2) \\\\\n       \\Rightarrow  & 3 + (2 + (num 1) \\\\\n       \\Rightarrow  & 3 + (2 + (1 + nsum 0)) \\\\\n       \\Rightarrow  & 3 + (2 + (1 + 0))\n\\end{aligned}\n$$\n\nThe nesting of parentheses is not just an artifact of\nour notation; it indicates a real problem.  The function gathers up a\ncollection of numbers, but none of the additions can be performed until `nsum\n0` is reached.  Meanwhile, the computer must store the numbers in an internal\ndata structure, typically the _stack_.  For large `n`, say `nsum 10000`, the\ncomputation might fail due to stack overflow.\n\nWe all know that the additions can be performed as we go along.  How do we\nmake the computer do that?\n"},{"cell_type":"markdown","metadata":{},"source":"## Iteratively summing the first `n` integers\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec summing n total =\n  if n = 0 then\n    total\n  else\n    summing (n - 1) (n + total)","outputs":[],"execution_count":14},{"cell_type":"markdown","metadata":{},"source":"\nFunction `summing` takes an additional argument: a running total.  If\n`n` is zero then it returns the running total; otherwise, `summing`\nadds to it and continues.  The recursive calls do not nest; the additions are\ndone immediately.\n\nA recursive function whose computation does not nest is called\n_iterative_ or _tail-recursive_. Many functions can be made iterative by\nintroducing an argument analogous to `total`, which is often called an\n_accumulator_.\n\nThe gain in efficiency is sometimes worthwhile and sometimes not.  The function\n`power` is not iterative because nesting occurs whenever the exponent is odd.\nAdding a third argument makes it iterative, but the change complicates the\nfunction and the gain in efficiency is minute; for 32-bit integers, the maximum\npossible nesting is 30 for the exponent $2^{31}-1$.\n\n"},{"cell_type":"markdown","metadata":{},"source":"## Recursion _vs_ Iteration\n\n\n\n- “Iterative” normally refers to a loop, coded using `while` for example (see the last lecture)\n- Tail-recursion is only efficient if the compiler detects it\n- Mainly it saves space (memory), though iterative code can also run after\n- Do not make programs iterative unless the gain is worth it\n\nA [classic book](https://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs)\nby Abelson and Sussman used _iterative_ to mean _tail-recursive_. It describes\nthe Lisp dialect known as Scheme. Iterative functions produce computations\nresembling those that can be done using while-loops in conventional languages.\n\nMany algorithms can be expressed naturally using recursion, but only awkwardly\nusing iteration. There is a story that Dijkstra sneaked recursion into Algol-60\nby inserting the words “any other occurrence of the procedure name denotes\nexecution of the procedure.” By not using the word “recursion”, he managed to\nslip this amendment past sceptical colleagues.\n\nObsession with tail recursion leads to a coding style in which functions\nhave many more arguments than necessary.  Write straightforward code first,\navoiding only gross inefficiency.  If the program turns out to be too slow,\ntools are available for pinpointing the cause.  Always remember KISS (Keep\nIt Simple, Stupid).\n\nI hope you have all noticed by now that the summation can be done even more\nefficiently using the arithmetic progression formula:\n\n$$ 1+\\cdots+n = n(n+1)/2 $$\n"},{"cell_type":"markdown","metadata":{},"source":"## Silly Summing the First _n_ Integers\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec sillySum n =\n  if n = 0 then\n    0\n  else\n    n + (sillySum (n - 1) + sillySum (n - 1)) / 2","outputs":[],"execution_count":15},{"cell_type":"markdown","metadata":{},"source":"\nThe function calls itself $2^n$ times!  Bigger inputs mean higher costs---but\nwhat’s the growth rate?\n\nNow let us consider how to estimate various costs associated with a program.\n_Asymptotic complexity_ refers to how costs---usually time or space---grow with\nincreasing inputs. Space complexity can never exceed time complexity, for it\ntakes time to do anything with the space.  Time complexity often greatly\nexceeds space complexity.\n\nThe function `sillySum` calls itself twice in each recursive step.  This\nfunction is contrived, but many mathematical formulas refer to a particular\nquantity more than once.  In OCaml, we can create a local binding to a computed\nvalue using the _local declaration_ syntax. In the following expression, `y` is\ncomputed once and used twice:\n"},{"cell_type":"code","metadata":{},"source":"let x = 2.0 in\nlet y = Float.pow x 20.0 in\ny *. (x /. y)","outputs":[],"execution_count":16},{"cell_type":"markdown","metadata":{},"source":"\nFast hardware does not make good algorithms unnecessary.  On the contrary,\nfaster hardware magnifies the superiority of better algorithms.  Typically, we\nwant to handle the largest inputs possible.  If we double our processing power,\nwhat do we gain?  How much can we increase $n$, the input to our function?\nWith `sillySum`, we can only go from $n$ to $n+1$.  We are limited to this\nmodest increase because the function’s running time is proportional to $2^n$.\nWith the function `npower` defined in the previous lecture, we can go from $n$\nto $2n$: we can handle problems twice as big.  With `power` we can do much\nbetter still, going from $n$ to $n^2$.\n\n| complexity | 1 second   | 1 minute   | 1 hour    | gain  |\n| :--------: | ---------: | ---------: | --------: | ----: |\n| $n$        |       1000 |     60 000 | 3 600 000 | $x60$ |\n| $n \\log n$ |        140 |      4 895 |   204 095 | $x41$ |\n| $n^{2}$    |         31 |        244 |     1 897 | $x8$  |\n| $n^{3}$    |         10 |         39 |       153 | $x4$  |\n| $2^{n}$    |          9 |         15 |        21 | $+6$  |\n\nThis table (excerpted from [a 40-year-old book](https://archive.org/details/designanalysisof00ahoarich)!)\nillustrates the effect of various time complexities.  The left-hand column indicates how many\nmilliseconds are required to process an input of size $n$.  The other entries\nshow the maximum size of $n$ that can be processed in the given time (one\nsecond, minute or hour).\n\nThe table illustrates how large an input can be processed as a function\nof time.  As we increase the computer time per input from one second to one\nminute and then to one hour, the size of the input increases accordingly.\n\nThe top two rows (complexities $n$ and $n \\lg n$) increase rapidly: for $n$, by\na factor of 60.  The bottom two start out close together, but $n^3$ (which\ngrows by a factor of 3.9) pulls well away from $2^n$ (whose growth is only\nadditive).  If an algorithm’s complexity is exponential then it can never\nhandle large inputs, even if it is given huge resources.  On the other hand,\nsuppose the complexity has the form $n^c$, where $c$ is a constant.  (We say\nthe complexity is _polynomial_.)  Doubling the argument then increases the\ncost by a constant factor.  That is much better, though if $c>3$ the algorithm\nmay not be considered practical.\n"},{"cell_type":"markdown","metadata":{},"source":"## Comparing Algorithms: O Notation\n\n\n\n- Formally, define $f(n) = O(g(n))$ provided $|f(n)| \\leq c|g(n)|$\n- $|f(n)|$ is bounded for some constant $c$ and all _sufficiently large_ $n$.\n- Intuitively, look at the _most significant_ term.\n- Ignore _constant factors_ as they seldom dominate and are often transitory\n\nFor example: consider $n^2$ instead of $3n^2+34n+433$.\n\nThe cost of a program is usually a complicated formula.  Often we should\nconsider only the most significant term.  If the cost is $n^2 + 99n + 900$\nfor an input of size $n$, then the $n^2$ term will eventually dominate,\neven though $99n$ is bigger for $n<99$.\nThe constant term $900$ may look big, but it is soon dominated by $n^2$.\n\nConstant factors in costs can be ignored unless they are large.  For one thing,\nthey seldom make a difference: $100n^2$ will be better than $n^3$ in the long\nrun: or _asymptotically_ to use the jargon.  Moreover, constant factors are\nseldom stable.  They depend upon details such as which hardware, operating\nsystem or programming language is being used.  By ignoring constant factors, we\ncan make comparisons between algorithms that remain valid in a broad range of\ncircumstances.\n\nThe “Big O” notation is commonly used to describe efficiency---to be precise,\n_asymptotic complexity_.  It concerns the limit of a function as its\nargument tends to infinity.  It is an abstraction that meets the informal\ncriteria that we have just discussed.\nIn the definition, _sufficiently large_ means there is some constant $n_0$\nsuch that $|f(n)|\\leq c|g(n)|$ for all $n$ greater than $n_0$.  The\nrole of $n_0$ is to ignore finitely many exceptions to the bound, such as the\ncases when $99n$ exceeds $n^2$.\n"},{"cell_type":"markdown","metadata":{},"source":"## Simple Facts About O Notation\n\n\n\n$$\n\\begin{aligned}\n O(2g(n)) & \\text{ is the same as } O(g(n)) \\\\\n O(\\log_{10}n) & \\text{ is the same as } O(\\ln n)  \\\\\n O(n^2+50n+36) & \\text{ is the same as } O(n^2) \\\\[1.5ex]\n O(n^2) & \\text{ is contained in }  O(n^3) \\\\\n O(2^n) & \\text{ is contained in }  O(3^n)  \\\\\n O(\\log n) & \\text{ is contained in } O(\\sqrt n)\n\\end{aligned}\n$$\n\n$O$ notation lets us reason about the costs of algorithms easily.\n- Constant factors such as the $2$ in $O(2g(n))$ drop out: we can use $O(g(n))$ with twice the value of $c$ in the definition.\n- Because constant factors drop out, the base of logarithms is irrelevant.\n- Insignificant terms drop out.  To see that $O(n^2+50n+36)$ is the same as $O(n^2)$, consider that $n^2+50n+36/n^2$ converges to 1 for increasing $n$. In fact, $n^2+50n+36 \\le 2n^2$ for $n\\ge 51$, so can double the constant factor\n\nIf $c$ and $d$ are constants (that is, they are independent of $n$) with $0 < c < d$ then\n- $O(n^c)$ is contained in $O(n^d)$\n- $O(c^n)$ is contained in $O(d^n)$\n- $O(\\log n)$ is contained $in O(n^c)$\n\nTo say that $O(c^n)$ _is contained in_ $O(d^n)$ means that the former gives\na tighter bound than the latter.  For example, if $f(n)=O(2^n)$ then\n$f(n)=O(3^n)$ trivially, but the converse does not hold.\n"},{"cell_type":"markdown","metadata":{},"source":"## Common Complexity Classes\n\n\n\n- $O(1)$ is _constant_\n- $O(\\log n)$ is _logarithmic_\n- $O(n)$ is _linear_\n- $O(n\\log n)$ is _quasi-linear_\n- $O(n^2)$ is _quadratic_\n- $O(n^3)$ is _cubic_\n- $O(a^n)$ is _exponential_ (for fixed $a$)\n\nLogarithms grow very slowly, so $O(\\log n)$ complexity is excellent.  Because\n$O$ notation ignores constant factors, the base of the logarithm is\nirrelevant!\n\nUnder linear we might mention $O(n\\log n)$, which occasionally is called\n_quasilinear_ and which scales up well for large $n$.\n\nAn example of quadratic complexity is matrix addition: forming the sum of two\n$n\\times n$ matrices obviously takes $n^2$ additions.  Matrix\nmultiplication is of cubic complexity, which limits the size of matrices that\nwe can multiply in reasonable time.  An $O(n^{2.81})$ algorithm exists, but it\nis too complicated to be of much use, even though it is theoretically better.\n\nAn exponential growth rate such as $2^n$ restricts us to small values of $n$.\nAlready with $n=20$ the cost exceeds one million.  However, the worst case\nmight not arise in normal practice.  OCaml type-checking is exponential in the\nworst case, but not for ordinary programs.\n"},{"cell_type":"markdown","metadata":{},"source":"## Sample costs in O notation\n\n\n\nRecall that `npower` computes $x^n$\nby repeated multiplication while `nsum` naively computes the sum\n$1+\\cdots+n$.  Each obviously performs $O(n)$ arithmetic operations.  Because\nthey are not tail recursive, their use of space is also $O(n)$.  The function\n`summing` is a version of `nsum` with an accumulating argument;\nits iterative behaviour lets it work in constant space.  $O$ notation spares\nus from having to specify the units used to measure space.\n\n\n| Function       | Time        | Space       |\n| :------------- | :---------- | :---------- |\n| npower, nsum   | O($n$)      | O($n$)      |\n| summing        | O($n$)      | O($1$)      |\n| $n(n+1)/2$     | O($1$)      | O($1$)      |\n| power          | O($\\log~n$) | O($\\log~n$) |\n| sillySum      | O($2^n$)    | O($n$)      |\n\nEven ignoring constant factors, the units chosen can influence the result.\nMultiplication may be regarded as a single unit of cost.  However, the cost of\nmultiplying two $n$-digit numbers for large $n$ is itself an important\nquestion, especially now that public-key cryptography uses numbers hundreds of\ndigits long.\n\nFew things can _really_ be done in constant time or stored in constant\nspace.  Merely to store the number $n$ requires $O(\\log n)$ bits.  If a\nprogram cost is $O(1)$, then we have probably assumed that certain operations\nit performs are also $O(1)$---typically because we expect never to exceed the\ncapacity of the standard hardware arithmetic.\n\nWith `power`, the precise number of operations depends upon $n$ in a\ncomplicated way, depending on how many odd numbers arise, so it is convenient\nthat we can just write $O(\\log n)$.  An accumulating argument could reduce its\nspace cost to $O(1)$.\n"},{"cell_type":"markdown","metadata":{},"source":"## Some Simple Recurrence Relations\n\n\n\n\nConsider $T(n)$ has a cost we want to bound using $O$ notation.\nA typical _base case_ is $T(1)=1$.  Some _recurrences_ are:\n\n| Equation           | Complexity   |\n| ------------------ | ------------ |\n| $T(n+1) = T(n)+1$  | $O(n)$       |\n| $T(n+1) = T(n)+n$  | $O(n^2)$     |\n| $T(n) = T(n/2)+1$  | $O(\\log n)$  |\n| $T(n) = 2T(n/2)+n$ | $O(n\\log n)$ |\n\nTo analyse a function, inspect its OCaml declaration.  Recurrence equations for\nthe cost function $T(n)$ can usually be read off.  Since we ignore constant\nfactors, we can give the base case a cost of one unit.  Constant work done in\nthe recursive step can also be given unit cost; since we only need an upper\nbound, this unit represents the larger of the two actual costs.  We could use\nother constants if it simplifies the algebra.\n\nFor example, recall our function `nsum`:\n"},{"cell_type":"code","metadata":{},"source":"let rec nsum n =\n  if n = 0 then \n    0\n  else\n    n + nsum (n - 1)","outputs":[],"execution_count":17},{"cell_type":"markdown","metadata":{},"source":"\nGiven $n+1$, it performs a constant amount of work (an addition and\nsubtraction) and calls itself recursively with argument $n$.  We get the\nrecurrence equations $T(0)=1$ and $T(n+1) = T(n)+1$.  The closed form is\nclearly $T(n)=n+1$, as we can easily verify by substitution.  The cost is\n_linear_.\n\nThis function, given $n+1$, calls `nsum`, performing $O(n)$ work.\nAgain ignoring constant factors, we can say that this call takes exactly $n$\nunits.\n"},{"cell_type":"code","metadata":{},"source":"let rec nsumsum n =\n  if n = 0 then\n    0\n  else\n    nsum n + nsumsum (n - 1)","outputs":[],"execution_count":18},{"cell_type":"markdown","metadata":{},"source":"\nWe get the recurrence equations $T(0)=1$ and $T(n+1) = T(n)+n$.  It is easy to\nsee that $T(n)=(n-1)+\\cdots+1=n(n-1)/2=O(n^2)$.  The cost is\n_quadratic_.\n\nThe function `power` divides its input $n$ into two, with\nthe recurrence equation $T(n) = T(n/2)+1$.  Clearly $T(2^n)=n+1$, so\n$T(n)=O(\\log n)$.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 2.1\n\n\nCode an iterative version of the function `power`.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 2.2\n\n\nAdd a column to the table of complexities from _The Design and Analysis of Computer Algorithms_ with the heading _60 hours:_\n\n| complexity |   1 second |   1 minute |    1 hour | 60 hours |\n| :--------: | ---------: | ---------: | --------: | -------: |\n| $n$        |       1000 |     60 000 | 3 600 000 |          |\n| $n \\log n$ |        140 |      4 895 |   204 095 |          |\n| $n^{2}$    |         31 |        244 |     1 897 |          |\n| $n^{3}$    |         10 |         39 |       153 |          |\n| $2^{n}$    |          9 |         15 |        21 |          |\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 2.3\n\n\nLet $g_1$, …, $g_k$ be functions such that $g_i(n)\\ge0$ for $i=1$, …, $k$ and all sufficiently\nlarge $n$.\n\nShow that if $f(n) = O(a_1 g_1(n)+\\cdots+a_k g_k(n))$ then $f(n) = O(g_1(n)+\\cdots+g_k(n))$.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 2.4\n\n\nFind an upper bound for the recurrence given by $T(1) = 1$ and $T(n) = 2T(n/2)+1$.  You should be\nable to find a tighter bound than $O(n\\log n)$.\n"},{"cell_type":"markdown","metadata":{},"source":"# Lecture 3: Lists\n\n\n"},{"cell_type":"code","metadata":{},"source":"let x = [3; 5; 9]","outputs":[],"execution_count":19},{"cell_type":"code","metadata":{},"source":"let y = [(1, \"one\"); (2, \"two\")]","outputs":[],"execution_count":20},{"cell_type":"markdown","metadata":{},"source":"\nA _list_ is an ordered series of elements; repetitions are significant.\nSo `[3; 5; 9]` differs from `[5; 3; 9]` and from `[3; 3; 5; 9]`.  Elements in the\nlist are separated with `;` when constructed, as opposed to the `,` syntax\nused for fixed-length tuples.\n\nAll elements of a list must have the same type.  Above we see a list of\nintegers and a list of `(integer, string)` pairs.  One can also have lists of\nlists, such as `[[3]; []; [5; 6]]`, which has type `int list list`.\n\nIn the general case, if $x_1; \\ldots; x_n$ all have the same type (say\n$\\tau$) then the list $[x_1;\\ldots;x_n]$ has type $(\\tau)\\texttt{list}$.\n\nLists are the simplest data structure that can be used to process collections\nof items.  Conventional languages use _arrays_ whose elements are\naccessed using subscripting: for example, $A[i]$ yields the $i$th element of\nthe array $A$.  Subscripting errors are a known cause of programmer grief,\nhowever, so arrays should be replaced by higher-level data structures whenever\npossible.\n"},{"cell_type":"code","metadata":{},"source":"x @ [2; 10]","outputs":[],"execution_count":21},{"cell_type":"code","metadata":{},"source":"List.rev [(1, \"one\"); (2, \"two\")]","outputs":[],"execution_count":22},{"cell_type":"markdown","metadata":{},"source":"\nThe infix operator `@` (also called `List.append`) concatenates two lists.\nAlso built-in is `List.rev`, which reverses a list.  These are demonstrated\nin the session above.\n"},{"cell_type":"markdown","metadata":{},"source":"## The List Primitives\n\n\n\nThere are two kinds of lists:\n\n- `[]` represents the empty list\n- `x :: l` is the list with head $x$ and tail $l$\n"},{"cell_type":"code","metadata":{},"source":"let nil = []","outputs":[],"execution_count":23},{"cell_type":"code","metadata":{},"source":"1 :: nil","outputs":[],"execution_count":24},{"cell_type":"code","metadata":{},"source":"1 :: 2 :: nil","outputs":[],"execution_count":25},{"cell_type":"markdown","metadata":{},"source":"\nThe operator `::` (also called `List.cons` for “construct”), puts a new element on\nto the head of an existing list.  While we should not be too preoccupied with\nimplementation details, it is essential to know that `::` is an $O(1)$\noperation.  It uses constant time and space, regardless of the length of the\nresulting list.  Lists are represented internally with a linked structure;\nadding a new element to a list merely hooks the new element to the front of\nthe existing structure.  Moreover, that structure continues to denote the same\nlist as it did before; to see the new list, one must look at the new `::` node\n(or “cons cell”) just created.\n\n\nHere we see the element `1` being consed to the front of the list `[3; 5; 9]`:\n\n$$\n\\let\\down=\\downarrow\n\\begin{array}{*{10}{c@{\\,}}c}\n:: & \\to & \\cdots & :: & \\to &  :: & \\to &  :: & \\to & nil \\\\\n\\down &  &        & \\down &  & \\down &  & \\down  \\\\\n1     &  &        & 3     &  & 5     &  & 9\n\\end{array}\n$$\n\nGiven a list, taking its first element (its “head”) or its list of\nremaining elements (its “tail”) also takes constant time.  Each\noperation just follows a link.  In the diagram above, the first down arrow\nleads to the head and the leftmost right arrow leads to the tail.  Once we\nhave the tail, its head is the second element of the original list, etc.\n\nThe tail is _not_ the last element; it is the _list_ of all elements\nother than the head!\n"},{"cell_type":"markdown","metadata":{},"source":"## Getting at the Head and Tail\n\n\n"},{"cell_type":"code","metadata":{},"source":"let null = function\n  | [] -> true\n  | x :: l -> false","outputs":[],"execution_count":26},{"cell_type":"code","metadata":{},"source":"null []","outputs":[],"execution_count":27},{"cell_type":"code","metadata":{},"source":"null [1; 2; 3]","outputs":[],"execution_count":28},{"cell_type":"code","metadata":{},"source":"let hd (x::l) = x","outputs":[],"execution_count":29},{"cell_type":"code","metadata":{},"source":"hd [1; 2; 3]","outputs":[],"execution_count":30},{"cell_type":"code","metadata":{},"source":"let tl (x::l) = l","outputs":[],"execution_count":31},{"cell_type":"code","metadata":{},"source":"tl [7; 6; 5]","outputs":[],"execution_count":32},{"cell_type":"markdown","metadata":{},"source":"\nThe empty list has neither head nor tail.  Applying `List.hd` `List.tl` to `[]`\nis an error---strictly speaking, an “exception”.  The function `null` can\nbe used to check for the empty list beforehand.  Taking a list apart using\ncombinations of `hd` and `tl` is hard to get right.  Fortunately, it is seldom\nnecessary because of _pattern-matching_.\n\nThe declaration of `null` above has two clauses: one for the empty list\n(for which it returns `true`) and one for non-empty lists (for which it\nreturns `false`).\n\nThe declaration of `hd` above has only one clause, for non-empty lists.  They\nhave the form `x::l` and the function returns `x`, which is the head.  OCaml\nprints a warning to tell us that calling the function could raise an exception\ndue to all possible inputs not being handled, including a counter-example (in\nthis case, the empty list `[]`). The declaration of `tl` is similar to `hd`.\n\nThese three primitive functions are _polymorphic_ and allow flexibility in the\ntypes of their arguments and results. Note their types!\n"},{"cell_type":"code","metadata":{},"source":"null","outputs":[],"execution_count":33},{"cell_type":"code","metadata":{},"source":"hd","outputs":[],"execution_count":34},{"cell_type":"code","metadata":{},"source":"tl","outputs":[],"execution_count":35},{"cell_type":"markdown","metadata":{},"source":"\nSymbols `'a` and `'b` are called _type variables_ and stand for any types. Code\nwritten using these functions is checked for type correctness at compile time.\nAnd this guarantees strong properties at run time, for example that the\nelements of any list all have the same type.\n"},{"cell_type":"markdown","metadata":{},"source":"## Computing the Length of a List\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec nlength = function\n| [] -> 0\n| x :: xs -> 1 + nlength xs","outputs":[],"execution_count":36},{"cell_type":"code","metadata":{},"source":"nlength []","outputs":[],"execution_count":37},{"cell_type":"code","metadata":{},"source":"nlength [5; 6; 7]","outputs":[],"execution_count":38},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{aligned}\nnlength[a; b; c] \\Rightarrow & 1 + nlength[b; c] \\\\\n   \\Rightarrow & 1 + (1 + nlength[c]) \\\\\n   \\Rightarrow & 1 + (1 + (1 + nlength[])) \\\\\n   \\Rightarrow & 1 + (1 + (1 + 0)) \\\\\n   \\Rightarrow & \\ldots \\;\\; 3\n\\end{aligned}\n$$\n\nMost list processing involves recursion.  This is a simple example; patterns\ncan be more complex.  Observe the use of a vertical bar `|` to separate the function’s\nclauses.  We have _one_ function declaration that handles two cases.\nTo understand its role, consider the following faulty code:\n"},{"cell_type":"code","metadata":{},"source":"let rec nlength [] = 0","outputs":[],"execution_count":39},{"cell_type":"code","metadata":{},"source":"let rec nlength (x::xs) = 1 + nlength xs","outputs":[],"execution_count":40},{"cell_type":"markdown","metadata":{},"source":"\nThese are two declarations, not one.  First we declare `nlength` to be a\nfunction that handles only empty lists.  Then we redeclare it to be a function\nthat handles only non-empty lists; it can never deliver a result.  We see that\na second `let` declaration replaces any previous one rather than extending it\nto cover new cases.\n\nNow, let us return to the declaration shown on the slide.  The length function\nis _polymorphic_ and applies to _all_ lists regardless of element\ntype!  Most programming languages lack such flexibility.\n\nUnfortunately, this length computation is naive and wasteful.  Like\n`nsum` earlier, it is not tail-recursive.  It\nuses $O(n)$ space, where $n$ is the length of its input.  As usual, the\nsolution is to add an accumulating argument.\n"},{"cell_type":"markdown","metadata":{},"source":"## Efficiently Computing the Length of a List\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec addlen = function\n  | (n, []) -> n\n  | (n, x::xs) -> addlen (n + 1, xs)","outputs":[],"execution_count":41},{"cell_type":"code","metadata":{},"source":"addlen (0, [5; 6; 7])","outputs":[],"execution_count":42},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{aligned}\naddlen(0, [a; b; c]) \\Rightarrow &  addlen(1, [b; c]) \\\\\n  \\Rightarrow  & addlen(2, [c]) \\\\\n  \\Rightarrow  & addlen(3, []) \\\\\n  \\Rightarrow  & 3\n\\end{aligned}\n$$\n\nPatterns can be as complicated as we like.  Here, the two patterns are\n`(n, [])` and `(n, x::xs)`.\n\nFunction `addlen` is again polymorphic.  Its type mentions the integer\naccumulator.\n\nNow we may declare an efficient length function.  It is simply a wrapper for\n`addlen`, supplying zero as the initial value of $n$.\n"},{"cell_type":"code","metadata":{},"source":"let length xs = addlen (0, xs)","outputs":[],"execution_count":43},{"cell_type":"code","metadata":{},"source":"length [5; 6; 7; 8]","outputs":[],"execution_count":44},{"cell_type":"markdown","metadata":{},"source":"\nThe recursive calls do not nest: this version is iterative.  It takes $O(1)$\nspace.  Obviously its time requirement is $O(n)$ because it takes at least $n$\nsteps to find the length of an $n$-element list.\n"},{"cell_type":"markdown","metadata":{},"source":"## Append: List Concatenation\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec append = function\n  | ([], ys) -> ys\n  | (x::xs, ys) -> x :: append (xs, ys)","outputs":[],"execution_count":45},{"cell_type":"code","metadata":{},"source":"append ([1; 2; 3], [4])","outputs":[],"execution_count":46},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{aligned}\nappend([1; 2; 3], [4]) \\Rightarrow & 1 :: append([2; 3], [4]) \\\\\n  \\Rightarrow & 1 :: (2 :: append([3], [4])) \\\\\n  \\Rightarrow & 1 :: (2 :: (3 :: append([], [4]))) \\\\\n  \\Rightarrow & 1 :: (2 :: (3 :: [4])) \\;; [1; 2; 3; 4]\n\\end{aligned}\n$$\n\nHere is how append might be declared, ignoring the details of how `@` is made\nan infix operator.  This function is also not iterative.  It scans its first\nargument, sets up a string of `cons` operations (`::`) and finally does them.\n\nIt uses $O(n)$ space and time, where $n$ is the length of its first argument.\n_Its costs are independent of its second argument._\n\nAn accumulating argument could make it iterative, but with considerable\ncomplication.  The iterative version would still require $O(n)$ space and time\nbecause concatenation requires copying all the elements of the first list.\nTherefore, we cannot hope for asymptotic gains; at best we can decrease the\nconstant factor involved in $O(n)$, but complicating the code is likely to\nincrease that factor.  Never add an accumulator merely out of habit.\n\nNote append’s polymorphic type. It tells us that two lists can be joined if\ntheir element types agree.\n"},{"cell_type":"markdown","metadata":{},"source":"## Reversing a List in O(n^2)\n\n\n\nLet us consider one way to reverse a list.\n"},{"cell_type":"code","metadata":{},"source":"let rec nrev = function\n  | [] -> []\n  | x::xs -> (nrev xs) @ [x]","outputs":[],"execution_count":47},{"cell_type":"code","metadata":{},"source":"nrev [1; 2; 3]","outputs":[],"execution_count":48},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{aligned}\nnrev[a; b; c] \\Rightarrow & nrev[b; c] @ [a] \\\\\n  \\Rightarrow &  (nrev[c] @ [b]) @ [a] \\\\\n  \\Rightarrow &  ((nrev[] @ [c]) @ [b]) @ [a] \\\\\n  \\Rightarrow &  (([] @ [c]) @ [b]) @ [a] \\;; \\ldots \\;; [c; b; a]\n\\end{aligned}\n$$\n\nThis reverse function is grossly inefficient due to poor usage of append, which\ncopies its first argument.  If `nrev` is given a list of length $n>0$, then\nappend makes $n-1$ conses to copy the reversed tail.  Constructing the list\n`[x]` calls `cons` again, for a total of $n$ calls.  Reversing the tail\nrequires $n-1$ more conses, and so forth.  The total number of conses is:\n\n$$ 0 + 1 + 2 + \\cdots + n = {n(n+1)/2} $$\n\nThe time complexity is therefore $O(n^2)$.  Space complexity is only $O(n)$\nbecause the copies don’t all exist at the same time.\n"},{"cell_type":"markdown","metadata":{},"source":"## Reversing a List in O(n)\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec revApp = function\n  | ([], ys) -> ys\n  | (x::xs, ys) -> revApp (xs, x::ys)","outputs":[],"execution_count":49},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{aligned}\nrevApp([a; b; c], []) \\Rightarrow & revApp([b; c], [a]) \\\\\n  \\Rightarrow & revApp([c], [b; a]) \\\\\n  \\Rightarrow & revApp([], [c; b; a]) \\\\\n  \\Rightarrow & [c; b; a]\n\\end{aligned}\n$$\n\nCalling `revApp (xs, ys)` reverses the elements of `xs` and\nprepends them to `ys`.  Now we may declare\n"},{"cell_type":"code","metadata":{},"source":"let rev xs = revApp (xs, [])","outputs":[],"execution_count":50},{"cell_type":"code","metadata":{},"source":"rev [1; 2; 3]","outputs":[],"execution_count":51},{"cell_type":"markdown","metadata":{},"source":"\nIt is easy to see that this reverse function performs just $n$ conses, given\nan $n$-element list.  For both reverse functions, we could count the number of\nconses precisely---not just up to a constant factor.  $O$ notation is still\nuseful to describe the overall running time: the time taken by a cons\nvaries from one system to another.\n\nThe accumulator $y$ makes the function iterative.  But the gain in complexity\narises from the removal of `append`.  Replacing an expensive operation (append)\nby a series of cheap operations (cons) is called _reduction in strength_\nand is a common technique in computer science.  It originated when many\ncomputers did not have a hardware multiply instruction; the series of products\n$i\\times r$ for $i=0$, $\\ldots, n$ could more efficiently be computed by\nrepeated addition.  Reduction in strength can be done in various ways; we\nshall see many instances of removing append.\n\nConsing to an accumulator produces the result in reverse.  If\nthat forces the use of an extra list reversal then the iterative function\nmay be much slower than the recursive one.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lists, Strings and Characters\n\n\n\nStrings are provided in most programming languages to allow text processing.\nStrings are essential for communication with users. Even a purely numerical\nprogram formats its results ultimately as strings.\n"},{"cell_type":"code","metadata":{},"source":"'a'   (* a character constant *)","outputs":[],"execution_count":52},{"cell_type":"code","metadata":{},"source":"\"a\"   (* a string constant of length 1 *)","outputs":[],"execution_count":53},{"cell_type":"code","metadata":{},"source":"\"abc\" (* a string constant of length 3 *)","outputs":[],"execution_count":54},{"cell_type":"code","metadata":{},"source":"String.length \"abc\"","outputs":[],"execution_count":55},{"cell_type":"code","metadata":{},"source":"\"abc\" ^ \"def\"  (* concatenate two strings *)","outputs":[],"execution_count":56},{"cell_type":"markdown","metadata":{},"source":"\nIn a few programming languages, strings simply are lists of characters, but\nthis is poor design. Strings are an abstract concept in themselves. Treating\nthem as lists leads to clumsy and inefficient code.\n\nSimilarly, characters are not strings of size one, but are a primitive concept.\nCharacter constants in OCaml have the form `'c'`, where $c$ is any character.\nFor example, the comma character is `','`.\n\nSpecial characters are coded in strings using _escape sequences_ involving the\nbackslash character; among many others, a double quote is written `\"\\\\\"` and\nthe newline character is written `\"\\n\"`. For example, the string\n`\"I\\nLIKE\\nCHEESE\\n\"` represents three text lines.\n\nIn addition to the operators described above, the relations `<` `<=` `>` and\n`>=` work for strings and yield alphabetic order (more precisely, lexicographic\norder with respect to ASCII character codes).\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 3.1\n\n\nCode a recursive function to compute the sum of a list’s elements. Then code an iterative version\nand comment on the improvement in efficiency.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 3.2\n\n\nCode a function to return the last element of a non-empty list. How efficiently can this be done?\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 3.3\n\n\nCode a function to return the list consisting of the even-numbered elements of the list given as its\nargument. For example, given `[a; b; c; d]` it should return `[b; d]`.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 3.4\n\n\nConsider the polymorphic types in these two function declarations:\n"},{"cell_type":"code","metadata":{},"source":"let id x = x","outputs":[],"execution_count":57},{"cell_type":"code","metadata":{},"source":"let rec loop x = loop x","outputs":[],"execution_count":58},{"cell_type":"markdown","metadata":{},"source":"Explain why these types make logical sense, preventing run time type errors, even for expressions\nlike `id [id [id 0]]` or `loop true / loop 3`.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exercise 3.5\n\n\nCode a function `tails` to return the list of the tails of its argument. For example, given\n`[1; 2; 3]` it should return `[[1; 2; 3]; [2; 3]; [3]; []]`.\n"},{"cell_type":"markdown","metadata":{},"source":"# Lecture 4: More on Lists\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"## List Utilities: take and drop\n\n\n\nRemoving the first $i$ elements of a list can be done as follows:\n"},{"cell_type":"code","metadata":{},"source":"let rec take = function\n  | ([], _) -> []\n  | (x::xs, i) ->\n      if i > 0 then\n        x :: take (xs, i - 1)\n      else\n        []","outputs":[],"execution_count":59},{"cell_type":"code","metadata":{},"source":"let rec drop = function\n  | ([], _) -> []\n  | (x::xs, i) ->\n      if i > 0 then\n        drop (xs, i-1)\n      else\n        x::xs","outputs":[],"execution_count":60},{"cell_type":"markdown","metadata":{},"source":"\nThis lecture examines more list utilities, illustrating more patterns of\nrecursion, and concludes with a small program for making change.\n\nThe functions `take` and `drop` divide a list\ninto parts, returning or discarding the first $i$ elements.\n\n$$\nxs = [\\underbrace{x_0,\\ldots,x_{i-1}}_{\\textstyle take(xs,i)},\n      \\underbrace{x_i,\\ldots,x_{n-1}}_{\\textstyle drop(xs,i)} ]\n$$\n\nApplications of `take` and `drop` will appear in future lectures.  Typically,\nthey divide a collection of items into equal parts for recursive processing.\n\nThe special pattern variable `_` appears in both functions.  This _wildcard\npattern_ matches anything.  We could have written `i` in both positions, but\nthe wildcard reminds us that the relevant clause ignores this argument.\n\nFunction `take` is not iterative, but making it so would not improve\nits efficiency.  The task requires copying up to $i$ list elements, which must\ntake $O(i)$ space and time.\n\nFunction `drop` simply skips over $i$ list elements.  This requires\n$O(i)$ time but only constant space.  It is iterative and much faster than\n`take`.  Both functions use $O(i)$ time, but skipping elements is faster\nthan copying them:  `drop`’s constant factor is smaller.\n\nBoth functions take a list and an integer, returning a list of the same type.\nSo their type is `'a list * int -> 'a list`.\n"},{"cell_type":"markdown","metadata":{},"source":"## Linear Search\n\n\n\n- find $x$ in list $[x_1,\\ldots,x_n]$ by comparing with each element\n- obviously $O(n)$ time\n- simple & general\n- ordered searching needs only $O(\\log n)$\n- indexed lookup needs only $O(1)$\n\n_Linear search_ is the obvious way to find a desired item in a\ncollection: simply look through all the items, one at a time.  If $x$ is in\nthe list, then it will be found in $n/2$ steps on average, and even the worst\ncase is obviously $O(n)$.\n\nLarge collections of data are usually ordered or indexed so that items can be\nfound in $O(\\log n)$ time, which is exponentially better than $O(n)$.  Even\n$O(1)$ is achievable (using a hash table), though subject to the usual\nproviso that machine limits are not exceeded.\n\nEfficient indexing methods are of prime importance: consider Web\nsearch engines.  Nevertheless, linear search is often used to search small\ncollections because it is so simple and general, and it is the starting point\nfor better algorithms.\n"},{"cell_type":"markdown","metadata":{},"source":"## Equality Tests\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec member x = function\n | [] -> false\n | y::l ->\n    if x = y then\n      true\n    else\n      member x l","outputs":[],"execution_count":61},{"cell_type":"markdown","metadata":{},"source":"\nAll the list functions we have encountered up to now have been “polymorphic”,\nworking for lists of any type.  Function `member` uses linear search to report\nwhether or not `x` occurs in `l`.\n\nTo do this generically, it uses a special feature of OCaml known as\n“polymorphic equality”, which manifests itself via the `=`, `>=`, `<=`, `>` and\n`<` operators.  These operators inspect the _structure_ of the values using a\nconsistent order.  Valid types you can compare this way include integers,\nstrings, booleans, and tuples or lists of primitive types.\n\nMore complex types can be compared this way within careful limits: recursive\nstructures or function values will not work (we will cover function values in\nthe Currying lecture later).  For now, it is sufficient to use these magic\npolymorphic equality operators.  As you get more familiar with OCaml and the\nuse of higher order functions (also covered in a later lecture), you will\nencounter the use of explicit `compare` functions that are used to provide more\ncomplex equality tests.\n\nThe presence of polymorphic equality is a contentious feature in OCaml.  While\nit provides a great ease of use in smaller codebases, it starts to become more\ndangerous when building larger OCaml-based systems.  Most large-scale users of\nOCaml tend towards not using it in important code, but it is just fine for our\npurposes while learning the beginning steps of computer science.\n\n"},{"cell_type":"markdown","metadata":{},"source":"## Building a List of Pairs\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec zip xs ys =\n  match xs, ys with\n  | (x::xs, y::ys) -> (x, y) :: zip xs ys\n  | _ -> []","outputs":[],"execution_count":62},{"cell_type":"markdown","metadata":{},"source":"\n$$ \\left.[x_1,\\ldots,x_n]\\atop\n         [y_1,\\ldots,y_n]\\right\\}\\;\\longmapsto\\;[(x_1,y_1),\\ldots,(x_n,y_n)]\n$$\n\nThe _wildcard_ pattern `_` matches _anything_. The patterns are also tested\nin order of their definitions.\n\nA list of pairs of the form $[(x_1,y_1),\\ldots,(x_n,y_n)]$ associates each\n$x_i$ with $y_i$.  Conceptually, a telephone directory could be regarded as\nsuch a list, where $x_i$ ranges over names and $y_i$ over the corresponding\ntelephone number.  Linear search in such a list can find the $y_i$ associated\nwith a given $x_i$, or vice versa---very slowly.\n\nIn other cases, the $(x_i,y_i)$ pairs might have been generated by applying a\nfunction to the elements of another list $[z_1,\\ldots,z_n]$.\n"},{"cell_type":"code","metadata":{},"source":"let rec unzip = function\n | [] -> ([], [])\n | (x, y)::pairs ->\n     let xs, ys = unzip pairs in\n     (x::xs, y::ys)","outputs":[],"execution_count":63},{"cell_type":"markdown","metadata":{},"source":"\nThe functions `zip` and `unzip` build and take apart lists of\npairs: `zip` pairs up corresponding list elements and `unzip`\ninverts this operation.  Their types reflect what they do:\n"},{"cell_type":"code","metadata":{},"source":"zip","outputs":[],"execution_count":64},{"cell_type":"code","metadata":{},"source":"unzip","outputs":[],"execution_count":65},{"cell_type":"markdown","metadata":{},"source":"\nIf the lists are of unequal length, `zip` discards surplus items at the\nend of the longer list.  Its first pattern only matches a pair of non-empty\nlists.  The second pattern is just a wildcard and could match anything.  OCaml\ntries the clauses in the order given, so the first pattern is tried first.\nThe second only gets arguments where at least one of the lists is empty.\n"},{"cell_type":"markdown","metadata":{},"source":"## Building a Pair of Results\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec unzip = function\n | [] -> ([], [])\n | (x, y)::pairs ->\n     let xs, ys = unzip pairs in\n     (x::xs, y::ys)","outputs":[],"execution_count":66},{"cell_type":"code","metadata":{},"source":"let rec revUnzip = function\n  | ([], xs, ys) -> (xs, ys)\n  | ((x, y)::pairs, xs, ys) ->\n      revUnzip (pairs, x::xs, y::ys)","outputs":[],"execution_count":67},{"cell_type":"markdown","metadata":{},"source":"\nGiven a list of pairs, `unzip` has to build _two_ lists of\nresults, which is awkward using recursion.  The version shown about uses the\n_local declaration_ `let D in E`,\nwhere $D$ consists of declarations and $E$ is the expression that can use\nthem. The let-construct counts as an expression and can be used\n(perhaps wrapped within parentheses) wherever an expression is expected.\n\nNote especially the declaration `let xs, ys = unzip pairs`\nwhich binds `xs` and `ys` to the results of the recursive call.\nIn general, the declaration `let P = E` matches the\npattern $P$ against the value of expression $E$.  It binds all the variables\nin $P$ to the corresponding values.\n\nHere is a version of `unzip` that replaces the local declaration by a\nfunction `conspair` for taking apart the pair of lists in the\nrecursive call.  It defines the same\ncomputation as the previous version of `unzip` and is possibly clearer,\nbut not every local declaration can be eliminated as easily.\n"},{"cell_type":"code","metadata":{},"source":"let conspair ((x, y), (xs, ys)) = (x::xs, y::ys)","outputs":[],"execution_count":68},{"cell_type":"code","metadata":{},"source":"let rec unzip = function\n  | [] -> ([], [])\n  | xy :: pairs -> conspair (xy, unzip pairs)","outputs":[],"execution_count":69},{"cell_type":"markdown","metadata":{},"source":"\nMaking the function iterative yields `revUnzip` above, which is\nvery simple.  Iteration can construct many results at once in different\nargument positions.  Both output lists are built in reverse order, which can\nbe corrected by reversing the input to `revUnzip`.  The total costs\nwill probably exceed those of `unzip` despite the advantages of\niteration.\n"},{"cell_type":"markdown","metadata":{},"source":"## An Application: Making Change\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec change till amt =\n  if amt = 0 then\n    []\n  else\n    match till with\n    | [] -> raise (Failure \"no more coins!\")\n    | c::till ->\n        if amt < c then\n          change till amt\n        else\n          c :: change (c::till) (amt - c)","outputs":[],"execution_count":70},{"cell_type":"markdown","metadata":{},"source":"\n- The recursion _terminates_ when `amt = 0`.\n- Tries the _largest coin first_ to use large coins.\n- The algorithm is _greedy_ and can fail!\n\nThe till has unlimited supplies of coins.  The largest coins should be tried\nfirst, to avoid giving change all in pennies.  The list of legal coin values,\ncalled `till`, is given in descending order, such as 50, 20, 10, 5,\n2 and 1.  (Recall that the head of a list is the element most easily reached.)\nThe code for `change` is based on simple observations:\n\n- Change for zero consists of no coins at all.  (Note the pattern of `0` in the first clause.)\n- For a nonzero amount, try the largest available coin.  If it is small enough, use it and decrease the amount accordingly.\n- Exclude from consideration any coins that are too large.\n\nAlthough nobody considers making change for zero, this is the simplest way to\nmake the algorithm terminate.  Most iterative procedures become simplest if,\nin their base case, they do nothing.  A base case of one instead of zero is\noften a sign of a novice programmer.\n\nThe function can terminate either with success or failure.  It fails by\nraising exception `Failure` namely if `till` becomes empty while `amt` is still nonzero.\n(Exceptions will be discussed later.)\n\nUnfortunately, failure can occur even when change can be made.  The greedy\n‘largest coin first’ approach is to blame.  Suppose we have coins of values 5\nand 2, and must make change for 6; the only way is $6=2+2+2$, ignoring the 5.\n_Greedy algorithms_ are often effective, but not here.\n"},{"cell_type":"markdown","metadata":{},"source":"## All Ways of Making Change\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec change till amt =\n  if amt = 0 then\n    [ [] ]\n  else\n    match till with\n    | [] -> []\n    | c::till ->\n        if amt < c then\n          change till amt\n        else\n          let rec allc = function\n            | [] -> []\n            | cs :: css -> (c::cs) :: allc css\n          in\n            allc (change (c::till) (amt - c)) @\n                  change till amt","outputs":[],"execution_count":71},{"cell_type":"markdown","metadata":{},"source":"\nNow we generalise the problem to return the list of _all possible ways_ of making change.\nLook at the type: the result is now a list of lists.\n\nThe code will never raise exceptions.  It expresses failure by returning an\nempty list of solutions: it returns `[]` if the till is empty and the\namount is nonzero.\n\nIf the amount is zero, then there is only one way of making change;\nthe result should be `[[]]`.  This is success in the base case.\n\nIn nontrivial cases, there are two sources of solutions: to use a coin (if\npossible) and decrease the amount accordingly, or to remove the current coin\nvalue from consideration.\n\nThe function `allc` is declared locally in order to make use\nof `c`, the current coin.  It adds an extra `c` to all the\nsolutions returned by the recursive call to make change for `amt - c`.\n\nObserve the naming convention: `cs` is a list of coins, while\n`css` is a list of such lists.  The trailing `s' is suggestive of a\nplural.\n\nThis complicated program, and the even trickier one on the next slide, are\nincluded as challenges.  Are you enthusiastic enough to work them out?  We\nshall revisit the “making change” task later to illustrate exception-handling.\n"},{"cell_type":"markdown","metadata":{},"source":"## All Ways of Making Change --- Faster!\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec change till amt chg chgs =\n  if amt = 0 then\n    chg::chgs\n  else\n    match till with\n    | []  -> chgs\n    | c::till ->\n        if amt < 0 then\n          chgs\n        else\n          change (c::till) (amt - c) (c::chg)\n                 (change till amt chg chgs)","outputs":[],"execution_count":72},{"cell_type":"markdown","metadata":{},"source":"We’ve added _another_ accumulating parameter!  Repeatedly improving simple code\nis called _stepwise refinement_.\n\nTwo extra arguments eliminate many `::` and append operations from the previous\nslide’s change function.  The first, `chg`, accumulates the coins chosen so\nfar; one evaluation of c::chg} replaces many evaluations of `allc`.  The\nsecond, `chgs`, accumulates the list of solutions so far; it avoids the need\nfor append.  This version runs several times faster than the previous one.\n\nMaking change is still extremely slow for an obvious reason: the number of\nsolutions grows rapidly in the amount being changed.  Using 50, 20, 10, 5,\n2 and 1, there are 4366 ways of expressing 99.\n \nOur three change functions illustrate a basic technique: program development\nby stepwise refinement.  Begin by writing a very simple program and add\nrequirements individually.  Add efficiency refinements last of all.\nEven if the simpler program cannot be included in the next version and has\nto be discarded, one has learned about the task by writing it.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 4.1\n\n\nCode a function to implement set union, by analogy with `inter` above. It should avoid introducing\nrepetitions, for example the union of the lists `[4; 7; 1]` and `[6; 4; 7]` should be `[1; 6; 4; 7]`\n(though the order does not matter).\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 4.2\n\n\nCode a function that takes a list of integers and returns two lists, the first consisting of all\nnon-negative numbers found in the input and the second consisting of all the negative numbers.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 4.3\n\n\nHow does this version of `zip` differ from the one above?"},{"cell_type":"code","metadata":{},"source":"let rec zip xs ys =\n  match xs, ys with\n  | (x::xs, y::ys) -> (x, y) :: zip xs ys\n  | ([], [])   -> []","outputs":[],"execution_count":73},{"cell_type":"markdown","metadata":{},"source":"## Exercise 4.4\n\n\nWhat assumptions do the ‘making change’ functions make about the variables `till` and `amt`?\nDescribe what could happen if these assumptions were violated.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 4.5\n\n\nShow that the number of ways of making change for $n$ (ignoring order) is $O(n)$ if there are two\nlegal coin values. What if there are three, four, … coin values?\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 4.6\n\n\nWe know nothing about the functions `f` and `g` other than their polymorphic types:"},{"cell_type":"code","metadata":{},"source":"val f : 'a * 'b -> 'b * 'a = <fun>\nval g : 'a -> 'a list = <fun>","outputs":[],"execution_count":74},{"cell_type":"markdown","metadata":{},"source":"Suppose that `f (1, true)` and `g 0` are evaluated and return their results. State, with reasons,\nwhat you think the resulting values will be.\n"},{"cell_type":"markdown","metadata":{},"source":"# Lecture 5: Sorting\n\n\n\nA few applications for sorting and arranging items into order are:\n\n- search\n- merging\n- duplicates\n- inverting tables\n- graphics algorithms\n\nSorting is perhaps the most deeply studied aspect of algorithm design.\nKnuth’s series _The Art of Computer Programming_ devotes an entire\nvolume to sorting and searching!  [Sedgewick](https://algs4.cs.princeton.edu/home/)\nalso covers sorting.  Sorting has countless applications.\n\nSorting a collection allows items to be found quickly.  Recall that linear\nsearch requires $O(n)$ steps to search among $n$ items.  A sorted collection\nadmits _binary search_ which requires only $O(\\log n)$ time.  The idea\nof binary search is to compare the item being sought with the middle item (in\nposition $n/2$) and then to discard either the left half or the right,\ndepending on the result of the comparison.  Binary search needs arrays or\ntrees, not lists; we shall come to binary search trees later.\n\nTwo sorted files can quickly be _merged_ to form a larger sorted file.  Other\napplications include finding _duplicates_ that, after sorting, are adjacent.\n\nA telephone directory is sorted alphabetically by name.  The same information\ncan instead be sorted by telephone number (useful to the police) or by street\naddress (useful to junk-mail firms).  Sorting information in different ways\ngives it different applications.\n\nCommon sorting algorithms include insertion sort, quicksort,\nmergesort and heapsort.  We shall consider the first three of\nthese.  Each algorithm has its advantages.\n\nAs a concrete basis for comparison, runtimes are quoted for DECstation\ncomputers.  (These were based on the MIPS chip, an early RISC design.)\n"},{"cell_type":"markdown","metadata":{},"source":"## How Fast Can We Sort?\n\n\n\n- typically count _comparisons_ $C(n)$\n- there are $n!$ permutations of $n$ elements\n- each comparison eliminates _half_ of the permutations $2^{C(n)}\\geq n!$\n- therefore $C(n)\\geq \\log(n!)\\approx n\\log n-1.44n$\n\nThe usual measure of efficiency for sorting algorithms is the number of\ncomparison operations required.  Mergesort requires only $O(n\\log n)$\ncomparisons to sort an input of $n$ items.  It is straightforward to prove\nthat this complexity is the best possible.  There\nare $n!$ permutations of $n$ elements and each comparison distinguishes two\npermutations.  The lower bound on the number of comparisons, $C(n)$, is\nobtained by solving $2^{C(n)}\\geq n!$; therefore $C(n)\\geq \\log(n!)\\approx\nn\\log n-1.44n$.\n\nIn order to compare the sorting algorithms, we use the [following source](http://www.firstpr.com.au/dsp/rand31/p1192-park.pdf) of\npseudo-random numbers. Never mind how this works: generating\nstatistically good random numbers is hard.  Much effort has gone into those few\nlines of code.\n"},{"cell_type":"code","metadata":{},"source":"let nextrandom seed =\n  let a = 16807.0 in\n  let m = 2147483647.0 in\n  let t = a *. seed in\n  t -. m *. (floor (t /. m))","outputs":[],"execution_count":75},{"cell_type":"code","metadata":{},"source":"let rec randlist (seed, seeds) = function\n  | 0 -> (seed, seeds)\n  | n -> randlist (nextrandom seed, seed::seeds) (n-1)","outputs":[],"execution_count":76},{"cell_type":"markdown","metadata":{},"source":"\nWe can now bind the identifier `rs` to a list of 10,000 random numbers.\n"},{"cell_type":"code","metadata":{},"source":"let seed, rs = randlist (1.0, []) 10000","outputs":[],"execution_count":77},{"cell_type":"markdown","metadata":{},"source":"## Insertion Sort\n\n\n\nAn insert operation does does $n/2$ comparisons on average.\n"},{"cell_type":"code","metadata":{},"source":"let rec ins = function\n  | x, [] -> [x]\n  | x, y::ys ->\n      if x <= y then\n        x :: y :: ys\n      else\n        y :: ins (x, ys)","outputs":[],"execution_count":78},{"cell_type":"markdown","metadata":{},"source":"\n_Insertion sort_ takes $O(n^2)$ comparisons on average:\n"},{"cell_type":"code","metadata":{},"source":"let rec insort = function\n    | [] -> []\n    | x::xs -> ins (x, insort xs)","outputs":[],"execution_count":79},{"cell_type":"markdown","metadata":{},"source":"\nItems from the input are copied one at a time to the output.  Each new item is\ninserted into the right place so that the output is always in order.\n\nWe could easily write iterative versions of these functions, but to no purpose.\nInsertion sort is slow because it does $O(n^2)$ comparisons (and a lot of list\ncopying), not because it is recursive.  Its quadratic runtime makes it nearly\nuseless: it takes 174 seconds for our example while the next-worst figure is\n1.4 seconds.\n\nInsertion sort is worth considering because it is easy to code and illustrates\nthe concepts.  Two efficient sorting algorithms, mergesort and heapsort, can be\nregarded as refinements of insertion sort.\n"},{"cell_type":"markdown","metadata":{},"source":"## Quicksort: The Idea\n\n\n\nThe Quicksort algorithm has the following flow:\n\n- Choose a _pivot_ element, $a$\n- Divide to partition the input into two sublists:\n  * those _at most_ $a$ in value\n  * those _exceeding_ $a$\n- Conquer using recursive calls to sort the sublists\n- Combine the sorted lists by appending one to the other\n\nQuicksort was invented by C. A. R. Hoare, who now works at Microsoft Research,\nCambridge.  Quicksort works by _divide and conquer,_ a basic algorithm design\nprinciple.  Quicksort chooses from the input some value $a$, called the\n_pivot_.  It partitions the remaining items into two parts: those $\\leq a$, and\nthose $>a$.  It sorts each part recursively, then puts the smaller part before\nthe greater.\n\nThe cleverest feature of Hoare’s algorithm was that the partition could be done\n_in place_ by exchanging array elements.  Quicksort was invented before\nrecursion was well known, and people found it extremely hard to understand.  As\nusual, we shall consider a list version based on functional programming.\n"},{"cell_type":"markdown","metadata":{},"source":"## Quicksort: The Code\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec quick = function\n  | [] -> []\n  | [x] -> [x]\n  | a::bs ->\n      let rec part = function\n        | (l, r, []) -> (quick l) @ (a :: quick r)\n        | (l, r, x::xs) ->\n            if (x <= a) then\n              part (x::l, r, xs)\n            else\n              part (l, x::r, xs)\n      in\n      part ([], [], bs)","outputs":[],"execution_count":80},{"cell_type":"markdown","metadata":{},"source":"\nOur OCaml quicksort copies the items.  It is still pretty fast, and it is much\neasier to understand.  It takes roughly 0.74 seconds to sort our list of random\nnumbers.\n\nThe function declaration consists of three clauses.  The first handles the\nempty list; the second handles singleton lists (those of the form `[x]`; the\nthird handles lists of two or more elements.  Often, lists of length up to five\nor so are treated as special cases to boost speed.\n\nThe locally declared function `part` partitions the input using `a` as the\npivot.  The arguments `l` and `r` accumulate items for the left ($\\leq a$) and\nright ($>a$) parts of the input, respectively.\n\nIt is not hard to prove that quicksort does $n\\log n$ comparisons, _in the average case_\n(see [page 94 of Aho](https://archive.org/details/designanalysisof00ahoarich)).  With random data, the pivot\nusually has an average value that divides the input in two approximately equal\nparts.  We have the recurrence $T(1) = 1$ and $T(n) = 2T(n/2)+n$, which is\n$O(n\\log n)$.  In our example, it is about 235 times faster than insertion\nsort.\n\nIn the worst case, quicksort’s running time is quadratic!  An example is when\nits input is almost sorted or reverse sorted.  Nearly all of the items end up\nin one partition; work is not divided evenly.  We have the recurrence\n$T(1) = 1$ and $T(n+1) = T(n)+n$, which is $O(n^2)$.  Randomising the input\nmakes the worst case highly unlikely.\n"},{"cell_type":"markdown","metadata":{},"source":"## Append-Free Quicksort\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec quik = function\n  | ([], sorted) -> sorted\n  | ([x], sorted) -> x::sorted\n  | a::bs, sorted ->\n     let rec part = function\n       | l, r, [] -> quik (l, a :: quik (r, sorted))\n       | l, r, x::xs ->\n           if x <= a then\n             part (x::l, r, xs)\n           else\n             part (l, x::r, xs)\n     in\n     part ([], [], bs)","outputs":[],"execution_count":81},{"cell_type":"markdown","metadata":{},"source":"\nThe list `sorted` accumulates the result in the _combine_ stage of\nthe quicksort algorithm.  We have again used the standard technique for\neliminating append.  Calling `quik(xs, sorted)` reverses the elements of\n`xs` and prepends them to the list `sorted`.\n\nLooking closely at `part`, observe that `quik(r, sorted)` is\nperformed first.  Then `a` is consed to this sorted list.  Finally,\n`quik` is called again to sort the elements of `l`.\n\nThe speedup is significant.  An imperative quicksort coded in Pascal (taken\nfrom [Sedgewick](https://algs4.cs.princeton.edu/20sorting/)) is just slightly faster than function\n`quik`.  The near-agreement is surprising because the computational overheads\nof lists exceed those of arrays.  In realistic applications, comparisons are\nthe dominant cost and the overheads matter even less.\n"},{"cell_type":"markdown","metadata":{},"source":"## Merging Two Lists\n\n\n\nMerge joins two sorted lists.\n"},{"cell_type":"code","metadata":{},"source":"let rec merge = function\n  | [], ys -> ys\n  | xs, [] -> xs\n  | x::xs, y::ys ->\n      if x <= y then\n        x :: merge (xs, y::ys)\n      else\n        y :: merge (x::xs, ys)","outputs":[],"execution_count":82},{"cell_type":"markdown","metadata":{},"source":"\nGeneralises insert to two lists, and does at most $m+n-1$ comparisons.\n\n_Merging_ means combining two sorted lists to form a larger sorted list.\nIt does at most $m+n$ comparisons, where $m$ and $n$ are the lengths of the\ninput lists.  If $m$ and $n$ are roughly equal then we have a fast way of\nconstructing sorted lists; if $n=1$ then merging degenerates to insertion,\ndoing much work for little gain.\n\nMerging is the basis of several sorting algorithms; we look at a\ndivide-and-conquer one.  Mergesort is seldom found in conventional programming\nbecause it is hard to code for arrays; it works nicely with lists.  It divides\nthe input (if non-trivial) into two roughly equal parts, sorts them\nrecursively, then merges them.\n\nFunction `merge` is not iterative; the recursion is deep.  An iterative\nversion is of little benefit for the same reasons that apply to\n`append` in the earlier lecture on Lists.\n"},{"cell_type":"markdown","metadata":{},"source":"## Top-down Merge sort\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec tmergesort = function\n  | [] -> []\n  | [x] -> [x]\n  | xs ->\n      let k = List.length xs / 2 in\n      let l = tmergesort (take (xs, k)) in\n      let r = tmergesort (drop (xs, k)) in\n      merge (l, r)","outputs":[],"execution_count":83},{"cell_type":"markdown","metadata":{},"source":"\n$O(n\\log n)$ comparisons in worst case\n\nMergesort’s _divide_ stage divides the input not by choosing a pivot (as\nin quicksort) but by simply counting out half of the elements.  The\n_conquer_ stage again involves recursive calls, and the _combine_\nstage involves merging.  Function `tmergesort` takes roughly 1.4\nseconds to sort the list `rs`.\n\nIn the worst case, mergesort does $O(n\\log n)$ comparisons, with the same\nrecurrence equation as in quicksort’s average case.  Because `take` and\n`drop` divide the input in two equal parts (they differ at most by\none element), we always have $T(n) = 2T(n/2)+n$.\n\nQuicksort is nearly 3 times as fast in the example.  But it risks a\nquadratic worst case!  Merge sort is safe but slow.  So which algorithm is\nbest?\n\nWe have seen a _top-down_ mergesort.  _Bottom-up_ algorithms also\nexist.  They start with a list of one-element lists and repeatedly merge\nadjacent lists until only one is left.  A refinement, which exploits any\ninitial order among the input, is to start with a list of increasing or\ndecreasing runs of input items.\n"},{"cell_type":"markdown","metadata":{},"source":"## Summary of Sorting Algorithms\n\n\n\n- Optimal is $O(n\\log n)$ comparisons\n- Insertion sort: simple to code; too slow (quadratic) [174 secs]\n- Quicksort: fast on average; quadratic in worst case [0.53 secs]\n- Mergesort: optimal in theory; often slower than quicksort [1.4 secs]\n- _Match the algorithm to the application_\n\nQuicksort’s worst case cannot be ignored.  For large $n$, a complexity of\n$O(n^2)$ is catastrophic.  Mergesort has an $O(n\\log n)$ worst case running\ntime, which is optimal, but it is typically slower than quicksort for random\ndata.\n\nNon-comparison sorting deserves mentioning.  We can sort a large number of\nsmall integers using their radix representation in $O(n)$ time.  This result\ndoes not contradict the comparison-counting argument because comparisons are\nnot used at all.  Linear time is achievable only if the greatest integer is\nfixed in advance; as $n$ goes to infinity, increasingly many of the items\nare the same.  It is a simple special case.\n\nMany other sorting algorithms exist. A few are outlined in the exercises.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 5.1\n\n\nAnother sorting algorithm (selection sort) consists of looking at the elements to be sorted,\nidentifying and removing a minimal element, which is placed at the head of the result. The tail is\nobtained by recursively sorting the remaining elements. State, with justification, the time\ncomplexity of this approach.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 5.2\n\n\nImplement selection sort (see previous exercise) using OCaml.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 5.3\n\n\nAnother sorting algorithm (bubble sort) consists of looking at adjacent pairs of elements,\nexchanging them if they are out of order and repeating this process until no more exchanges are\npossible. State, with justification, the time complexity of this approach.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 5.4\n\n\nImplement bubble sort (see previous exercise) using OCaml.\n\n# Lecture 6: Datatypes and Trees\n"},{"cell_type":"markdown","metadata":{},"source":"## An Enumeration Type\n\n\n\n"},{"cell_type":"code","metadata":{},"source":"type vehicle = Bike\n             | Motorbike\n             | Car\n             | Lorry","outputs":[],"execution_count":84},{"cell_type":"markdown","metadata":{},"source":"\n- We have declared a _new type_ named `vehicle`.\n- $\\ldots$ along with four new constants.\n- They are the _constructors_ of the datatype.\n\nThe `type` declaration adds a new type to our OCaml session.  Type\n`vehicle` is as good as any built-in type and even admits\npattern-matching.  The four new identifiers of type `vehicle` are\ncalled _constructors_.\n\nWe could represent the various vehicles by the numbers 0--3.  However, the code would be\nhard to read and even harder to maintain.  Consider adding `Tricycle`\nas a new vehicle. If we wanted to add it before `Bike`, then all the\nnumbers would have to be changed.  Using `type`, such additions are\ntrivial and the compiler can (at least sometimes) warn us when it encounters a\nfunction declaration that doesn’t yet have a case for `Tricycle`.\n\nRepresenting vehicles by strings like `\"Bike\"`, `\"Car\"`, etc.,\nis also bad.  Comparing string values is slow and the compiler\ncan’t warn us of misspellings like `\"MOtorbike\"`: they will make our\ncode fail.\n\nMost programming languages allow the declaration of types like\n`vehicle`.  Because they consist of a series of identifiers, they are\ncalled _enumeration types_.  Other common examples are days of the week\nor colours.  The compiler chooses the integers for us; type-checking prevents\nus from confusing `Bike` with `Red` or `Sunday`.\n"},{"cell_type":"markdown","metadata":{},"source":"## Declaring a Function on Vehicles\n\n\n"},{"cell_type":"code","metadata":{},"source":"let wheels = function\n| Bike -> 2\n| Motorbike -> 2\n| Car -> 4\n| Lorry -> 18","outputs":[],"execution_count":85},{"cell_type":"markdown","metadata":{},"source":"\n- Datatype constructors can be used in patterns.\n- Pattern-matching is fast, even complicated nested patterns.\n\nThe beauty of datatype declarations is that the new types behave as if they\nwere built into OCaml. Type-checking catches common errors, such as mixing up\ndifferent datatypes in a function like `wheels`, as well as missing\nand redundant patterns.\n"},{"cell_type":"markdown","metadata":{},"source":"## A Datatype with Constructor Functions\n\n\n"},{"cell_type":"code","metadata":{},"source":"type vehicle = Bike\n             | Motorbike of int\n             | Car       of bool\n             | Lorry     of int","outputs":[],"execution_count":86},{"cell_type":"markdown","metadata":{},"source":"\n- Constructor functions (like `Lorry`) make _distinct values_.\n- Different kinds of `vehicle` can belong to one list: `[Bike, Car true, Motorbike 450]`\n\nOCaml generalises the notion of enumeration type to allow data to be associated\nwith each constructor.  The constructor `Bike` is a vehicle all by itself, but\nthe other three constructors are functions for creating vehicles.\n\nSince we might find it hard to remember what the various `int` and\n`bool` components are for, it is wise to include _comments_ in\ncomplex declarations.  In OCaml, comments are enclosed in the brackets\n`(*` and `*)`.  Programmers should comment their code to explain\ndesign decisions and key features of the algorithms (sometimes by citing a\nreference work).\n"},{"cell_type":"code","metadata":{},"source":"type vehicle = Bike\n             | Motorbike of int  (* engine size in CCs *)\n             | Car       of bool (* true if a Reliant Robin *)\n             | Lorry     of int  (* number of wheels *)","outputs":[],"execution_count":87},{"cell_type":"markdown","metadata":{},"source":"The list shown on the slide represents a bicycle, a Reliant Robin and a large\nmotorbike.  It can be almost seen as a mixed-type list containing integers and\nbooleans.  It is actually a list of vehicles; datatypes lessen the impact of\nthe restriction that all list elements must have the same type.\n"},{"cell_type":"markdown","metadata":{},"source":"## A Finer Wheel Computation\n\n\n"},{"cell_type":"code","metadata":{},"source":"let wheels = function\n| Bike -> 2\n| Motorbike _ -> 2\n| Car robin -> if robin then 3 else 4\n| Lorry w -> w","outputs":[],"execution_count":88},{"cell_type":"markdown","metadata":{},"source":"\nThis function consists of four clauses:\n- A Bike has two wheels.\n- A Motorbike has two wheels.\n- A Reliant Robin has three wheels; all other cars have four.\n- A Lorry has the number of wheels stored with its constructor.\n\nThere is no overlap between the `Motorbike` and `Lorry` cases.  Although\n`Motorbike` and `Lorry` both hold an integer, OCaml takes the\nconstructor into account. A Motorbike is distinct from any Lorry.\n\nVehicles are one example of a concept consisting of several varieties with\ndistinct features.  Most programming languages can represent such concepts\nusing something analogous to datatypes.  (They are sometimes called\n_union types_ or _variant records_ whose _tag fields_ play the\nrole of the constructors.)\n\n\nA pattern may be built from the constructors of several datatypes, including\nlists. A pattern may also contain integer and string constants. There is no\nlimit to the size of patterns or the number of clauses in a function\ndeclaration. OCaml performs pattern-matching [efficiently](https://dl.acm.org/citation.cfm?id=507641)\n(you do not need to understand the details of how it optimises them at this stage).\n"},{"cell_type":"markdown","metadata":{},"source":"## Error Handling: Exceptions\n\n\n\nDuring a computation, what happens if something goes _wrong?_\n- Division by zero\n- Pattern matching failure\n\n_Exception-handling_ lets us recover gracefully.\n- Raising an exception abandons the current computation.\n- Handling the exception attempts an alternative computation.\n- The raising and handling can be far apart in the code.\n- Errors of _different sorts_ can be handled separately.\n\nExceptions are necessary because it is not always possible to tell in advance\nwhether or not a search will lead to a dead end or whether a numerical\ncalculation will encounter errors such as overflow or divide by zero. Rather\nthan just crashing, programs should check whether things have gone wrong, and\nperhaps attempt an alternative computation (perhaps using a different algorithm\nor higher precision). A number of modern languages provide exception handling.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exceptions in OCaml\n\n\n"},{"cell_type":"code","metadata":{},"source":"exception Failure","outputs":[],"execution_count":89},{"cell_type":"code","metadata":{},"source":"exception NoChange of int","outputs":[],"execution_count":90},{"cell_type":"code","metadata":{},"source":"raise Failure","outputs":[],"execution_count":91},{"cell_type":"code","metadata":{},"source":"try\n  print_endline \"pre exception\";\n  raise (NoChange 1);\n  print_endline \"post exception\";\nwith\n  | NoChange _ ->\n      print_endline \"handled a NoChange exception\"","outputs":[],"execution_count":92},{"cell_type":"markdown","metadata":{},"source":"\nEach `exception` declaration introduces a distinct sort of exception, which can\nbe handled separately from others. If $E$ raises an exception, then its\nevaluation has failed; _handling_ an exception means evaluating another\nexpression and returning its value instead. One exception handler can specify\nseparate expressions for different sorts of exceptions.\n\nException names are _constructors_ of the special datatype `exn`.  This is a\npeculiarity of OCaml that lets exception-handlers use pattern-matching. Note that\nexception `Failure` is just an error indication, while `NoChange n` carries\nfurther information: the integer $n$.\n\nThe effect of `raise <expr>` is to jump to the most recently-encountered\nhandler that matches `<expr>`.  The matching handler can only be found\n_dynamically_ (during execution); contrast with how OCaml associates occurrences\nof identifiers with their matching declarations, which does not require running\nthe program.\n\nOne criticism of OCaml’s exceptions is that---unlike the Java language---nothing\nin a function declaration indicates which exceptions it might raise. One\nalternative to exceptions is to instead return a value of datatype `option`.\n"},{"cell_type":"code","metadata":{},"source":"let x = Some 1","outputs":[],"execution_count":93},{"cell_type":"code","metadata":{},"source":"let y = None","outputs":[],"execution_count":94},{"cell_type":"code","metadata":{},"source":"type 'a option = None | Some of 'a","outputs":[],"execution_count":95},{"cell_type":"markdown","metadata":{},"source":"\n`None` signifies an error, while `Some x` returns the solution $x$.  This\napproach looks clean, but the drawback is that many places in the code would\nhave to check for `None`.  Despite this, there is a builtin `option` type\nin OCaml as it is so useful.\n"},{"cell_type":"markdown","metadata":{},"source":"## Making Change with Exceptions\n\n\n"},{"cell_type":"code","metadata":{},"source":"exception Change\nlet rec change till amt =\n  if amt = 0 then\n    []\n  else\n    match till with\n    | [] ->\n        raise Change\n    | c::till ->\n        if amt < 0 then\n          raise Change\n        else\n          try\n             c :: change (c::till) (amt - c)\n          with Change ->\n             change till amt","outputs":[],"execution_count":96},{"cell_type":"markdown","metadata":{},"source":"\nIn the Lists lectures, we considered the problem of making change.  The greedy\nalgorithm presented there could not express “6 using 5 and 2” because it always\ntook the largest coin.  Returning the list of all possible solutions avoids\nthat problem rather expensively: we only need one solution.\n\nUsing exceptions, we can code a _backtracking_ algorithm: one that can undo\npast decisions if it comes to a dead end.  The exception `Change` is raised if\nwe run out of coins (with a non-zero amount) or if the amount goes negative.\nWe always try the largest coin, but enclose the recursive call in an exception\nhandler, which undoes the choice if it goes wrong.\n\nCarefully observe how exceptions interact with recursion.  The exception\nhandler always undoes the _most recent_ choice, leaving others possibly to\nbe undone later.  If making change really is impossible, then eventually\n`exception Change` will be raised with no handler to catch it, and it\nwill be reported at top level.\n"},{"cell_type":"markdown","metadata":{},"source":"## Making Change: A Trace\n\n\n\nHere is the full execution. Observe how the exception handlers nest and how\nthey drop away once the given expression has returned a value.\n"},{"cell_type":"raw","metadata":{},"source":"change [5; 2] 6\n  ⇒ 5::change [5; 2] 1 with C -> change [2] 6\n  ⇒ 5::(5::change [5; 2] -4) with C -> change [2] 1\n                             with C -> change [2] 6\n  ⇒ 5::(change [2] 1) with C -> change [2] 6\n  ⇒ 5::(2::change [2] -1) with Chang -> change [] 1\n                          with C -> change [2] 6\n  ⇒ 5::(change [] 1) with C -> change [2] 6\n  ⇒ change [2] 6\n  ⇒ 2::(change [2] 4) with C -> change [] 6\n  ⇒ 2::(2::change [2] 2) with C -> change [] 4\n                         with C -> change [] 6\n  ⇒ 2::(2::(2::change [2] 0)) with C -> change [] 2\n                              with C -> change [] 4\n                              with C -> change [] 6\n  ⇒ 2::(2::[2]) with C -> change [] 4\n                with C -> change [] 6\n  ⇒ 2::[2; 2] with C -> change [] 6\n  ⇒ [2; 2; 2]"},{"cell_type":"markdown","metadata":{},"source":"## Binary Trees, a Recursive Datatype\n\n\n"},{"cell_type":"code","metadata":{},"source":"type 'a tree =\n  Lf\n| Br of 'a * 'a tree * 'a tree","outputs":[],"execution_count":97},{"cell_type":"markdown","metadata":{},"source":"\n![](bintree.png)\n"},{"cell_type":"code","metadata":{},"source":"Br(1, Br(2, Br(4, Lf, Lf),\n            Br(5, Lf, Lf)),\n      Br(3, Lf, Lf))","outputs":[],"execution_count":98},{"cell_type":"markdown","metadata":{},"source":"\nA data structure with multiple branching is called a “tree”.  Trees can\nrepresent mathematical expressions, logical formulae, computer programs, the\nphrase structure of English sentences, etc.\n\n_Binary trees_ are nearly as fundamental as lists.  They can provide\nefficient storage and retrieval of information.  In a binary tree, each node\nis empty ($Lf$), or is a branch ($Br$) with a label and two subtrees.\n\nOCaml lists are a datatype and could be declared as follows:\n"},{"cell_type":"code","metadata":{},"source":"type 'a mylist =\n| Nil\n| Cons of 'a * 'a mylist","outputs":[],"execution_count":99},{"cell_type":"markdown","metadata":{},"source":"\nWe could even declare `::` as an infix constructor.  The only\nthing we could not define is the `[...]` notation, which is\npart of the OCaml grammar (although there does exist a mechanism\nto use a _similar_ syntax for custom indexed datatypes).\n\nA recursive type does not have to be polymorphic.\nFor example, here is a simple datatype of tree shapes with no attached data\nthat is recursive but not polymorphic.\n"},{"cell_type":"code","metadata":{},"source":"type shape =\n| Null\n| Join of shape * shape","outputs":[],"execution_count":100},{"cell_type":"markdown","metadata":{},"source":"\nThe datatype `'a option` (mentioned above) is the opposite -- it is\npolymorphic, but not recursive.\n"},{"cell_type":"markdown","metadata":{},"source":"## Basic Properties of Binary Trees\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec count = function\n| Lf -> 0  (* number of branch nodes *)\n| Br (v, t1, t2) -> 1 + count t1 + count t2","outputs":[],"execution_count":101},{"cell_type":"code","metadata":{},"source":"let rec depth = function\n| Lf -> 0  (* length of longest path *)\n| Br (v, t1, t2) -> 1 + max (depth t1) (depth t2)","outputs":[],"execution_count":102},{"cell_type":"markdown","metadata":{},"source":"\nThe invariant $\\texttt{count}(t)\\le 2^{\\texttt{depth}(t)} - 1$ holds in the functions above.\n\nFunctions on trees are expressed recursively using pattern-matching.  Both\nfunctions above are analogous to \\texttt{length} on lists.  Here is a third\nmeasure of a tree’s size:\n"},{"cell_type":"code","metadata":{},"source":"let rec leaves = function\n| Lf -> 1\n| Br (v, t1, t2) -> leaves t1 + leaves t2","outputs":[],"execution_count":103},{"cell_type":"markdown","metadata":{},"source":"\nThis function is redundant because of a basic fact about trees, which can be\nproved by induction: for every tree $t$, we have $\\texttt{leaves}(t) =\n\\texttt{count}(t)+1$.  The inequality shown on the slide also has an elementary\nproof by induction.\n\nA tree of depth 20 can store $2^{20}-1$ or approximately one million elements.\nThe access paths to these elements are short, particularly when compared with\na million-element list!\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 6.1\n\n\nGive the declaration of an OCaml type for the days of the week. Comment on the practicality of such\na type in a calendar application.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 6.2\n\n\nWrite an OCaml function taking a binary tree labelled with integers and returning their sum.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 6.3\n\n\nUsing the definition of `'a tree` from before:"},{"cell_type":"code","metadata":{},"source":"type 'a tree = Lf | Br of 'a * 'a tree * 'a tree","outputs":[],"execution_count":104},{"cell_type":"markdown","metadata":{},"source":"Examine the following function declaration. What does `ftree (1, n)` accomplish?"},{"cell_type":"code","metadata":{},"source":"let rec ftree k n =\n  if n = 0 then\n    Lf\n  else\n    Br (k, ftree (2 * k) (n - 1), ftree (2 * k + 1) (n - 1))","outputs":[],"execution_count":105},{"cell_type":"markdown","metadata":{},"source":"## Exercise 6.4\n\n\nGive the declaration of an OCaml type for arithmetic expressions that have the following\npossibilities: real numbers, variables (represented by strings), or expressions of the form $-E$,\n$E+E$, $E\\times E$.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 6.5\n\n\nContinuing the previous exercise, write a function that evaluates an expression. If the expression\ncontains any variables, your function should raise an exception indicating the variable name.\n"},{"cell_type":"markdown","metadata":{},"source":"# Lecture 7: Dictionaries and Functional Arrays\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"## Dictionaries\n\n\n\n- lookup: find an item in the dictionary\n- update (insert): replace (store) an item in the dictionary\n- delete: remove an item from the dictionary\n- empty: the null dictionary\n- Missing: exception for errors in `lookup` and `delete`\n\nIdeally, an _abstract type_ should provide these operations and hide the internal data structures.\n\nA dictionary attaches values to identifiers, called “keys”.  Before choosing\nthe internal representation for a data structure, you need to specify the full\nset of operations. In fact, here we only consider `update` (associating a\nvalue with an identifier) and `lookup` (retrieving such a value). Deletion\nis more difficult and would limit our choices. Some applications may need\nadditional operations, such as `merge (combining two dictionaries). We\nshall see that update can be done efficiently in a functional style, without\nexcessive copying.\n\nAn _abstract type_ provides specified operations while hiding low-level\ndetails, such as the data structure used to represent dictionaries. Abstract\ntypes can be declared in any modern programming language. Java’s _objects_\nserve this role, as do OCaml’s modules. This course does not cover modules, and we\nsimply declare the dictionary operations individually.\n\nAn _association list_ (a list of pairs) is the simplest dictionary representation.\nLookup is by linear search, and therefore slow: $O(n)$. Association lists are\nonly usable if there are few keys in use. However, they are general in that the\nkeys do not need a concept of ordering, only equality.\n"},{"cell_type":"code","metadata":{},"source":"exception Missing","outputs":[],"execution_count":106},{"cell_type":"code","metadata":{},"source":"let rec lookup = function\n| [], a -> raise Missing\n| (x, y) :: pairs, a ->\n    if a = x then\n      y\n    else\n      lookup (pairs, a)","outputs":[],"execution_count":107},{"cell_type":"code","metadata":{},"source":"let update (l, b, y) = (b, y) :: l","outputs":[],"execution_count":108},{"cell_type":"markdown","metadata":{},"source":"\nTo enter a new `(key, value)` pair, simply “cons” it to the list with `update`.\nThis takes constant time, which is the best we could hope for.  But the space\nrequirement is huge: linear in the number of updates, not in the number of\ndistinct keys. Obsolete entries are never deleted: that would require first\nfinding them, increasing the update time from $O(1)$ to $O(n)$.\n"},{"cell_type":"markdown","metadata":{},"source":"## Binary Search Trees\n\n\n\nA _dictionary_ associates _values_ (here, numbers) with _keys_.\n\n![](binsearch.png)\n\nBinary search trees are an important application of binary trees.  They work\nfor keys that have a total ordering, such as strings.  Each branch of the tree\ncarries a $(key, value)$ pair; its left subtree holds smaller keys; the right\nsubtree holds greater keys.  If the tree remains reasonably balanced, then\nupdate and lookup both take $O(\\log n)$ for a tree of size $n$.  These times\nhold in the average case; given random data, the tree is likely to remain\nbalanced.\n\nAt a given node, all keys in the left subtree are smaller (or equal) while all\ntrees in the right subtree are greater.\n\nAn unbalanced tree has a linear access time in the worst case.  Examples\ninclude building a tree by repeated insertions of elements in increasing or\ndecreasing order; there is a close resemblance to quicksort.  Building a binary\nsearch tree, then converting it to inorder, yields a sorting algorithm called\n_treesort_.\n\nSelf-balancing trees, such as Red-Black trees, attain $O(\\log n)$ in the worst\ncase.  They are complicated to implement.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lookup: Seeks Left or Right\n\n\n"},{"cell_type":"code","metadata":{},"source":"exception Missing of string","outputs":[],"execution_count":109},{"cell_type":"code","metadata":{},"source":"let rec lookup = function\n| Br ((a, x), t1, t2), b ->\n    if b < a then\n      lookup (t1, b)\n    else if a < b then\n      lookup (t2, b)\n    else\n      x\n| Lf, b -> raise (Missing b)","outputs":[],"execution_count":110},{"cell_type":"markdown","metadata":{},"source":"\nGuaranteed $O(\\log n)$ access time _if_ the tree is balanced!\n\nLookup in the binary search tree goes to the left subtree if the desired\nkey is smaller than the current one and to the right if it is greater.\nIt raises `Missing` if it encounters an empty tree.\n\nSince an ordering is involved, we have to declare the functions for a specific\ntype, here `string`.  Now exception `Missing` mentions that type: if lookup\nfails, the exception returns the missing key.  The exception could be\neliminated using type `option` of our earlier Datatypes lecture, using the\nconstructor `None` for failure.\n"},{"cell_type":"markdown","metadata":{},"source":"## Update\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec update k v = function\n| Lf -> Br ((k, v), Lf, Lf)\n| Br ((a, x), t1, t2) ->\n    if k < a then\n      Br ((a, x), update k v t1, t2)\n    else if a < k then\n      Br ((a, x), t1, update k v t2)\n    else (* a = k *)\n      Br ((a, v), t1, t2)","outputs":[],"execution_count":111},{"cell_type":"markdown","metadata":{},"source":"\nThis is also $O(\\log n)$ as it copies the path only, and _not whole subtrees!_\n\nIf you are familiar with the usual update operation for this sort of tree, you\nmay wonder whether it can be implemented in OCaml, where there is no direct way to\nreplace part of a data structure by something else.\n\nThe update operation is a nice piece of functional programming.  It searches\nin the same manner as `lookup`, but the recursive calls reconstruct a\nnew tree around the result of the update.  One subtree is updated and the\nother left unchanged.  The internal representation of trees ensures that\nunchanged parts of the tree are not copied, but _shared_.\nTherefore, update copies only the path from the root to the new\nnode.  Its time and space requirements, for a reasonably balanced tree, are\nboth $O(\\log n)$.\n\nThe comparison between $b$ and $a$ allows three cases:\n- smaller: update the left subtree; share the right\n- greater: update the right subtree; share the left\n- equal: update the label and share both subtrees\n\nNote: in the function definition, `(* a = b*)` is a comment.  Comments\nin OCaml are enclosed in the brackets `(*` and `*)`.\n"},{"cell_type":"markdown","metadata":{},"source":"## Aside: Traversing Trees (3 Methods)\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec preorder = function\n| Lf -> []\n| Br (v, t1, t2) ->\n    [v] @ preorder t1 @ preorder t2","outputs":[],"execution_count":112},{"cell_type":"code","metadata":{},"source":"let rec inorder = function\n| Lf -> []\n| Br (v, t1, t2) ->\n    inorder t1 @ [v] @ inorder t2","outputs":[],"execution_count":113},{"cell_type":"code","metadata":{},"source":"let rec postorder = function\n| Lf -> []\n| Br (v, t1, t2) ->\n    postorder t1 @ postorder t2 @ [v]","outputs":[],"execution_count":114},{"cell_type":"markdown","metadata":{},"source":"\n_Tree traversal_ means examining each node of a tree in some order.  [D. E.\nKnuth](https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming) has\nidentified three forms of tree traversal: preorder, inorder and\npostorder.  We can code these “visiting orders” as functions\nthat convert trees into lists of labels.  Algorithms based on these notions\ntypically perform some action at each node; the functions above simply copy\nthe nodes into lists.  Consider the tree:\n\n![](bintree2.png)\n\n- `preorder` visits the label first (“Polish notation”), yielding `ABDECFG`\n- `inorder` visits the label midway, yielding `DBEAFCG`\n- `postorder` visits the label last (“Reverse Polish”), yielding `DEBFGCA`. You will be familiar with this concept if you own an RPN calculator.\n\nWhat is the use of `inorder`? Consider applying it to a binary search tree: the\nresult is a sorted list of pairs. We could use this, for example, to merge two\nbinary search trees. It is not difficult to transform a sorted list of pairs\ninto a binary search tree.\n"},{"cell_type":"markdown","metadata":{},"source":"## Efficiently Traversing Trees\n\n\n\nUnfortunately, the functions shown on the previous slide are quadratic in the\nworst case: the appends in the recursive calls are inefficient.  To correct\nthat problem, we (as usual) add an accumulating argument.  Observe how\neach function constructs its result list and compare with how appends were\neliminated from `quicksort` in the Sorting lecture.\n"},{"cell_type":"code","metadata":{},"source":"let rec preord = function\n| Lf, vs -> vs\n| Br (v, t1, t2), vs ->\n    v :: preord (t1, preord (t2, vs))","outputs":[],"execution_count":115},{"cell_type":"code","metadata":{},"source":"let rec inord = function\n| Lf, vs -> vs\n| Br (v, t1, t2), vs ->\n    inord (t1, v::inord (t2, vs))","outputs":[],"execution_count":116},{"cell_type":"code","metadata":{},"source":"let rec postord = function\n| Lf, vs -> vs\n| Br (v, t1, t2), vs ->\n    postord (t1, postord (t2, v::vs))","outputs":[],"execution_count":117},{"cell_type":"markdown","metadata":{},"source":"\nOne can prove equations relating each of these functions to its counterpart on\nthe previous slide.  For example:\n\n$$ \\texttt{inord}(t, vs) = \\texttt{inorder}(t) @ vs $$\n\nThese three types of tree traversal are related in that all are depth-first.\nThey each traverse the left subtree in full before traversing the right\nsubtree.  Breadth-first search (from the Queues lecture) is another\npossibility.  That involves going through the levels of a tree one at a time.\n"},{"cell_type":"markdown","metadata":{},"source":"## Arrays\n\n\n\n- A conventional array is an indexed storage area.\n  * It is updated _in place_ by the command `a.(k) <- x`\n  * The concept is inherently _imperative_.\n- A _functional array_ is a finite map from integers to data.\n  * Updating implies _copying_ to return `update(A, k, x)`\n  * The new array equals `A` except that `A[k] = x`.\n- Can we do updates efficiently?\n\nThe elements of a list can only be reached by counting from the front.\nElements of a tree are reached by following a path from the root.  An\n_array_ hides such structural matters; its elements are uniformly\ndesignated by number.  Immediate access to arbitrary parts of a data structure\nis called _random access_.\n\nArrays are the dominant data structure in conventional programming languages.\nThe ingenious use of arrays is the key to many of the great classical\nalgorithms, such as Hoare’s original quicksort (the partition step) and\nWarshall’s transitive-closure algorithm.\n\nThe drawback is that subscripting is a chief cause of programmer error.  That\nis why arrays play little role in this introductory course.\n\nFunctional arrays are described below in order to illustrate another way of\nusing trees to organise data.  Here is a summary of basic dictionary data\nstructures in order of decreasing generality and increasing efficiency:\n- Linear search: Most general, needing only equality on keys, but inefficient: linear time.\n- Binary search: Needs an ordering on keys.  Logarithmic access time in the average case, but our binary search trees are linear in the worst case.\n- Array subscripting: Least general, requiring keys to be integers, but even worst-case time is logarithmic.\n"},{"cell_type":"markdown","metadata":{},"source":"## Functional Arrays as Binary Trees\n\n\n\nThe path to element $i$ follows the _binary code_ for $i$ (its “subscript”).\n\n![](array1.png)\n\nThis simple representation (credited to W. Braun) ensures that the tree is\nbalanced.  Complexity of access is always $O(\\log n)$, which is optimal.  For\nactual running time, access to conventional arrays is much faster: it requires\nonly a few hardware instructions.  Array access is often taken to be $O(1)$,\nwhich (as always) presumes that hardware limits are never exceeded.\n\nThe lower bound for array indices is one.  The upper bound starts at zero\n(which signifies the empty array) and can grow without limit.  Inspection of\nthe diagram above should make it clear that these trees are always balanced:\nthe left subtree can have at most one node more than the right subtree,\nrecursively all the way down.  (This assumes that the array is defined for\nsubscripts $1\\ldots n$ with no gaps; an array defined only for odd numbers, for\nexample, would obviously be unbalanced.)\n\nThe numbers in the diagram above are not the labels of branch nodes, but\nindicate the positions of array elements. For example, the label corresponding\nto $A[2]$ is at the position shown. The nodes of a functional array are\nlabelled with the data we want to store, not with these integers.\n"},{"cell_type":"markdown","metadata":{},"source":"## The Lookup Function\n\n\n"},{"cell_type":"code","metadata":{},"source":"exception Subscript\nlet rec sub = function\n| Lf, _ -> raise Subscript  (* Not found *)\n| Br (v, t1, t2), k ->\n    if k = 1 then v\n    else if k mod 2 = 0 then\n      sub (t1, k / 2)\n    else\n      sub (t2, k / 2)","outputs":[],"execution_count":118},{"cell_type":"code","metadata":{},"source":"let rec sub = function (* Alternative implementation *)\n| Lf, _ -> raise Subscript\n| Br (v, t1, t2), 1 -> v\n| Br (v, t1, t2), k when k mod 2 = 0 -> sub (t1, k / 2)\n| Br (v, t1, t2), k -> sub (t2, k / 2)","outputs":[],"execution_count":119},{"cell_type":"markdown","metadata":{},"source":"\nThe lookup function `sub`, divides the subscript by 2 until 1 is\nreached.  If the remainder is 0 then the function follows the left subtree,\notherwise the right.  If it reaches a leaf, it signals error by raising\nexception `Subscript`.\n\nArray access can also be understood in terms of the subscript’s binary code.\nBecause the subscript must be a positive integer, in binary it has a leading\none.  Discard this one and reverse the remaining bits.  Interpreting zero\nas _left_ and one as _right_ yields the path from the root to the\nsubscript.\n\nPopular literature often explains the importance of binary as being led by\nhardware: because a circuit is either on or off.  The truth is almost the\nopposite.  Designers of digital electronics go to a lot of trouble to suppress\nthe continuous behaviour that would naturally arise.  The real reason why\nbinary is important is its role in algorithms: an `if-then-else` decision leads\nto binary branching.\n\nData structures, such as trees, and algorithms, such as mergesort, use binary\nbranching in order to reduce a cost from $O(n)$ to $O(\\log n)$.  Two is the\nsmallest integer divisor that achieves this reduction.  (Larger divisors are\nonly occasionally helpful, as in the case of B-trees, where they reduce the\nconstant factor.)  The simplicity of binary arithmetic compared with decimal\narithmetic is just another instance of the simplicity of algorithms based on\nbinary choices.\n"},{"cell_type":"markdown","metadata":{},"source":"## The Update Function\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec update = function\n| Lf, k, w ->\n    if k = 1 then\n      Br (w, Lf, Lf)\n    else\n      raise Subscript  (* Gap in tree *)\n| Br (v, t1, t2), k, w ->\n    if k = 1 then\n      Br (w, t1, t2)\n    else if k mod 2 = 0 then\n      Br (v, update (t1, k / 2, w), t2)\n    else\n      Br (v, t1, update (t2, k / 2, w))","outputs":[],"execution_count":120},{"cell_type":"markdown","metadata":{},"source":"\nThe `update` function also divides the subscript repeatedly by two.  When it\nreaches a value of one, it has identified the element position.  Then it\nreplaces the branch node by another branch with the new label.\n\nA leaf may be replaced by a branch, extending the array, provided no\nintervening nodes have to be generated.  This suffices for arrays without gaps\nin their subscripting.  (The data structure can be modified to allow _sparse_\narrays, where most subscript positions are undefined.) Exception `Subscript`\nindicates that the subscript position does not exist and cannot be created.\nThis use of exceptions is not easily replaced by `None` and `Some`.\n\nNote that there are two tests involving $k=1$.  If we have reached a leaf,\nit returns a branch, extending the array by one.  If we are still at a branch\nnode, then the effect is to update an existing array element.\n\nA similar function can _shrink_ an array by one.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 7.1\n\n\nDraw the binary search tree that arises from successively inserting the following pairs into the\nempty tree: `(\"Alice\", 6)`\", `(\"Tobias\", 2)`\", `(\"Gerald\", 8)`\", `(\"Lucy\", 9)`. Then repeat this\ntask using the order `(\"Gerald\", 8)`\", `(\"Alice\", 6)`\", `(\"Lucy\", 9)`\", `(\"Tobias\", 2)`. Why are\nresults different?\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 7.2\n\n\nCode an insertion function for binary search trees. It should resemble the existing `update`\nfunction except that it should raise the exception `Collision` if the item to be inserted is already\npresent.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 7.3\n\n\nContinuing the previous exercise, it would be natural for exceptional `Collision` to return the\nvalue previously stored in the dictionary. Why is that goal difficult to achieve?\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 7.4\n\n\nDescribe an algorithm for deleting an entry from a binary search tree. Comment on the suitability of\nyour approach.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 7.5\n\n\nCode the delete function outlined in the previous exercise.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 7.6\n\n\nShow that the functions `preorder`, `inorder` and `postorder` all require $O(n^2)$ time in the worst\ncase, where $n$ is the size of the tree.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 7.7\n\n\nShow that the functions `preord`, `inord` and `postord` all take linear time in the size of the\ntree.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 7.8\n\n\nWrite a function to remove the first element from a functional array. All the other elements are to\nhave their subscripts reduced by one. The cost of this operation should be linear in the size of the\narray.\n"},{"cell_type":"markdown","metadata":{},"source":"# Lecture 8: Functions as Value\n\n\n\nIn OCaml, functions can be\n- passed as arguments to other functions,\n- returned as results,\n- put into lists, trees, etc.,\n- but _not_ tested for equality.\n"},{"cell_type":"code","metadata":{},"source":"[(fun n -> n * 2);\n (fun n -> n * 3);\n (fun n -> n + 1)]","outputs":[],"execution_count":121},{"cell_type":"markdown","metadata":{},"source":"\nProgress in programming languages can be measured by what abstractions they\nadmit.  Conditional expressions (descended from conditional jumps based\non the sign of some numeric variable) and parametric types such as\n$\\alpha\\,\\texttt{list}$ are examples.  The idea that functions could be used\nas values in a computation arose early, but it took some time before the idea\nwas fully realised.  Many programming languages let functions be passed as\narguments to other functions, but few take the trouble needed to allow\nfunctions to be returned as results.\n\nIn mathematics, a _functional_ or _higher-order function_ is a\nfunction that operates on other functions.  Many functionals are familiar from\nmathematics: for example, the differential operator maps functions to their\nderivatives, which are also functions.  To a mathematician, a function is\ntypically an infinite, uncomputable object.  We use OCaml functions to represent\nalgorithms.  Sometimes they represent infinite collections of data given by\ncomputation rules.\n\nFunctions cannot be compared for equality. We could compare the machine\naddresses of the compiled code, but that would merely be a test of identity: it\nwould regard any two separate functions as unequal even if they were compiled\nfrom identical pieces of source code.  Such a low-level feature has no place in\na principled language.\n"},{"cell_type":"markdown","metadata":{},"source":"## Functions Without Names\n\n\n\nIf functions are to be regarded as computational values, then we need a\nnotation for them.  The `fun` notation expresses a non-recursive function\nvalue without giving the function a name.\n\n$\\tt fun x \\rightarrow E$ is the function $f$ such that $f(x)=E$.\nThe function `fun n => n*2` is a _doubling function_.\n"},{"cell_type":"code","metadata":{},"source":"fun n -> n * 2","outputs":[],"execution_count":122},{"cell_type":"code","metadata":{},"source":"(fun n -> n * 2) 17","outputs":[],"execution_count":123},{"cell_type":"markdown","metadata":{},"source":"\nThe main purpose of `fun`-notation is to package up small expressions that are to be\napplied repeatedly using some other function.\nThe expression `fun n -> n*2` has the same value as the identifier\n`double`, declared as follows:\n"},{"cell_type":"code","metadata":{},"source":"let double n = n * 2","outputs":[],"execution_count":124},{"cell_type":"markdown","metadata":{},"source":"\nThe `fun` notation can also do pattern matching, and the `function` keyword\nadds an anonymous variable name to pattern match against.  The following functions\nare all equivalent, with the latter definitions bound to the `is_zero` value and the earlier ones anonymous:\n"},{"cell_type":"code","metadata":{},"source":"fun x -> match x with 0 -> true | _ -> false","outputs":[],"execution_count":125},{"cell_type":"code","metadata":{},"source":"function 0 -> true | _ -> false","outputs":[],"execution_count":126},{"cell_type":"code","metadata":{},"source":"let is_zero = fun x -> match x with 0 -> true | _ -> false","outputs":[],"execution_count":127},{"cell_type":"code","metadata":{},"source":"let is_zero = function 0 -> true | _ -> false","outputs":[],"execution_count":128},{"cell_type":"markdown","metadata":{},"source":"## Curried Functions\n\n\n\nA _curried function_ returns another function as its result.    \n"},{"cell_type":"code","metadata":{},"source":"let prefix = fun a -> fun b -> a ^ b","outputs":[],"execution_count":129},{"cell_type":"code","metadata":{},"source":"let promote = prefix \"Senior \"","outputs":[],"execution_count":130},{"cell_type":"code","metadata":{},"source":"prefix \"Junior \" \"Professor\"","outputs":[],"execution_count":131},{"cell_type":"code","metadata":{},"source":"promote \"Professor\"","outputs":[],"execution_count":132},{"cell_type":"markdown","metadata":{},"source":"\nA short form for the definition of `prefix` is simply to pass multiple\narguments to the function definition.  The following two definitions\nare equivalent in OCaml:\n"},{"cell_type":"code","metadata":{},"source":"let prefix = fun a -> fun b -> a ^ b","outputs":[],"execution_count":133},{"cell_type":"code","metadata":{},"source":"let prefix a b = a ^ b","outputs":[],"execution_count":134},{"cell_type":"markdown","metadata":{},"source":"\nCurrying is the technique of expressing a function taking multiple arguments as nested functions, each taking a single argument.\nThe `fun`-notation lets us package `n*2` as the function\n`fun n -> n * 2`, but what if there are several variables, as in\n`fun n -> n * 2 + k`?  A function of two arguments could be coded using\npattern-matching on pairs, writing `fun (n, k) -> n * 2 + k`.\n\nCurrying is an alternative, where we _nest_ the `fun`-notation:\n"},{"cell_type":"code","metadata":{},"source":"fun k -> fun n -> n * 2 + k","outputs":[],"execution_count":135},{"cell_type":"markdown","metadata":{},"source":"\nApplying this curried function to the argument 1 yields another function, in which `k` has been replaced by 1:\n"},{"cell_type":"code","metadata":{},"source":"let fn = fun k -> fun n -> n * 2 + k","outputs":[],"execution_count":136},{"cell_type":"code","metadata":{},"source":"let fn' = fn 1 (* n * 2 + 1 *)","outputs":[],"execution_count":137},{"cell_type":"code","metadata":{},"source":"fn' 3  (* 3 * 2 + 1 *)","outputs":[],"execution_count":138},{"cell_type":"markdown","metadata":{},"source":"\nAnd this function, when applied to 3, yields the result 7. The two arguments are supplied one after another.\n\nThe example on the slide is similar but refers to the expression `a^b`,\nwhere `^` is the infix operator for string concatenation. Function `promote` binds the first argument of `prefix` to\n`\"Professor\"`; the resulting function prefixes that title\nto any string to which it is applied.\n"},{"cell_type":"markdown","metadata":{},"source":"## Shorthand for Curried Functions\n\n\n\nA function-returning function is just a function of two arguments.\n\nThis curried function syntax is nicer than nested `fun` binders:\n"},{"cell_type":"code","metadata":{},"source":"let prefix a b = a ^ b","outputs":[],"execution_count":139},{"cell_type":"code","metadata":{},"source":"let dub = prefix \"Sir \"","outputs":[],"execution_count":140},{"cell_type":"markdown","metadata":{},"source":"\nCurried functions allows _partial application_ (to the first argument).\n\nIn OCaml, an $n$-argument curried function `f` can be declared using the syntax:\n\n$$\\tt let \\; f \\; x_1 \\: \\ldots \\: x_{n} \\: = \\: E$$\n\nand applied using the syntax:\n\n$$\\tt \\; E_1 \\; \\ldots \\; E_n$$\n\nIf `f` is not recursive, then it is equivalent to the function expressed via nesting as follows:\n\n$$\\tt fun \\; x_1 \\; \\rightarrow \\cdots \\rightarrow fun \\; x_{n} \\rightarrow E) $$\n\nWe now have two ways of expressing functions of multiple arguments: either by\npassing a pair of arguments or by currying.  Currying allows _partial application_\nwhich is useful when fixing the first argument yields a function\nthat is interesting in its own right.  An example from mathematics is the\nfunction $x^y$, where fixing $y=2$ yields a function in $x$ alone, namely\nsquaring. Similarly, $y=3$ yields cubing, while $y=1$ yields the identity\nfunction.\n\nThough the function `hd` (which returns the head of a list) is not\ncurried, it may be used with the curried application syntax in some\nexpressions:\n"},{"cell_type":"code","metadata":{},"source":"List.hd [dub; promote] \"Hamilton\"","outputs":[],"execution_count":141},{"cell_type":"markdown","metadata":{},"source":"\nHere `List.hd` is applied to a list of functions, and the resulting function\n`dub` is then applied to the string `\"Hamilton\"`.  The idea of\nexecuting code stored in data structures reaches its full development in\n_object-oriented_ programming, like in Java.\n"},{"cell_type":"markdown","metadata":{},"source":"## Partial Application: A Curried Insertion Sort\n\n\n"},{"cell_type":"code","metadata":{},"source":"let insort lessequal =\n  let rec ins = function\n    | x, [] -> [x]\n    | x, y::ys ->\n        if lessequal x y then\n           x :: y :: ys\n        else\n           y :: ins (x, ys)\n  in\n  let rec sort = function\n    | [] -> []\n    | x::xs -> ins (x, sort xs)\n  in\n  sort","outputs":[],"execution_count":142},{"cell_type":"markdown","metadata":{},"source":"\nThe sorting functions we discussed in earlier lectures are coded to sort real\nnumbers.  They can be generalised to an arbitrary ordered type by passing the\nordering predicate `leq` as an argument.\n\nFunctions `ins` and `sort` are declared locally, referring to `lessequal`.\nThough it may not be obvious, `insort` is a curried function.  Given its first\nargument, a predicate for comparing some particular type of items, it returns\nthe function `sort` for sorting lists of that type of items.\n\nSome examples of its use:\n"},{"cell_type":"code","metadata":{},"source":"insort (<=) [5; 3; 9; 8]","outputs":[],"execution_count":143},{"cell_type":"code","metadata":{},"source":"insort (<=) [\"bitten\"; \"on\"; \"a\"; \"bee\"]","outputs":[],"execution_count":144},{"cell_type":"code","metadata":{},"source":"insort (>=) [5; 3; 9; 8]","outputs":[],"execution_count":145},{"cell_type":"markdown","metadata":{},"source":"\nAn obscure point: the syntax `(<=)` denotes the comparison operator as a\nfunction, which is then given to `insort`.  Passing the relation $\\geq$ for\n`lessequal` gives a decreasing sort.  This is no coding trick; it is justified\nin mathematics, since if $\\leq$ is a partial ordering then so is $\\geq$.\n"},{"cell_type":"markdown","metadata":{},"source":"## map: the “Apply to All” Function\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec map f = function\n  | [] -> []\n  | x::xs -> (f x) :: map f xs","outputs":[],"execution_count":146},{"cell_type":"code","metadata":{},"source":"map (fun s -> s ^ \"ppy\") [\"Hi\"; \"Ho\"]","outputs":[],"execution_count":147},{"cell_type":"code","metadata":{},"source":"map (map double) [[1]; [2; 3]]","outputs":[],"execution_count":148},{"cell_type":"markdown","metadata":{},"source":"\nThe functional `map` applies a function to every element of a list,\nreturning a list of the function’s results.  “Apply to all” is a fundamental\noperation and we shall see several applications of it below.  We\nagain see the advantages of `fun`-notation, currying and\n`map`.  If we did not have them, the first example on the slide\nwould require a preliminary function declaration:\n"},{"cell_type":"code","metadata":{},"source":"let rec sillylist = function\n  | [] -> []\n  | s::ss -> (s ^ \"ppy\") :: sillylist ss","outputs":[],"execution_count":149},{"cell_type":"markdown","metadata":{},"source":"\nAn expression containing several applications of functionals---such as our\nsecond example---can abbreviate a long series of declarations.  Sometimes this\ncoding style is cryptic, but it can be clear as crystal.  Treating functions\nas values lets us capture common program structures once and for all.\n\nIn the second example, `double` is the obvious integer doubling function we\ndefined earlier.  Note that \\texttt{map} is a built-in OCaml function in the\nform of `List.map`.  OCaml’s standard library includes, among much else, many\nlist functions.\n"},{"cell_type":"markdown","metadata":{},"source":"## Example: Matrix Transpose\n\n\n\n$$\n\\begin{pmatrix}\n           a   & b & c \\\\\n           d   & e & f\n   \\end{pmatrix}^T =\n   \\begin{pmatrix}\n        a & d     \\\\\n        b & e     \\\\\n        c & f\n\\end{pmatrix}\n$$\n"},{"cell_type":"code","metadata":{},"source":"let rec transp = function\n  | []::_ -> []\n  | rows -> (map List.hd rows) ::\n            (transp (map List.tl rows))","outputs":[],"execution_count":150},{"cell_type":"markdown","metadata":{},"source":"\nA matrix can be viewed as a list of rows, each row a list of matrix elements.\nThis representation is not especially efficient compared with the conventional\none (using arrays).  Lists of lists turn up often, though, and we can see how\nto deal with them by taking familiar matrix operations as examples.\n_ML for the Working Programmer_ goes as far as Gaussian elimination,\nwhich presents surprisingly few difficulties.\n\nThe transpose of the matrix\n$\\left(\\begin{smallmatrix} a & b & c \\\\\n                           d & e & f\\end{smallmatrix}\\right)$\nis\n$\\left(\\begin{smallmatrix}\n        a & d     \\\\\n        b & e     \\\\\n        c & f\n   \\end{smallmatrix}\\right)$,\nwhich in OCaml corresponds to the following transformation on lists of lists:\n"},{"cell_type":"raw","metadata":{},"source":"[[a; b; c]; [d; e; f]] => [[a; d]; [b; e]; [c; f]]"},{"cell_type":"markdown","metadata":{},"source":"\nThe workings of function `transp` are simple.  If `rows` is the\nmatrix to be transposed, then `map hd` extracts its first column and\n`map tl` extracts its second column:\n"},{"cell_type":"raw","metadata":{},"source":"map hd rows => [a; d]\nmap tl rows => [[b; c]; [e; f]]"},{"cell_type":"markdown","metadata":{},"source":"\nA recursive call transposes the latter matrix, which is then given the column\n`[a; d]` as its first row.\nThe two functions expressed using `map` would otherwise have to be declared\nseparately.\n"},{"cell_type":"markdown","metadata":{},"source":"## Review of Matrix Multiplication\n\n\n\n$$\n\\begin{pmatrix} A_1 & \\cdots & A_k \\end{pmatrix}  \\cdot\n   \\begin{pmatrix}\n        B_1 \\\\ \\vdots \\\\ B_k\n   \\end{pmatrix}   =\n   \\begin{pmatrix}\n        A_1 B_1 + \\cdots + A_k B_k\n   \\end{pmatrix}\n$$\n\nThe right side is the _vector dot product_ $\\vec{A}\\cdot \\vec{B}$.\nRepeat for each _row_ of $A$ and _column_ of $B$.\n\nThe _dot product_ of two vectors is\n$$ (a_1,\\ldots,a_k) \\cdot (b_1,\\ldots,b_k) = a_1b_1 + \\cdots + a_kb_k $$\n\nA simple case of matrix multiplication is when $A$ consists of a single row\nand $B$ consists of a single column.  Provided $A$ and $B$ contain the same\nnumber $k$ of elements, multiplying them yields a $1\\times1$ matrix whose\nsingle element is the dot product shown above.\n\nIf $A$ is an $m\\times k$ matrix and $B$ is a $k\\times n$ matrix\nthen $A\\times B$ is an $m\\times n$ matrix.\nFor each $i$ and $j$, the $(i,j)$ element of $A\\times B$ is the dot\nproduct of row $i$ of $A$ with column $j$ of $B$.\n\n$$\n\\begin{pmatrix}\n        2 & 0 \\\\\n        3 &-1 \\\\\n        0 & 1 \\\\\n        1 & 1\n   \\end{pmatrix}\n   \\begin{pmatrix}\n        1 & 0 & 2 \\\\\n        4 &-1 & 0\n   \\end{pmatrix}   =\n   \\begin{pmatrix}\n        2 & 0 & 4 \\\\\n       -1 & 1 & 6 \\\\\n        4 &-1 & 0 \\\\\n        5 &-1 & 2\n\\end{pmatrix}\n$$\n\nThe (1, 1) element above is computed by\n$$ (2,0)\\cdot(1,4) = 2\\times1 + 0\\times4 = 2. $$\n\nCoding matrix multiplication in a conventional programming language usually\ninvolves three nested loops.  It is hard to avoid mistakes in the subscripting,\nwhich often runs slowly due to redundant internal calculations.\n"},{"cell_type":"markdown","metadata":{},"source":"## Matrix Multiplication in OCaml\n\n\n\n_Dot product_ of two vectors---a _curried function_\n"},{"cell_type":"code","metadata":{},"source":"let rec dotprod xs ys =\n  match xs, ys with\n  | [], [] -> 0.0\n  | x::xs, y::ys ->  (x *. y) +. (dotprod xs ys)","outputs":[],"execution_count":151},{"cell_type":"markdown","metadata":{},"source":"\n_Matrix product_\n"},{"cell_type":"code","metadata":{},"source":"let rec matprod arows brows =\n  let cols = transp brows in\n  map (fun row -> map (dotprod row) cols) arows","outputs":[],"execution_count":152},{"cell_type":"markdown","metadata":{},"source":"\nThe `transp brows` converts $B$ into a list of columns.  It yields a\nlist, whose elements are the columns of $B$.  Each row of $A\\times B$ is\nobtained by multiplying a row of $A$ by the columns of $B$.\n\nBecause `dotprod` is curried, it can be applied to a row of $A$.  The\nresulting function is applied to all the columns of $B$.  We have another\nexample of currying and partial application.\n\nThe outer `map` applies `dotprod` to each row of $A$.  The inner\n`map`, using `fun`-notation, applies `dotprod row` to each\ncolumn of $B$.  Compare with the version in _ML for the Working\n  Programmer_ (page 89) which does not use `map` and requires two\nadditional function declarations.\n\nIn the dot product function, the two vectors must have the same length.\nOtherwise, exception `Match_failure` is raised.\n"},{"cell_type":"markdown","metadata":{},"source":"## List Functionals for Predicates\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec exists p = function\n| [] -> false\n| x::xs -> (p x) || (exists p xs)","outputs":[],"execution_count":153},{"cell_type":"code","metadata":{},"source":"let rec filter p = function\n| [] -> []\n| x::xs ->\n    if p x then\n      x :: filter p xs\n    else\n      filter p xs","outputs":[],"execution_count":154},{"cell_type":"markdown","metadata":{},"source":"\nA _predicate_ is a _boolean-valued_ function.\n\nThe functional `exists` transforms a predicate into a predicate over\nlists.  Given a list, `exists p` tests whether or not some list element\nsatisfies `p` (making it return `true`).  If it finds one, it stops\nsearching immediately, thanks to the behaviour of the lazy `||` operator.\n\nDually, we have a functional to test whether all list elements satisfy the\npredicate.  If it finds a counterexample then it, too, stops searching.\n"},{"cell_type":"code","metadata":{},"source":"let rec all p = function\n| [] -> true\n| x::xs -> (p x) && all p xs","outputs":[],"execution_count":155},{"cell_type":"markdown","metadata":{},"source":"\nThe `filter` functional is related to `map`.  It applies a predicate to all the\nlist elements, but instead of returning the resulting values (which could only\nbe `true` or `false`), it returns the list of elements satisfying the\npredicate.\n"},{"cell_type":"markdown","metadata":{},"source":"## Applications of the Predicate Functionals\n\n\n"},{"cell_type":"code","metadata":{},"source":"let member y xs =\n  exists (fun x -> x=y) xs","outputs":[],"execution_count":156},{"cell_type":"code","metadata":{},"source":"let inter xs ys =\n  filter (fun x -> member x ys) xs","outputs":[],"execution_count":157},{"cell_type":"markdown","metadata":{},"source":"\n_Testing whether two lists have no common elements_\n"},{"cell_type":"code","metadata":{},"source":"let disjoint xs ys =\n  all (fun x -> all (fun y -> x<>y) ys) xs","outputs":[],"execution_count":158},{"cell_type":"markdown","metadata":{},"source":"\nThe Lists lecture presented the function `member`, which tests whether a\nspecified value can be found as a list element, and `inter`, which returns the\n“intersection” of two lists: the list of elements they have in common.\n\nBut remember: the purpose of list functionals is not to replace the\ndeclarations of popular functions, which probably are available already.  It is\nto eliminate the need for separate declarations of ad-hoc functions.  When they\nare nested, like the calls to \\texttt{all} in \\texttt{disjoint} above, the\ninner functions are almost certainly one-offs, not worth declaring separately.\n\nOur primitives themselves can be seen as a programming language.  Part of the\ntask of programming is to extend our programming language with notation for\nsolving the problem at hand.  The levels of notation that we define should\ncorrespond to natural levels of abstraction in the problem domain.\n\nHistorical Note:\nAlonzo Church’s $\\lambda$-calculus gave a simple syntax, $\\lambda$-notation,\nfor expressing functions.  It is the direct precursor of OCaml’s\n`fun`-notation.  It was soon shown that his system was equivalent in\ncomputational power to Turing machines, and _Church’s thesis_ states that\nthis defines precisely the set of functions that can be computed effectively.\n\nThe $\\lambda$-calculus had a tremendous influence on the design of functional\nprogramming languages.  McCarthy’s Lisp was something of a false start; it\ninterpreted variable binding incorrectly, an error that stood for some 20\nyears.  But in 1966, Peter Landin (of Queen Mary College, University of London)\nsketched out the main features of functional languages.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 8.1\n\n\nWhat does the following function do, and what are its uses?"},{"cell_type":"code","metadata":{},"source":"let sw f x y = f y x","outputs":[],"execution_count":159},{"cell_type":"markdown","metadata":{},"source":"## Exercise 8.2\n\n\nThere are many ways of combining orderings. The `lexicographic ordering` uses two keys for\ncomparisons. It is specified by\n$$(x',y')<(x,y)\\iff x'<x \\vee (x'=x \\wedge y'<y).$$\nWrite an OCaml function to lexicographically combine two orderings, supplied as functions. Explain\nhow it allows function `insort` to sort a list of pairs.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 8.3\n\n\nWithout using `map` write a function `map2` such that `map2 f` is equivalent to `map (map f)`. The\nobvious solution requires declaring two recursive functions. Try to get away with only one by\nexploiting nested pattern-matching.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 8.4\n\n\nThe type `'a option`, declared below, can be viewed as a type of lists having at most one element.\n(It is typically used as an alternative to exceptions.) Declare an analogue of the function `map`\nfor type `'a option`."},{"cell_type":"code","metadata":{},"source":"type 'a option = None | Some of 'a","outputs":[],"execution_count":160},{"cell_type":"markdown","metadata":{},"source":"## Exercise 8.5\n\n\nRecall the making change function of Lecture 4:"},{"cell_type":"code","metadata":{},"source":"let rec change till amt = …\n  | c::till ->\n      if …\n      else\n        let rec allc = function\n          | [] -> []\n          | cs::css -> (c::cs) :: allc css\n        in\n          allc (change (c::till) (amt - c)) @\n                change till amt","outputs":[],"execution_count":161},{"cell_type":"markdown","metadata":{},"source":"Function `allc` applies the function ‘cons a `c`’ to every element of a list. Eliminate it by\ndeclaring a curried cons function and applying `map`.\n"},{"cell_type":"markdown","metadata":{},"source":"# Lecture 9: Sequences, or Lazy Lists\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"## A Pipeline\n\n\n\n$$\n\\framebox{\\emph{Producer}} \\to \\framebox{\\emph{Filter}} \\to\\cdots\\to\n \\framebox{\\emph{Filter}} \\to \\framebox{\\emph{Consumer}}\n$$\n\n- Produce sequence of items\n- Filter sequence in stages\n- Consume results as needed\n- _Lazy lists_ join the stages together\n\nTwo types of program can be distinguished.  A sequential program\naccepts a problem to solve, processes for a while, and finally terminates\nwith its result.  A typical example is the huge numerical simulations that are\nrun on supercomputers.  Most of our OCaml functions also fit this model.\n\nAt the other extreme are _reactive_ programs, whose job is to interact\nwith the environment.  They communicate constantly during their operation and\nrun for as long as is necessary.  A typical example is the software that\ncontrols many modern aircraft.  Reactive programs often consist of\n_concurrent processes_ running at the same time and communicating with\none another.\n\nConcurrency is too difficult to consider in this course, but we can model\nsimple pipelines such as that shown above.  The _Producer_ represents one\nor more sources of data, which it outputs as a stream.  The _Filter_\nstages convert the input stream to an output stream, perhaps consuming several\ninput items to yield a single output item.  The _Consumer_ takes as many\nelements as necessary.\n\nThe Consumer drives the pipeline: nothing is computed except in response to\nits demand for an additional datum.  Execution of the Filter stages is\ninterleaved as required for the computation to go through.  The programmer\nsets up the data dependencies but has no clear idea of what happens when.  We\nhave the illusion of concurrent computation.\n\nThe Unix operating system provides similar ideas through its _pipes_ that\nlink processes together.  In OCaml, we can model pipelines using _lazy lists_.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lazy Lists (or Streams)\n\n\n\n- Lists of possibly _infinite_ length\n- Elements _computed upon demand_\n- _Avoids waste_ if there are many solutions\n- _Infinite_ values are a useful abstraction\n\nIn OCaml, we can implement laziness by _delaying evaluation_ of the tail of\nthe list.\n\nLazy lists have practical uses.  Some algorithms, like making change, can\nyield many solutions when only a few are required.  Sometimes the original\nproblem concerns infinite series: with lazy lists, we can pretend they really\nexist!\n\nWe are now dealing with _infinite_ (or at least unbounded) computations.\nA potentially infinite source of data is processed one element at a time, upon\ndemand.  Such programs are harder to understand than terminating ones and have\nmore ways of going wrong.\n\nSome purely functional languages, such as Haskell, use lazy evaluation\neverywhere.  Even the if-then-else construct can be a function, and all lists\nare lazy.  In OCaml, we can declare a type of lists such that evaluation of the\ntail does not occur until demanded.  _Delayed_ evaluation is weaker than\n_lazy_ evaluation, but it is good enough for our purposes and often the\nbest compromise for performance and memory usage.\n\nThe traditional word “stream” is reserved in OCaml parlance for\ninput/output channels.  Let us call lazy lists _sequences_ instead.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lazy Lists in OCaml\n\n\n\n- The empty tuple `()` and its _type_ `unit`\n- Delayed version of $E$ is `fun () -> E`\n"},{"cell_type":"code","metadata":{},"source":"type 'a seq =\n| Nil\n| Cons of 'a * (unit -> 'a seq)","outputs":[],"execution_count":162},{"cell_type":"code","metadata":{},"source":"let head (Cons (x, _)) = x","outputs":[],"execution_count":163},{"cell_type":"code","metadata":{},"source":"let tail (Cons (_, xf)) = xf ()","outputs":[],"execution_count":164},{"cell_type":"markdown","metadata":{},"source":"\n$\\tt Cons(x, xf)$ has _head_ $x$ and _tail function_ $xf$\n\nThe primitive OCaml type `unit` has one element, which is\nwritten `()`.  This element may be regarded as a 0-tuple, and\n`unit` as the nullary Cartesian product.  (Think of the connection\nbetween multiplication and the number 1.)\n\nThe empty tuple serves as a placeholder in situations where no information is\nrequired.  It may:\n- appear in a data structure.  For example, a `unit`-valued dictionary represents a set of keys.\n- be the argument of a function, where its effect is to _delay evaluation_.\n- be the argument or result of a procedure. (see the Procedural Programming section)\n\nThe empty tuple, like all tuples, is a constructor and is allowed in patterns:\n"},{"cell_type":"code","metadata":{},"source":"let f () = …","outputs":[],"execution_count":165},{"cell_type":"markdown","metadata":{},"source":"\nIn particular $\\texttt{\\uline{fun}() => $E$}$ is the function that takes an argument of\ntype `unit` and returns the value of $E$ as its result.  Expression $E$\nis not evaluated until the function is called, even though the only possible\nargument is `()`.  The function simply delays the evaluation of $E$.\n"},{"cell_type":"markdown","metadata":{},"source":"## The Infinite Sequence: $k$, $k+1$, $k+2$, …\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec from k = Cons (k, fun () -> from (k+1))","outputs":[],"execution_count":166},{"cell_type":"code","metadata":{},"source":"let it = from 1","outputs":[],"execution_count":167},{"cell_type":"code","metadata":{},"source":"let it = tail it","outputs":[],"execution_count":168},{"cell_type":"code","metadata":{},"source":"let it = tail it","outputs":[],"execution_count":169},{"cell_type":"markdown","metadata":{},"source":"\nFunction `from` constructs the infinite sequence of integers starting\nfrom $k$.  Execution terminates because of the `fun` enclosing the\nrecursive call.  OCaml displays the tail of a sequence as `fun`, which\nstands for some function value.  Each call to `tail` generates the next\nsequence element.  We could do this forever.\n\nThis example is of little practical value because the cost of computing a\nsequence element will be dominated by that of creating the dummy function.\nLazy lists tend to have high overheads.\n"},{"cell_type":"markdown","metadata":{},"source":"## Consuming a Sequence\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec get n s =\n  if n = 0 then\n    []\n  else\n    match s with\n    | Nil -> []\n    | Cons (x, xf) -> x :: get (n-1) (xf ())","outputs":[],"execution_count":170},{"cell_type":"markdown","metadata":{},"source":"\nThe above code gets the first $n$ elements as a list.\n`xf ()` _forces_ evaluation.\n\nThe function `get` converts a sequence to a list.  It takes the\nfirst $n$ elements; it takes all of them if $n<0$, which can terminate only if\nthe sequence is finite.\n\nIn the third line of `get`, the expression `xf()` calls the tail\nfunction, demanding evaluation of the next element.  This operation is called\n_forcing_ the list.\n"},{"cell_type":"markdown","metadata":{},"source":"## Sample Evaluation\n\n\n\n$$\n\\begin{aligned}\n\\tt get(2,\\, from \\; 60) \\\\\n\\tt get(2,\\, Cons(6, fun \\; () \\rightarrow from \\; (6+1))) \\\\\n\\tt 6 :: get(1,\\, from \\; (6+1)) \\\\\n\\tt 6 :: get(1,\\, Cons \\; (7,\\, fun \\; () \\rightarrow from \\; (7+1))) \\\\\n\\tt 6 :: 7 :: get(0,\\, Cons \\; (8,\\, fun \\; () \\rightarrow from \\; (8+1))) \\\\\n\\tt 6 :: 7 :: [] \\\\\n\\tt [6; 7]\n\\end{aligned}\n$$\n\nHere we ask for two elements of the infinite sequence.  In fact, three\nelements are computed: 6, 7 and 8.  Our implementation is slightly too eager.\nA more complicated `type` declaration could avoid this problem.\nAnother problem is that if one repeatedly examines some particular list\nelement using forcing, that element is repeatedly evaluated.  In a lazy\nprogramming language, the result of the first evaluation would be stored for\nlater reference.  To get the same effect in OCaml requires the use of\nreferences.\n\nWe should be grateful that the potentially infinite computation is kept\nfinite.  The tail of the original sequence even contains the unevaluated\nexpression 6+1.\n"},{"cell_type":"markdown","metadata":{},"source":"## Joining Two Sequences\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec appendq xq yq =\n  match xq with\n  | Nil -> yq \n  | Cons (x, xf) ->\n      Cons(x, fun () -> appendq (xf ()) yq)","outputs":[],"execution_count":171},{"cell_type":"markdown","metadata":{},"source":"\nA more fair alternative:\n"},{"cell_type":"code","metadata":{},"source":"let rec interleave xq yq =\n  match xq with\n  | Nil -> yq\n  | Cons (x, xf) ->\n      Cons (x, fun () -> interleave yq (xf ()))","outputs":[],"execution_count":172},{"cell_type":"markdown","metadata":{},"source":"\nMost list functions and functionals have analogues on sequences, but strange\nthings can happen.  Can an infinite list be reversed?\n\nFunction `appendq` is precisely the same idea as `append`\nfrom the Lists lecture; it concatenates two sequences.  If the first\nargument is infinite, then `appendq` never gets to its second argument,\nwhich is lost.  Concatenation of infinite sequences is not terribly\ninteresting.\n\nThe function `interleave` avoids this problem by exchanging the two\narguments in each recursive call.  It combines the two lazy lists, losing no\nelements.  Interleaving is the right way to combine two potentially infinite\ninformation sources into one.\n\nIn both function declarations, observe that each `xf ()` is enclosed\nwithin a ${\\tt \\uline{fn}()=>\\ldots}$.  Each _force_ is enclosed within a\n_delay_.  This practice makes the functions lazy.  A force not enclosed\nin a delay, as in `get` above, runs the risk of evaluating the sequence\nin full.\n"},{"cell_type":"markdown","metadata":{},"source":"## Functionals for Lazy Lists\n\n\n\nFiltering lazy lists:\n"},{"cell_type":"code","metadata":{},"source":"let rec filterq p = function\n| Nil -> Nil\n| Cons (x, xf) ->\n    if p x then\n      Cons (x, fun () -> filterq p (xf ()))\n    else\n      filterq p (xf ())","outputs":[],"execution_count":173},{"cell_type":"markdown","metadata":{},"source":"\nThe infinite sequence $x$, $f(x)$, $f(f(x))$, …\n"},{"cell_type":"code","metadata":{},"source":"let rec iterates f x =\n  Cons (x, fun () -> iterates f (f x))","outputs":[],"execution_count":174},{"cell_type":"markdown","metadata":{},"source":"\nThe functional `filterq` demands elements of `xq` until it finds\none satisfying `p`.  (Recall `filter`, the analogous operation for ordinary lists.)  It\ncontains a _force_ not protected by a _delay_.  If `xq` is\ninfinite and contains no satisfactory element, then `filtering` runs\nforever.\n\nThe functional `iterates` generalises `from`.  It creates the\nnext element not by adding one but by calling the function `f`.\n"},{"cell_type":"markdown","metadata":{},"source":"## Numerical Computations on Infinite Sequences\n\n\n"},{"cell_type":"code","metadata":{},"source":"let next a x = (a /. x +. x) /. 2.0 ","outputs":[],"execution_count":175},{"cell_type":"markdown","metadata":{},"source":"\nClose enough?\n"},{"cell_type":"code","metadata":{},"source":"let rec within eps = function\n| Cons (x, xf) ->\n    match xf () with\n    | Cons (y, yf) ->\n        if abs_float (x -. y) <= eps then\n          y\n        else\n          within eps (Cons (y, yf))","outputs":[],"execution_count":176},{"cell_type":"markdown","metadata":{},"source":"\nSquare Roots:\n"},{"cell_type":"code","metadata":{},"source":"let root a = within 1e6 (iterates (next a) 1.0)","outputs":[],"execution_count":177},{"cell_type":"markdown","metadata":{},"source":"\nThe _Newton-Raphson method_ is widely used for computing square roots.\nThe infinite series $x_0, (a/x_0+x_0)/2, \\ldots{}$ converges rapidly to $\\sqrt{a}$.\nThe initial approximation, $x_0$, is typically retrieved from a table, and is accurate enough\nthat only a few iterations of the method are necessary.\nCalling `iterates (next a) x0` generates the _infinite series_ of\napproximations to the square root of $a$ using the Newton-Raphson method.\nTo compute $\\sqrt2$, the resulting series begins 1, 1.5, 1.41667, 1.4142157, 1.414213562, …,\nand this last figure is already accurate to 10 significant digits!\n\nFunction `within` searches down the lazy list for two points whose\ndifference is less than `eps`.  It tests their absolute difference.\nRelative difference and other “close enough” tests can be coded.  Such\ncomponents can be used to implement other numerical functions directly as\nfunctions over sequences.  The point is to build programs from small,\ninterchangeable parts.\n\nFunction `root` uses `within, `iterates` and `next` to\nto apply Newton-Raphson with a tolerance of $10^{-6}$\nand a (poor) initial approximation of 1.0.\n\nThis treatment of numerical computation has received some attention in the\nresearch literature; a recurring example is Richardson extrapolation.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 9.1\n\n\nCode an analogue of `map` for sequences.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 9.2\n\n\nConsider the list function `concat`, which concatenates a list of lists to form a single list. Can\nit be generalised to concatenate a sequence of sequences? What can go wrong?"},{"cell_type":"code","metadata":{},"source":"let rec concat = function\n| [] ->\n    []\n| l::ls ->\n    l @ concat ls","outputs":[],"execution_count":178},{"cell_type":"markdown","metadata":{},"source":"## Exercise 9.3\n\n\nCode a function to make change using lazy lists, delivering the sequence of all possible ways of\nmaking change. Using sequences allows us to compute solutions one at a time when there exists an\nastronomical number. Represent lists of coins using ordinary lists. (_Hint_: to  benefit from\nlaziness you may need to pass around the sequence of alternative solutions as a function of type\n`unit -> (int list) seq`.)\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 9.4\n\n\nA _lazy binary tree_ is either empty or is a branch containing a label and two lazy binary trees,\npossibly to infinite depth. Present an OCaml datatype to represent lazy binary trees, along with a\nfunction that accepts a lazy binary tree and produces a lazy list that contains all of the tree’s\nlabels. (Taken from the exam question 2008 Paper 1 Question 5.)\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 9.5\n\n\nCode the lazy list whose elements are all ordinary lists of zeroes and ones, namely\n`[]; [0]; [1]; [0; 0]; [0; 1]; [1; 0]; [1; 1]; [0; 0; 0]; `….  (Taken from the exam question\n2003 Paper 1 Question 5.)\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 9.6\n\n\n(Continuing the previous exercise.)\nA _palindrome_ is a list that equals its own reverse. Code the lazy list whose elements are all\npalindromes of 0s and 1s, namely\n`[]; [0]; [1]; [0; 0]; [0; 0; 0]; [0; 1; 0]; [1; 1]; [1; 0; 1]; [1; 1; 1]; [0; 0; 0; 0]; `, …. You\nmay take the reversal function `List.rev` as given.\n"},{"cell_type":"markdown","metadata":{},"source":"# Lecture 10: Queues and Search Strategies\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"## Breadth-First v Depth-First Tree Traversal\n\n\n\n- binary trees as _decision trees_\n- look for _solution nodes_\n  * Depth-first: search one subtree in full before moving on\n  * Breadth-first: search all nodes at level $k$ before moving to $k+1$\n- finds _all_ solutions --- nearest first!\n\nPreorder, inorder and postorder tree traversals all have something in common:\nthey are depth-first.  At each node, the left subtree is entirely\ntraversed before the right subtree.  Depth-first traversals are easy to code\nand can be efficient, but they are ill-suited for some problems.\n\nSuppose the tree represents the possible moves in a puzzle, and the purpose\nof the traversal is to search for a node containing a solution.  Then a\ndepth-first traversal may find one solution node deep in the left subtree,\nwhen another solution is at the very top of the right subtree.  Often we\nwant the shortest path to a solution.\n\nSuppose the tree is _infinite_ or simply extremely large.  Depth-first search\nis almost useless with such trees, for if the left subtree is infinite then the\nsearch will never reach the right subtree.  OCaml can represent infinite trees by\nthe means discussed in the lecture on laziness. Another tree representation (suitable\nfor solving solitaire, for example) is by a function `next : pos -> pos list`,\nwhich maps a board position to a list of the positions possible after\nthe next move.  For simplicity, the examples below use the OCaml datatype\n`tree`, which has only finite trees.\n\nA _breadth-first_ traversal explores the nodes horizontally rather than\nvertically.  When visiting a node, it does not traverse the subtrees until\nit has visited all other nodes at the current depth.  This is easily\nimplemented by keeping a list of trees to visit.  Initially, this list\nconsists of one element: the entire tree.  Each iteration removes a tree\nfrom the head of the list and adds its subtrees after the end of the\nlist.\n"},{"cell_type":"markdown","metadata":{},"source":"## Breadth-First Tree Traversal --- Using Append\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec nbreadth = function\n| [] -> []\n| Lf :: ts -> nbreadth ts\n| Br (v, t, u) :: ts ->\n    v :: nbreadth (ts @ [t; u])","outputs":[],"execution_count":179},{"cell_type":"markdown","metadata":{},"source":"\nKeeps an _enormous queue_ of nodes of search, and is a wasteful use of `append`.\n\nBreadth-first search can be inefficient, this naive implementation especially\nso.  When the search is at depth $d$ of the tree, the list contains all the\nremaining trees at depth $d$, followed by the subtrees (all at depth $d+1$) of\nthe trees that have already been visited.  At depth 10, the list could already\ncontain 1024 elements.  It requires a lot of space, and aggravates this with a\ngross misuse of append.  Evaluating `ts@[t, u]` copies the long list\n`ts` just to insert two elements.\n"},{"cell_type":"markdown","metadata":{},"source":"## An Abstract Data Type: Queues\n\n\n\n- `qempty` is the _empty queue_\n- `qnull` _tests_ whether a queue is empty\n- `qhd` _returns_ the element at the _head_ of a queue\n- `deq` _discards_ the element at the _head_ of a queue\n- `enq` _adds_ an element at the _end_ of a queue\n\nBreadth-first search becomes much faster if we replace the lists by\n_queues_.  A queue represents a sequence, allowing elements to be taken\nfrom the head and added to the tail.  This is a First-In-First-Out (FIFO)\ndiscipline: the item next to be removed is the one that has been in the queue\nfor the longest time.  Lists can implement queues, but append is a poor means\nof adding elements to the tail.\n\nOur functional arrays are suitable, provided we\naugment them with a function to delete the first array element.  (See _ML\n  for the Working Programmer_ page 156.)  Each operation would take $O(\\log\nn)$ time for a queue of length $n$.\n\nWe shall describe a representation of queues that is purely functional, based\nupon lists, and efficient.  Operations take $O(1)$ time when “amortized”:\naveraged over the lifetime of a queue.\n\nA conventional programming technique is to represent a queue by an array.  Two\nindices point to the front and back of the queue, which may wrap around the\nend of the array.  The coding is somewhat tricky.  Worse, the length of the\nqueue must be given a fixed upper bound.\n"},{"cell_type":"markdown","metadata":{},"source":"## Efficient Functional Queues: Idea\n\n\n\n- Represent the queue $x_1\\; x_2\\; \\ldots\\; x_m\\; y_n\\; \\ldots\\; y_1$ by any _pair of lists_\n$$ ([x_1,x_2,\\ldots,x_m], \\; [y_1,y_2,\\ldots,y_n])$$\n- Add new items to the _rear list_\n- Remove items from _front list_ and if empty move _rear_ to _front_\n- _Amortized_ time per operation is $O(1)$\n\nQueues require efficient access at both ends: at the front, for removal, and\nat the back, for insertion.  Ideally, access should take constant time,\n$O(1)$.  It may appear that lists cannot provide such access.  If\n`enq(q, x)` performs `q@[x]`, then this operation will be $O(n)$.  We\ncould represent queues by reversed lists, implementing `enq(q, x)` by\n`x::q`, but then the `deq` and `qhd` operations would be\n$O(n)$.  Linear time is intolerable: a series of $n$ queue operations\ncould then require $O(n^2)$ time.\n\nThe solution is to represent a queue by a pair of lists, where\n$$ ([x_1,x_2,\\ldots,x_m], \\, [y_1,y_2,\\ldots,y_n]) $$\nrepresents the queue $x_1 x_2 \\ldots x_m y_n \\ldots y_1$.\n\nThe front part of the queue is stored in order, and the rear part is stored in\nreverse order.  The `enq` operation adds elements to the rear part\nusing cons, since this list is reversed; thus, `enq` takes constant\ntime.  The `deq` and `qhd` operations look at the front part,\nwhich normally takes constant time, since this list is stored in order.  But\nsometimes `deq` removes the last element from the front part; when this\nhappens, it reverses the rear part, which becomes the new front part.\n\n_Amortized_ time refers to the cost per operation averaged over the\nlifetime of any complete execution.  Even for the worst possible execution,\nthe average cost per operation turns out to be constant; see the analysis\nbelow.\n"},{"cell_type":"markdown","metadata":{},"source":"## Efficient Functional Queues: Code\n\n\n"},{"cell_type":"code","metadata":{},"source":"type 'a queue =\n| Q of 'a list * 'a list","outputs":[],"execution_count":180},{"cell_type":"code","metadata":{},"source":"let norm = function\n| Q ([], tls) -> Q (List.rev tls, [])\n| q -> q","outputs":[],"execution_count":181},{"cell_type":"code","metadata":{},"source":"let qnull = function\n| Q ([], []) -> true\n| _ -> false","outputs":[],"execution_count":182},{"cell_type":"code","metadata":{},"source":"let enq (Q (hds, tls)) x = norm (Q (hds, x::tls))","outputs":[],"execution_count":183},{"cell_type":"code","metadata":{},"source":"exception Empty","outputs":[],"execution_count":184},{"cell_type":"code","metadata":{},"source":"let deq = function\n| Q (x::hds, tls) -> norm (Q (hds, tls))\n| _ -> raise Empty","outputs":[],"execution_count":185},{"cell_type":"code","metadata":{},"source":"let qempty = Q ([], [])","outputs":[],"execution_count":186},{"cell_type":"code","metadata":{},"source":"let qhd = function\n| Q (x::_, _) -> x\n| _ -> raise Empty","outputs":[],"execution_count":187},{"cell_type":"markdown","metadata":{},"source":"\nThe datatype of queues prevents confusion with other pairs of lists.  The empty\nqueue has both parts empty.\n\nThe function `norm` puts a queue into normal form, ensuring that the front part\nis never empty unless the entire queue is empty.  Functions `deq` and `enq`\ncall `norm` to normalise their result.\n\nBecause queues are in normal form, their head is certain to be in their\nfront part, so `qhd` looks there.\n\nLet us analyse the cost of an execution comprising (in any possible order) $n$\n`enq` operations and $n$ `deq` operations, starting with an\nempty queue.  Each `enq` operation will perform one cons, adding an\nelement to the rear part.  Since the final queue must be empty, each element\nof the rear part gets transferred to the front part.  The corresponding\nreversals perform one cons per element.  Thus, the total cost of the series of\nqueue operations is $2n$ cons operations, an average of 2 per operation.  The\namortized time is $O(1)$.\n\nThere is a catch.  The conses need not be distributed evenly; reversing a long\nlist could take up to $n-1$ of them.  Unpredictable delays make the approach\nunsuitable for _real-time programming_ where deadlines must be met.\n"},{"cell_type":"markdown","metadata":{},"source":"## Breadth-First Tree Traversal --- Using Queues\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec breadth q =\n  if qnull q then []\n  else\n    match qhd q with\n    | Lf -> breadth (deq q)\n    | Br (v, t, u) -> v :: breadth (enq (enq (deq q) t) u)","outputs":[],"execution_count":188},{"cell_type":"markdown","metadata":{},"source":"\nThis function implements the same algorithm as `nbreadth` but uses a different\ndata structure.  It represents queues using type `queue` instead of type\n`list`.\n\nTo compare their efficiency, I applied both functions to the full binary tree\nof depth 12, which contains 4095 labels.  The function `nbreadth` took 30\nseconds while `breadth` took only 0.15 seconds: faster by a factor of 200.\n\nFor larger trees, the speedup would be greater.  Choosing the right data\nstructure pays handsomely.\n"},{"cell_type":"markdown","metadata":{},"source":"## Iterative deepening: Another Exhaustive Search\n\n\n\n\n- Breadth-first search examines $O(b^d)$ nodes:\n$$ 1 + b + \\cdots + b^d = {b^{d+1}-1 \\over b-1}\n  \\qquad \\begin{array}[c]{r@{}l}\n            b & {} = \\hbox{branching factor}\\\\\n            d & {} = \\hbox{depth}\n          \\end{array}\n$$\n- Recompute nodes at depth $d$ instead of storing them\n- Time factor is $b/(b-1)$ if $b>1$; complexity is still $O(b^d)$\n- Space required at depth $d$ drops from $b^d$ to $d$\n\nBreadth-first search is not practical for infinite trees: it uses too much\nspace. Large parts of the tree have to be stored.\nConsider the slightly more general problem of searching trees whose\nbranching factor is $b$ (for binary trees, $b=2$).  Then breadth-first search\nto depth $d$ examines $(b^{d+1}-1)/(b-1)$ nodes, which is $O(b^d)$, ignoring\nthe constant factor of $b/(b-1)$.  Since all nodes that are examined are also\nstored, the space and time requirements are both $O(b^d)$.\n\n_Depth-first iterative deepening_ combines the space efficiency of\ndepth-first with the “nearest-first” property of breadth-first search.  It\nperforms repeated depth-first searches with increasing depth bounds, each time\ndiscarding the result of the previous search.  Thus it searches to depth 1,\nthen to depth 2, and so on until it finds a solution.  We can afford to\ndiscard previous results because the number of nodes is growing exponentially.\nThere are $b^{d+1}$ nodes at level $d+1$; if $b\\geq2$, this number actually\nexceeds the total number of nodes of all previous levels put together, namely\n$(b^{d+1}-1) / (b-1)$.\n\n[Korf shows](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.288) that the time needed for iterative deepening to reach\ndepth $d$ is only $b/(b-1)$ times that for breadth-first search, if $b>1$.\nThis is a constant factor; both algorithms have the same time complexity,\n$O(b^d)$.  In typical applications where $b\\geq2$ the extra factor of\n$b/(b-1)$ is quite tolerable.  The reduction in the space requirement is\nexponential, from $O(b^d)$ for breadth-first to $O(d)$ for iterative\ndeepening. Of course, this assumes that the tree itself is not stored in memory.\n"},{"cell_type":"markdown","metadata":{},"source":"## Another Abstract Data Type: Stacks\n\n\n\n- `empty` is the _empty stack_\n- `null` tests whether a stack is empty\n- `top` returns the element at the _top_ of a stack\n- `pop` discards the element at the _top_ of a stack\n- `push` adds an element at the _top_ of a stack\n\nA _stack_ is a sequence such that items can be added or removed from the head\nonly.  A stack obeys a Last-In-First-Out (LIFO) discipline: the item next to be\nremoved is the one that has been in the queue for the _shortest_ time.  Lists\ncan easily implement stacks because both `cons` and `hd` affect the head.  But\nunlike lists, stacks are often regarded as an imperative data structure: the\neffect of `push` or `pop` is to change an existing stack, not return a new one.\n\nIn conventional programming languages, a stack is often implemented by storing\nthe elements in an array, using a variable (the “stack pointer”) to count them.\nMost language processors keep track of recursive function calls using an\ninternal stack.\n"},{"cell_type":"markdown","metadata":{},"source":"## A Survey of Search Methods\n\n\n\n- Depth-first: use a _stack_  (efficient but incomplete)\n- Breadth-first: use a _queue_ (uses too much space!)\n- Iterative deepening: use depth-first to get benefits of breadth-first (trades time for space)\n- Best-first: use a _priority queue_ (heuristic search)\n\nThe data structure determines the search!\n\nSearch procedures can be classified by the data structure used to store\npending subtrees.  Depth-first search stores them on a stack, which is\nimplicit in functions like `inorder`, but can be made explicit.\nBreadth-first search stores such nodes in a queue.\n\nAn important variation is to store the nodes in a priority queue, which\nis an ordered sequence.  The priority queue applies some sort of ranking\nfunction to the nodes, placing higher-ranked nodes before lower-ranked ones.\nThe ranking function typically estimates the distance from the node to a\nsolution.  If the estimate is good, the solution is located swiftly.  This\nmethod is called best-first search.\n\nThe priority queue can be kept as a sorted list, although this is slow.\nBinary search trees would be much better on average, and fancier data\nstructures improve matters further.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 10.1\n\n\nSuppose that we have an implementation of queues, based on binary trees, such that each operation\ntakes logarithmic time in the worst case. Outline the advantages and drawbacks of such an\nimplementation compared with one presented above.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 10.2\n\n\nThe traditional way to implement queues uses a fixed-length array. Two indices into the array\nindicate the start and end of the queue, which wraps around from the end of the array to the start.\nHow appropriate is such a data structure for implementing breadth-first search?\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 10.3\n\n\nWrite a version of the function `breadth` using a nested `let` construction rather than\n`match`.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 10.4\n\n\nIterative deepening is inappropriate if $b\\approx1$, where $b$ is the branching factor. What search\nstrategy is appropriate in this case?\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 10.5\n\n\nConsider the following OCaml function."},{"cell_type":"code","metadata":{},"source":"let next n = [2 * n; 2 * n + 1]","outputs":[],"execution_count":189},{"cell_type":"markdown","metadata":{},"source":"If we regard it as representing a tree, where the subtrees are computed from the current label, what\ntree does `next 1` represent?\n"},{"cell_type":"markdown","metadata":{},"source":"# Lecture 11: Elements of Procedural Programming\n\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"## Procedural Programming\n\n\n\n- Procedural programs can change the machine state.\n- They can interact with its environment\n- They use control structures like branching, iteration and procedures.\n\nThey use data abstractions of the computer’s memory:\n- _references_ to memory cells\n- _arrays_ that are blocks of memory cells\n- _linked structures_ such as _linked lists_\n\nProcedural programming is programming in the traditional sense of the word.  A\nprogram _state_ is repeatedly transformed by the execution of\n_commands_ or _statements_.  A state change might be local to the\nmachine and consist of updating a variable or array.  A state change might\nconsist of sending data to the outside world.  Even reading data counts as a\nstate change, since this act normally removes the data from the environment.\n\nProcedural programming languages provide primitive commands and control\nstructures for combining them.  The primitive commands include\n_assignment_ for updating variables, and various _input/output_\ncommands for communication.  Control structures include `if` and\n`match` constructs for conditional execution, and repetitive constructs\nsuch as `while`.  Programmers can package up their own commands as\n_procedures_ taking arguments.  The need for such “subroutines” was\nevident from the earliest days; they represent one of the first\nexamples of abstraction in programming languages.\n\nOCaml makes no distinction between commands and expressions. OCaml provides built-in\n‘functions’ to perform assignment and communication, and these can be used\nin the traditional (procedural) style. OCaml programmers often follow a\nfunctional style for most internal computations and use imperative features\nmainly for communication with the outside world.\n"},{"cell_type":"markdown","metadata":{},"source":"## OCaml Primitives for References\n\n\n\n| Syntax   | Effect  |\n| -- | -- |\n| `ref E` | _create_ a reference with _initial contents_ = value of `E` |\n| `!P`    | return _current contents_ of reference `P` |\n| `P := E` | _update_ contents of `P` to value of `E` |\n\n\nThe above text presents the OCaml primitives, but most languages have analogues of\nthem, often heavily disguised.  We need a means of creating references (or\nallocating storage), getting at the current contents of a reference cell, and\nupdating that cell.\n\nThe function `ref` creates references (also called\n“locations”).  Calling `ref` allocates a new location in memory.\nInitially, this location holds the value given by\nexpression $E$.\n\nThe function `!`, when applied to a reference, returns its contents.\nThis operation is called _dereferencing_.  Clearly `!` is not a\nmathematical function; its result depends upon the store.\n\nThe assignment `P:=E` evaluates expression $P$, which must return a\nreference $p$, and $E$.  It stores at address $p$ the value of $E$.\nSyntactically, `:=` is a function and `P:=E` is an\nexpression, even though it updates the store.  Like many functions that change\nthe state, it returns the value `()` of type `unit`.\n\nIf $\\tau$ is some OCaml type, then $\\tau$ `ref` is the type of references\nto cells that can hold values of $\\tau$.  Please do not confuse the type\n`ref` with the function `ref`.  This table of the primitive\nfunctions and their types might be useful:\n\n| Syntax | OCaml Type |\n| --- | -------------- |\n| `ref` | `'a -> 'a ref` |\n| `!` | `'a ref -> 'a `|\n| `:=` | `'a ref -> 'a -> unit` |\n"},{"cell_type":"markdown","metadata":{},"source":"## Trying Out References\n\n\n"},{"cell_type":"code","metadata":{},"source":"let p = ref 5 (* create a reference *)","outputs":[],"execution_count":190},{"cell_type":"code","metadata":{},"source":"p := !p + 1   (* p now holds value 6 *)","outputs":[],"execution_count":191},{"cell_type":"code","metadata":{},"source":"let ps = [ ref 77; p ]","outputs":[],"execution_count":192},{"cell_type":"code","metadata":{},"source":"List.hd ps := 3","outputs":[],"execution_count":193},{"cell_type":"code","metadata":{},"source":"ps","outputs":[],"execution_count":194},{"cell_type":"markdown","metadata":{},"source":"\nThe first line declares `p` to hold a reference to an integer,\ninitially 5.  Its type is `int ref`, not just `int`, so it\nadmits assignment.  Assignment never changes `let` bindings: they are\n_immutable_.  The identifier `p` will always denote the reference\nmentioned in its declaration unless superseded by a new usage of `p`.\nOnly the _contents_ of the reference is mutable.\n\nOCaml displays a reference value as `{contents=v}`, where value $v$ is the\ncontents.  This notation is readable but gives us no way of telling whether\ntwo references holding the same value are actually the same reference.  To\ndisplay a reference as a machine address has obvious drawbacks!\n\nIn the first assignment, the expression `!p` yields the reference’s\ncurrent contents, namely 5.  The assignment changes the contents of `p`\nto 6.  Most languages do not have an explicit dereferencing operator\n(like `!`) because of its inconvenience.  Instead, by convention,\noccurrences of the reference on the _left-hand_ side of the `:=`\ndenote locations and those on the _right-hand_ side denote the contents.\nA special ‘address of’ operator may be available to override the convention\nand make a reference on the right-hand side to denote a location.  Logically\nthis is a mess, but it makes programs shorter.\n\nThe list `ps` is declared to hold a new reference (initially\ncontaining 77) as well as `p`.  Then the new reference is\nupdated to hold 3.  The assignment to `hd ps` does _not_\nupdate `ps`, only the contents of a reference in that list.\n"},{"cell_type":"markdown","metadata":{},"source":"## Commands: Expressions with Effects\n\n\n\n- Basic commands update references, write to files, etc.\n- $C_1 ; \\ldots ; C_n$ causes a series of expressions to be evaluated and returns the value of $C_n$.\n- A typical command returns the empty tuple: `()`\n- `if` $B$ `then` $C_1$ `else` $C_2$ behaves like the traditional control structure if $C_1$ and $C_2$ have effects.\n- Other OCaml constructs behave naturally with commands, including `match` expressions and recursive functions.\n\nWe use the term _command_ informally to refer to an expression that has an\neffect on the state. All expressions denote some value, but they can return\n`()`, which conveys no actual information.\n\nWe need a way to execute one command after another.\nThe construct $C_1 ; \\ldots ; C_n$ evaluates the expressions $C_1$\nto $C_n$ in the order given and returns the value of $C_n$.  The values\nof the other expressions are discarded; their only purpose is to change the\nstate.\n\nCommands may be used with `if` and `match` much as in conventional languages.\nOCaml functions play the role of procedures.\n\nOther languages that combine the functional and imperative programming\nparadigms include Lisp (and its dialect Scheme), Scala, and even a\nsystems programming language, BLISS (now long extinct).\n"},{"cell_type":"markdown","metadata":{},"source":"## Iteration: the `while` command\n\n\n"},{"cell_type":"code","metadata":{},"source":"let tlopt = function\n| [] -> None\n| _::xs -> Some xs","outputs":[],"execution_count":195},{"cell_type":"code","metadata":{},"source":"let length xs =\n  let lp  = ref xs in (* list of uncounted elements *)\n  let np  = ref 0  in (* accumulated count *)\n  let fin = ref false in\n  while not !fin do\n    match tlopt !lp with\n    | None -> fin := true\n    | Some xs ->\n        lp := xs;\n        np := 1 + !np\n  done;\n  !np (* the final count is returned *)","outputs":[],"execution_count":196},{"cell_type":"markdown","metadata":{},"source":"\nOnce we can change the state, we need to do so repeatedly.  Recursion can\nserve this purpose, but having to declare a procedure for every loop is\nclumsy, and compilers for conventional languages seldom exploit\ntail-recursion.\n\nEarly programming languages provided little support for repetition.  The\nprogrammer had to set up loops using goto commands, exiting the loop using\nanother goto controlled by an `if`.  Modern languages provide a\nconfusing jumble of looping constructs, the most fundamental of which is\n`while B do C`.  The boolean expression $B$ is evaluated,\nand if true, command $C$ is executed and the command repeats.  If $B$\nevaluates to false then the `while` command terminates, perhaps without\nexecuting $C$ even once.\n\nOCaml’s main looping construct is `while`, which returns the value `()`.  The\nfunction `length` declares references to hold the list under\nexamination (`lp`) and number of elements counted so far (`np`) as well\nas whether the end of the list has been reached (the boolean reference `fin`).\nWhile the list is non-empty, we skip over one more element (by setting it to\nits tail) and count that element.\n\nThe body of the `while` loop first checks to see if the end of the list has\nbeen reached, in which case it sets the `fin` variable to true.  If there is a\ntail value, then two assignments are executed in sequence.  The `lp` reference\nis set to the tail of the list, and the `np` reference integer is incremented\nby one.  When the while loop terminates due to the `fin` variable being set to\ntrue, the expression `!np` returns the computed length as the function’s\nresult.\n"},{"cell_type":"markdown","metadata":{},"source":"## Private, Persistent References\n\n\n"},{"cell_type":"code","metadata":{},"source":"exception TooMuch of int","outputs":[],"execution_count":197},{"cell_type":"code","metadata":{},"source":"let makeAccount initBalance =\n  let balance = ref initBalance in\n  let withdraw amt =\n     if amt > !balance then\n       raise (TooMuch (amt - !balance))\n     else begin\n       balance := !balance - amt;\n       !balance\n     end\n  in\n  withdraw","outputs":[],"execution_count":198},{"cell_type":"markdown","metadata":{},"source":"\nAs you may have noticed, OCaml’s programming style looks clumsy compared with\nthat of languages like C.  OCaml omits the defaults and abbreviations they\nprovide to shorten programs.  However, OCaml’s explicitness makes it ideal for\nteaching the fine points of references and arrays.  OCaml’s references are more\nflexible than those found in other languages.\n\nThe function `makeAccount` models a bank.  Calling the function with a\nspecified initial balance creates a new reference `balance`) to\nmaintain the account balance and returns a function (`withdraw`) having\nsole access to that reference.  Calling `withdraw` reduces the balance\nby the specified amount and returns the new balance.  You can pay money in by\nwithdrawing a negative amount.  The `if`-construct prevents the account\nfrom going overdrawn, raising an exception.\n\nLook at the $\\tt (E_1; E_2)$ construct in the _else_ part above.\nThe first expression updates the account balance and returns the trivial\nvalue ().  The second expression, `!balance`, returns the current\nbalance but does not return the reference itself: that would allow\nunauthorised updates.\n\nThis example is based on one by Dr A C Norman.\n"},{"cell_type":"markdown","metadata":{},"source":"## Two Bank Accounts\n\n\n"},{"cell_type":"code","metadata":{},"source":"let student = makeAccount 500","outputs":[],"execution_count":199},{"cell_type":"code","metadata":{},"source":"let director = makeAccount 4000000;","outputs":[],"execution_count":200},{"cell_type":"code","metadata":{},"source":"student 5        (* coach fare *)","outputs":[],"execution_count":201},{"cell_type":"code","metadata":{},"source":"director 150000  (* Tesla *)","outputs":[],"execution_count":202},{"cell_type":"code","metadata":{},"source":"student 500      (* oh oh *)","outputs":[],"execution_count":203},{"cell_type":"markdown","metadata":{},"source":"\nEach call to `makeAccount` returns a copy of `withdraw` holding\na _fresh_ instance of the reference `balance`.  As with a real bank\npass-book, there is no access to the account balance except via the\ncorresponding `withdraw` function.  If that function is discarded, the\nreference cell becomes unreachable; the computer will eventually reclaim it,\njust as banks close down dormant accounts.\n\nHere we see two people managing their accounts.  For better or worse, neither\ncan take money from the other.\n\nWe could generalise _makeAccount_ to return several functions that\njointly manage information held in shared references.  The functions might be\npackaged using OCaml records, which are not discussed in this course.\nMost procedural languages do not properly support the concept of private\nreferences, although _object-oriented_ languages take them as a basic theme.\n"},{"cell_type":"markdown","metadata":{},"source":"## OCaml Primitives for Arrays\n\n\n"},{"cell_type":"code","metadata":{},"source":"[|\"a\"; \"b\"; \"c\"|] (* allocate a fresh string array *)","outputs":[],"execution_count":204},{"cell_type":"code","metadata":{},"source":"Array.make 3 'a'   (* array of size 10 with cell containing 'a' *)","outputs":[],"execution_count":205},{"cell_type":"code","metadata":{},"source":"let aa = Array.init 5 (fun i -> i * 10) (* array of size 5 initialised to (fn i) *)","outputs":[],"execution_count":206},{"cell_type":"code","metadata":{},"source":"Array.get aa 3  (* retrieve the 4th cell in the array *)","outputs":[],"execution_count":207},{"cell_type":"code","metadata":{},"source":"Array.set aa 3 42 (* set the 4th cell's value to 42 *)","outputs":[],"execution_count":208},{"cell_type":"markdown","metadata":{},"source":"\nThere are many other array operations in the `Array` module in the OCaml standard\nlibrary.\n"},{"cell_type":"code","metadata":{},"source":"Array.make","outputs":[],"execution_count":209},{"cell_type":"code","metadata":{},"source":"Array.init","outputs":[],"execution_count":210},{"cell_type":"code","metadata":{},"source":"Array.get","outputs":[],"execution_count":211},{"cell_type":"code","metadata":{},"source":"Array.set","outputs":[],"execution_count":212},{"cell_type":"markdown","metadata":{},"source":"\nOCaml arrays are like references that hold several elements instead of one.  The\nelements of an $n$-element array are designated by the integers from 0\nto $n-1$.  The $i$th array element is usually written $A.[i]$.\nIf $\\tau$ is a type then $\\tau$ `array` is the\ntype of arrays (of any size) with elements from $\\tau$.\n\nCalling `Array.init i n` creates an array of the size specified in $i$\nby expression $n$.  Initially, element $A.[i]$ holds the value of $f(i)$ for $i=0$, \\ldots,~$n-1$.\nLike `ref`, it allocates mutable storage to hold the specified values.\n\nCalling `Array.get A i` returns the contents of $A.[i]$.\n\nCalling `Array.set A i E` modifies the array $A$ by storing the\nvalue of $E$ as the new contents of $A[i]$; it returns `()` as its value.\n\nOCaml’s arrays are much safer than C’s. In C, an array is nothing more than an\naddress indicating the start of a storage area. Nothing indicates the size of\nthe area.  Therefore C programs are vulnerable to _buffer overrun attacks:_ an\nattacker sends more data than the receiving program expects, overrunning the\narea of storage set aside to hold it. The attack eventually overwrites the\nprogram itself, replacing it with code controlled by the attacker.\n"},{"cell_type":"markdown","metadata":{},"source":"## Array Examples\n\n\n\nIn the following session, the identifier `ar` is bound to an array of 20 elements, which\nare initially set to the squares of their subscripts.  The array’s third\nelement (which actually has subscript 2) is inspected and found to be four. The\nsecond call to `Array.get` supplies a subscript that is out of range, so OCaml\nrejects it.\n"},{"cell_type":"code","metadata":{},"source":"let ar = Array.init 20 (fun i -> i * i)","outputs":[],"execution_count":213},{"cell_type":"code","metadata":{},"source":"Array.get ar 2","outputs":[],"execution_count":214},{"cell_type":"code","metadata":{},"source":"Array.get ar 20","outputs":[],"execution_count":215},{"cell_type":"code","metadata":{},"source":"Array.set ar 2 33; ar","outputs":[],"execution_count":216},{"cell_type":"markdown","metadata":{},"source":"\nBy calling `Array.set`, we then modify the element with subscript 2. Note\nhowever that we cannot modify the array’s length. If we outgrow the array, we\nhave to create a new one, copy the data into it, and then forget the old array.\nTypically the new array would be double the size of the old one, so that the\ncost of copying is insignificant.\n\nOCaml provides numerous operators for modifying, computing over and searching in\narrays. Many are analogous to functions on lists. For example,\n`Array.exists` takes a boolean-valued function and returns `true` if an\narray element satisfies it.\n"},{"cell_type":"code","metadata":{},"source":"Array.exists (fun i -> i > 200) ar","outputs":[],"execution_count":217},{"cell_type":"code","metadata":{},"source":"Array.exists (fun i -> i < 0) ar","outputs":[],"execution_count":218},{"cell_type":"markdown","metadata":{},"source":"## References: OCaml _vs_ conventional languages\n\n\n\n- We must write `!p` to get the _contents_ of `p`\n- We write just `p` for the _address_ of `p`\n- We can store private reference cells (like `balance`) in functions---analogous to elements of _object-oriented programming_\n- OCaml’s assignment syntax is $\\tt V \\, := \\, E$ instead of $V$ = $E$\n- OCaml has few control structures: `while`, `match`, `if` and `for` (the latter is not covered in this course)\n- OCaml has syntax for updating an array via the `a.[i] <- v` syntax which is the same as `Array.set a i v`.\n\nConventional syntax for variables and assignments has hardly changed since\nFortran, the first high-level language. In conventional languages,\nvirtually all variables can be updated.  We declare something like\n`p: int`, mentioning no reference type even if the language provides\nthem.  If we do not specify an initial value, we may get whatever bits were\npreviously at that address.  Illegal values arising from uninitialised\nvariables can cause errors that are almost impossible to diagnose.\n\nDereferencing operators (like OCaml’s `!`) are especially unpopular, because\nthey clutter the program text. Virtually all programming languages make dereferencing\nimplicit (that is, automatic).\n\nIt is generally accepted these days that a two-dimensional array $A$ is nothing\nbut an array of arrays.  An assignment to such an array is typically written\nsomething like $A[i,j] {:=} x$; in C, the syntax is `A[i][j] = x`. Higher\ndimensions are treated analogously.  The corresponding OCaml code can either\ndeclare an array of arrays, or use the `A.(i)` syntax to calculate the linear\noffset into a single array.\n\nYou can use the constructs we have learnt to easily create linked (mutable) lists as\nan alternative to arrays.\n"},{"cell_type":"code","metadata":{},"source":"type 'a mlist =\n| Nil\n| Cons of 'a * 'a mlist ref","outputs":[],"execution_count":219},{"cell_type":"markdown","metadata":{},"source":"\nIt is worth mentioning that OCaml’s references fully suffice for coding the sort of linked data structures\ntaught in algorithms courses, and is illustrated in the figure above. The\nprogramming style is a little different from the usual, but the principles are\nthe same.  OCaml also provides comprehensive input/output primitives for various\ntypes of file and operating system.\n\nOCaml’s system of modules include _structures,_ which can be seen as encapsulated\ngroups of declarations, and _signatures,_ which are specifications of\nstructures listing the name and type of each component.  Finally, there are\n_functors,_ which are analogous to functions that combine a number of argument\nstructures, and which can be used to plug program components together. These\nprimitives are useful for managing large programming projects.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 11.1\n\n\nComment, with examples, on the differences between an `int ref list` and an `int list ref`.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 11.2\n\n\nWrite a version of function `power` (Lecture 1) using `while` instead of recursion.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 11.3\n\n\nWhat is the effect of `while `$C_1$`; `$B$` do `$C_2$` done`?\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 11.4\n\n\nWrite a function to exchange the values of two references, `xr` and `yr`.\n"},{"cell_type":"markdown","metadata":{},"source":"## Exercise 11.5\n\n\nArrays of multiple dimensions are represented in OCaml by arrays of arrays. Write functions to\n(a) create an $n\\times n$ identity matrix, given $n$, and\n(b) to transpose an $m\\times n$ matrix. Identity matrices have the following form:\n$$\n\\left( {\n\\begin{array}{cccc}\n   1 & 0 &  \\cdots  & 0  \\\\\n   0 & 1 &  \\cdots  & 0  \\\\\n    \\vdots  &  \\vdots  &  \\ddots  &  \\vdots   \\\\\n   0 & 0 &  \\cdots  & 1  \\\\\n \\end{array}\n } \\right)\n$$"}]}