{"metadata":{"kernelspec":{"display_name":"OCaml 4.07.1","language":"OCaml","name":"ocaml-jupyter"},"language_info":{"name":"OCaml","version":"4.07.1","codemirror_mode":"text/x-ocaml","file_extension":".ml","mimetype":"text/x-ocaml","nbconverter_exporter":null,"pygments_lexer":"OCaml"}},"nbformat":4,"nbformat_minor":2,"cells":[{"cell_type":"markdown","metadata":{},"source":"\nThis course has two aims. The first is to teach programming. The second is to\npresent some fundamental principles of computer science, especially algorithm\ndesign. Most students will have some programming experience already, but there\nare few people whose programming cannot be improved through greater knowledge\nof basic principles. Please bear this point in mind if you have extensive\nexperience and find parts of the course rather slow.\n\nThe programming in this course is based on the language [OCaml](https://ocaml.org)\nand mostly concerns the functional programming style. Functional programs tend\nto be shorter and easier to understand than their counterparts in conventional\nlanguages such as C. In the space of a few weeks, we shall cover many\nfundamental data structures and learn basic methods for estimating efficiency.\n\n**this is a work-in-progress port of Lawrence C. Paulson's 1819 Cambridge\ncourse notes**\n"},{"cell_type":"markdown","metadata":{},"source":"\n- Computers: a child can use them; **nobody** can fully understand them!\n- We can master complexity through levels of abstraction.\n- Focus on 2 or 3 levels at most!\n\n**Recurring issues:**\n- what services to provide at each level\n- how to implement them using lower-level services\n- the interface: how the two levels should communicate\n\nA basic concept in computer science is that large systems can only be\nunderstood in levels, with each level further subdivided into functions or\nservices of some sort. The interface to the higher level should supply the\nadvertised services. Just as important, it should block access to the means by\nwhich those services are implemented. This _abstraction barrier_ allows one\nlevel to be changed without affecting levels above. For example, when a\nmanufacturer designs a faster version of a processor, it is essential that\nexisting programs continue to run on it. Any differences between the old and\nnew processors should be invisible to the program.\n\nModern processors have elaborate specifications, which still sometimes leave\nout important details. In the old days, you then had to consult the circuit\ndiagrams.\n"},{"cell_type":"markdown","metadata":{},"source":"\n- Abstract level: dates over a certain interval\n- Concrete level: typically 6 characters: `YYMMDD` (where each character is represented by 8 bits)\n- Date crises caused by __inadequate__ internal formats:\n  * Digital’s PDP-10: using 12-bit dates (good for at most 11 years)\n  * 2000 crisis: 48 bits could be good for lifetime of universe!\n\nDigital Equipment Corporation’s date crisis occurred in 1975.  The\nPDP-10 was a 36-bit mainframe computer. It represented dates using a 12-bit\nformat designed for the tiny PDP-8. With 12 bits, one can distinguish\n$2^{12} = 4096$ days or 11 years.\n\nThe most common industry format for dates uses six characters: two for the\nyear, two for the month and two for the day. The most common \"solution\" to the\nyear 2000 crisis is to add two further characters, thereby altering file sizes.\nOthers have noticed that the existing six characters consist of 48 bits,\nalready sufficient to represent all dates over the projected lifetime of the\nuniverse: $2^{48}$ = $2.8 * 1014$ days = $7.7 * 1011$ years!\n\nMathematicians think in terms of unbounded ranges, but the representation we\nchoose for the computer usually imposes hard limits. A good programming\nlanguage like OCaml lets one easily change the representation used in the\nprogram.  But if files in the old representation exist all over the place,\nthere will still be conversion problems. The need for compatibility with older\nsystems causes problems across the computer industry.\n"},{"cell_type":"markdown","metadata":{},"source":"\nComputers have integers like `1066` and floats like $1.066 x 10^3$.\nA floating-point number is represented by two integers.\nThe concept of _data type_ involves:\n* how a value is represented inside the computer\n* the suite of operations given to programmers\n* valid and invalid (or exceptional) results, such as “infinity”\nComputer arithmetic can yield _incorrect answers_!\n\nIn science, numbers written with finite precision and a decimal exponent are\nsaid to be in _standard form_. The computational equivalent is the _floating\npoint number_. These are familiar to anybody who has used a scientific\ncalculator.  Internally, a float consists of two integers.\n\nBecause of its finite precision, floating-point computations are potentially\ninaccurate. To see an example, use your nearest electronic calculator to\ncompute $(2^{1/10000})10000$. I get $1.99999959$! With certain computations,\nthe errors spiral out of control. Many programming languages fail to check\nwhether even integer computations fall within the allowed range: you can add\ntwo positive integers and get a negative one!\n\nMost computers give us a choice of precisions. In 32-bit precision, integers\ntypically range from $2^{31} − 1$ (namely $2,147,483,647$) to $−2^{31}$; reals\nare accurate to about six decimal places and can get as large as 1035 or so.\nFor reals, 64-bit precision is often preferred. Early languages like Fortran\nrequired variables to be declared as `INTEGER`, `REAL` or `COMPLEX` and barred\nprogrammers from mixing numbers in a computation. Nowadays, programs handle\nmany different kinds of data, including text and symbols. The concept of a\n_data type_ can ensure that different types of data are not combined in a\nsenseless way.\n\nInside the computer, all data are stored as bits. In most programming\nlanguages, the compiler uses types to generate correct machine code, and types\nare not stored during program execution. In this course, we focus almost\nentirely on programming in a high-level language: OCaml.\n"},{"cell_type":"markdown","metadata":{},"source":"\n- to describe a computation so that it can be done _mechanically_:\n  * Expressions compute values.\n  * Commands cause effects.\n- to do so efficiently and **correctly**, giving the right answers quickly\n- to allow easy modification as needs change\n  * Through an orderly _structure_ based on abstraction principles\n  * Such as modules or (Java) classes\n\nProgramming _in-the-small_ concerns the writing of code to do simple, clearly\ndefined tasks. Programs provide expressions for describing mathematical\nformulae and so forth. (This was the original contribution of FORTRAN, the\nFORmula TRANslator.) Commands describe how control should flow from one part of\nthe program to the next.\n\nAs we code layer upon layer, we eventually find ourselves programming\n_in the large_ : joining large modules to solve some messy task. Programming\nlanguages have used various mechanisms to allow one part of the program to\nprovide interfaces to other parts. Modules encapsulate a body of code, allowing\noutside access only through a programmer-defined interface. _Abstract Data\nTypes_ are a simpler version of this concept, which implement a single concept\nsuch as dates or floating-point numbers.\n\n_Object-oriented programming_ is the most complicated approach to modularity.\n_Classes_ define concepts, and they can be built upon other classes. Operations\ncan be defined that work in appropriately specialized ways on a family of\nrelated classes. _Objects_ are instances of classes and hold the data that is\nbeing manipulated.\n\nThis course does not cover OCaml's sophisticated module system, which can do\nmany of the same things as classes. You will learn all about objects when you\nstudy Java.\n"},{"cell_type":"markdown","metadata":{},"source":"\n* Why Program in ML?\n* It is interactive.\n* It has a flexible notion of _data type_.\n* It hides the underlying hardware: _no crashes_.\n* Programs can easily be understood mathematically.\n* It distinguishes naming something from _updating memory_.\n* It manages storage for us.\n\nStandard ML is the outcome of years of research into\nprogramming languages. It is unique, defined using a mathematical formalism (an\noperational semantics) that is both precise and comprehensible. Several\nsupported compilers are available, and thanks to the formal definition, there\nare remarkably few incompatibilities among them. _(TODO edit)_\n\nBecause of its connection to mathematics, ML programs can be designed and\nunderstood without thinking in detail about how the computer will run them.\nAlthough a program can abort, it cannot crash: it remains under the control of\nthe OCaml system. It still achieves respectable efficiency and provides\nlower-level primitives for those who need them. Most other languages allow\ndirect access to the underlying machine and even try to execute illegal\noperations, causing crashes.\n\nThe only way to learn programming is by writing and running programs. This\nweb notebook provides an interactive environment where you can modify\nthe example fragments and see the results for yourself.  You should also\nconsider installing OCaml on your own computer so that you try more\nadvanced programs locally.\n"},{"cell_type":"code","metadata":{},"source":"let pi = 3.14159265358979","outputs":[],"execution_count":1},{"cell_type":"markdown","metadata":{},"source":"\nThe first line of this simple session is a _value declaration_. It makes the\nname `pi` stand for the floating point number `3.14159`. (Such names are called\n_identifiers_.)  OCaml echoes the name (`pi`) and type (`float`) of the\ndeclared identifier.\n"},{"cell_type":"code","metadata":{},"source":"pi *. 1.5 *. 1.5","outputs":[],"execution_count":2},{"cell_type":"markdown","metadata":{},"source":"\nThe second line computes the area of the circle with radius `1.5` using the\nformula $A = \\pi r^2$. We use `pi` as an abbreviation for `3.14159`.\nMultiplication is expressed using `*.`, which is called an _infix operator_\nbecause it is written between its two operands.\n\nOCaml replies with the computed value (about `7.07`) and its type (again `float`).\n"},{"cell_type":"code","metadata":{},"source":"let area r = pi *. r *. r","outputs":[],"execution_count":3},{"cell_type":"markdown","metadata":{},"source":"\nTo work abstractly, we should provide the service \"compute the area of a\ncircle,\" so that we no longer need to remember the formula. This sort of\nencapsulated computation is called a _function_. The third line declares the\nfunction `area`. Given any floating point number `r`, it returns another\nfloating point number computed using the `area` formula; note that the function\nhas type `float -> float`.\n"},{"cell_type":"code","metadata":{},"source":"area 2.0","outputs":[],"execution_count":4},{"cell_type":"markdown","metadata":{},"source":"\nThe fourth line calls the function `area` supplying `2.0` as the argument. A\ncircle of radius `2` has an area of about `12.6`. Note that brackets around a\nfunction argument are not necessary.\n\nThe function uses `pi` to stand for `3.14159`. Unlike what you may have seen in\nother programming languages, `pi` cannot be \"assigned to\" or otherwise updated.\nIts meaning within `area` will persist even if we issue a new `let` declaration\nfor `pi` afterwards.\n"},{"cell_type":"code","metadata":{},"source":"let rec npower x n =\n  if n = 0 then 1.0\n  else x *. npower x (n-1)","outputs":[],"execution_count":5},{"cell_type":"markdown","metadata":{},"source":"\nThe function `npower` raises its real argument `x` to the power `n`, a\nnon-negative integer. The function is _recursive_: it calls itself. This concept\nshould be familiar from mathematics, since exponentiation is defined by the\nrules shown above. You may also have seen recursion in the product rule for\ndifferentiation: $(u · v)′ = u · v′ + u′ · v.$.\n\nIn finding the derivative of $u.v$, we recursively find the derivatives of $u$\nand $v$, combining them to obtain the desired result. The recursion is\nmeaningful because it terminates: we reduce the problem to two smaller\nproblems, and this cannot go on forever. The ML programmer uses recursion\nheavily.  For $n>=0$, the equation $x^{n+1} = x * x^n$ yields an obvious\ncomputation:\n\n$$ x^3 = x\\times x^2 = x\\times x\\times x^1 = x\\times x\\times x\\times x^0 = x\\times x\\times x $$\n\nThe equation clearly holds even for negative $n$. However, the corresponding\ncomputation runs forever:\n\n$$ x^{-1} = x\\times x^{-2} = x\\times x\\times x^{-3}=\\cdots $$\n\nNote that the function `npower` contains both an integer constant (0) and a\nfloating point constant (1.0). The decimal point makes all the difference. The\nML system will notice and ascribe different meaning to each type of constant.\n"},{"cell_type":"code","metadata":{},"source":"let square x = x *. x;","outputs":[],"execution_count":6},{"cell_type":"markdown","metadata":{},"source":"\nNow for a tiresome but necessary aside. In most languages, the types of\narguments and results must always be specified. ML is unusual that it normally\ninfers the types itself. However, sometimes ML could use a hint; function\n`square` above has a type constraint to say its result is a float.\n\nML can still infer the type even if you don't specify them, but in some cases\nit will use a more inefficient function than a specialised one.  Some languages\nhave just one type of number, converting automatically between different\nformats; this is slow and could lead to unexpected rounding errors.  Type\nconstraints are allowed almost anywhere. We can put one on any occurrence of x\nin the function. We can constrain the function’s result:\n"},{"cell_type":"code","metadata":{},"source":"let square (x:float) = x *. x","outputs":[],"execution_count":7},{"cell_type":"code","metadata":{},"source":"let square x : float = x *. x","outputs":[],"execution_count":8},{"cell_type":"markdown","metadata":{},"source":"\nML treats the equality and comparison test specially. Expressions like\n\n$$ if x=y then ...$$\n\nare fine provided `x` and `y` have the same type and equality testing is\npossible for that type. (We discuss equality further in a later lecture.)\nNote that `x <> y` is ML for `x  ̸= y`.\n\nA characteristic feature of the computer is its ability to test for conditions\nand act accordingly.  In the early days, a program might jump to a given\naddress depending on the sign of some number.  Later, John McCarthy defined\nthe _conditional expression_ to satisfy:\n\n$$if true then x else y = x$$\n$$if false then x else y = y$$\n\nML evaluates the expression $if B then E_1 else E_2$ by first evaluating $B$.\nIf the result is `true` then ML evaluates $E_1$ and otherwise $E_2$.  Only one\nof the two expressions $E_1$ and $E_2$ is evaluated!  If both were evaluated,\nthen recursive functions like `npower` above would run forever.\n\nThe _if-expression_ is governed by an expression of type `bool`, whose two\nvalues are `true` and `false`.  In modern programming languages, tests are not\nbuilt into \"conditional branch\" constructs but have an independent status.\nTests, or _Boolean expressions_, can be expressed using relational operators\nsuch as `<` and `=`. They can be combined using the Boolean operators for\nnegation (`not`), `and` (written as `&&`) and `or` (written as `||`).  New\nproperties can be declared as functions: here, to test whether an integer is\neven.\n\nFor large `n`, computing powers using $x^{n+1} = x\\times x^n$ is too slow to\nbe practical.  The equations above are much faster. Example:\n\n$$ 2^{12} = 4^6 = 16^3 = 16\\times 256^1 = 16\\times 256 = 4096. $$\n\nInstead of `n` multiplications, we need at most $2 lg n$ multiplications,\nwhere $lg n$ is the logarithm of $n$ to the base $2$.\n\nWe use the function `even`, declared previously, to test whether the\nexponent is even.  Integer division (`div`) truncates its result to an\ninteger: dividing $2n+1$ by 2 yields $n$.\n\nA recurrence is a useful computation rule only if it is bound to terminate.\nIf $n>0$ then $n$ is smaller than both $2n$ and $2n+1$.  After enough\nrecursive calls, the exponent will be reduced to $1$.  The equations also hold\nif $n\\leq0$, but the corresponding computation runs forever.\n\nOur reasoning assumes arithmetic to be _exact_. Fortunately, the calculation is\nwell-behaved using floating-point.\n\nTODO edit for OCaml. The negation of `x` is written `~x` rather than `-x`\nplease note.  Most languages use the same symbol for minus and subtraction,\nbut ML regards all operators, whether infix or not, as functions.  Subtraction\ntakes a pair of numbers, but minus takes a single number; they are distinct\nfunctions and must have distinct names.\n\nTODO edit for OCaml. Computer numbers have a finite range, which if exceeded gives rise to an\nOverflow error.  Some ML systems can represent integers of arbitrary size.\n\nIf integers and reals must be combined in a calculation, ML provides functions\nto convert between them:\n"},{"cell_type":"code","metadata":{},"source":"int_of_float ;;","outputs":[],"execution_count":9},{"cell_type":"code","metadata":{},"source":"int_of_float 3.14159 ;;","outputs":[],"execution_count":10},{"cell_type":"code","metadata":{},"source":"float_of_int ;;","outputs":[],"execution_count":11},{"cell_type":"code","metadata":{},"source":"float_of_int 3 ;;","outputs":[],"execution_count":12},{"cell_type":"markdown","metadata":{},"source":"\nML's libraries are organized using _modules_, so we many use compound\nidentifiers such as `Float.of_int` to refer to library functions.  In OCaml,\nlibrary units can also be loaded by commands such as `#require \"num\"`.  There\nare many thousands of library functions available in the OCaml ecosystem,\nincluding text-processing and operating systems functions in addition to the\nusual numerical ones.\n\nTODO summarise OCaml syntax.\n\n"},{"cell_type":"markdown","metadata":{},"source":"\nExpression evaluation concerns expressions and the values they return. This view of computation may seem to be too narrow. It is certainly far removed from computer hardware, but that can be seen as an advantage. For the traditional concept of computing solutions to problems, expression evaluation is entirely adequate.\n\nStarting with $E_0$, the expression $E_i$ is reduced to $E_{i+1}$ until this\nprocess concludes with a value~$v$.  A _value_ is something like a number\nthat cannot be further reduced.\n\nWe write $E\\red E'$ to say that $E$ is \\emph{reduced} to~$E'$.\nMathematically, they are equal: $E=E'$, but the computation goes from $E$ to\n$E'$ and never the other way around.\n\nComputers also interact with the outside world.  For a start, they need some\nmeans of accepting problems and delivering solutions.  Many computer systems\nmonitor and control industrial processes.  This role of computers is familiar\nnow, but was never envisaged in the early days. Computer pioneers focused on\nmathematical calculations.  Modelling interaction and control requires a notion\nof _states_ that can be observed and changed.  Then we can consider\nupdating the state by assigning to variables or performing input/output,\nfinally arriving at conventional programs as coded in C, for instance.\n\nFor now, we remain at the level of expressions, which is usually termed\n_functional programming_.\n\n"},{"cell_type":"code","metadata":{},"source":"let rec nsum n =\n  if n = 0 then 0\n           else n + nsum (n-1)","outputs":[],"execution_count":13},{"cell_type":"markdown","metadata":{},"source":"\nThe function call `nsum n` computes the sum `1+...+nz rather naively, hence the\ninitial `n` in its name.  The nesting of parentheses is not just an artifact of\nour notation; it indicates a real problem.  The function gathers up a\ncollection of numbers, but none of the additions can be performed until `nsum\n0` is reached.  Meanwhile, the computer must store the numbers in an internal\ndata structure, typically the _stack_.  For large `n`, say `nsum 10000`, the\ncomputation might fail due to stack overflow.\n\nWe all know that the additions can be performed as we go along.  How do we\nmake the computer do that?\n"},{"cell_type":"code","metadata":{},"source":"let rec summing n total =\n  if n = 0 then total\n           else summing (n-1) (n + total)","outputs":[],"execution_count":14},{"cell_type":"markdown","metadata":{},"source":"\nFunction `summing` takes an additional argument: a running total.  If\n`n` is zero then it returns the running total; otherwise, `summing`\nadds to it and continues.  The recursive calls do not nest; the additions are\ndone immediately.\n\nA recursive function whose computation does not nest is called\n_iterative_ or _tail-recursive_. Many functions can be made iterative by\nintroducing an argument analogous to _total_, which is often called an\n_accumulator_.\n\nThe gain in efficiency is sometimes worthwhile and sometimes not.  The function\n`power` is not iterative because nesting occurs whenever the exponent is odd.\nAdding a third argument makes it iterative, but the change complicates the\nfunction and the gain in efficiency is minute; for 32-bit integers, the maximum\npossible nesting is 30 for the exponent $2^{31}-1$.\n\n\nTODO slide\n\nA [classic\nbook](https://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs)\nby Abelson and Sussman used _iterative_ to mean _tail-recursive_. It describes\nthe Lisp dialect known as Scheme. Iterative functions produce computations\nresembling those that can be done using while-loops in conventional languages.\n\nMany algorithms can be expressed naturally using recursion, but only awkwardly\nusing iteration. There is a story that Dijkstra sneaked recursion into Algol-60\nby inserting the words \"any other occurrence of the procedure name denotes\nexecution of the procedure\". By not using the word \"recursion\", he managed to\nslip this amendment past sceptical colleagues.\n\nObsession with tail recursion leads to a coding style in which functions\nhave many more arguments than necessary.  Write straightforward code first,\navoiding only gross inefficiency.  If the program turns out to be too slow,\ntools are available for pinpointing the cause.  Always remember KISS (Keep\nIt Simple, Stupid).\n\nI hope you have all noticed by now that the summation can be done even more\nefficiently using the arithmetic progression formula:\n\n$$ 1+\\cdots+n = n(n+1)/2 $$\n"},{"cell_type":"code","metadata":{},"source":"let rec stupidSum n =\n  if n = 0 then 0\n           else n + (stupidSum (n-1) + stupidSum (n-1)) / 2","outputs":[],"execution_count":15},{"cell_type":"markdown","metadata":{},"source":"\nThe function calls itself $2^n$ times!  Bigger inputs mean higher costs---but\nwhat's the _growth rate_?\n\nNow let us consider how to estimate various costs associated with a program.\n_Asymptotic complexity_ refers to how costs---usually time or space---grow with\nincreasing inputs. Space complexity can never exceed time complexity, for it\ntakes time to do anything with the space.  Time complexity often greatly\nexceeds space complexity.\n\nThe function `stupidSum` calls itself twice in each recursive step.  This\nfunction is contrived, but many mathematical formulas refer to a particular\nquantity more than once.  In OCaml, we can create a local binding to a computed\nvalue using the _local declaration_ syntax:\n\nTODO make power a real function\n"},{"cell_type":"markdown","metadata":{},"source":"\nFast hardware does not make good algorithms unnecessary.  On the contrary,\nfaster hardware magnifies the superiority of better algorithms.  Typically, we\nwant to handle the largest inputs possible.  If we double our processing power,\nwhat do we gain?  How much can we increase $n$, the input to our function?\nWith `stupidSum`, we can only go from $n$ to $n+1$.  We are limited to this\nmodest increase because the function's running time is proportional to $2^n$.\nWith the function `npower` defined in the previous lecture, we can go from $n$\nto $2n$: we can handle problems twice as big.  With `power` we can do much\nbetter still, going from $n$ to $n^2$.\n\nTODO table\n\nThis table (excerpted from a 40-year-old book! TODO cite aho74) illustrates the\neffect of various time complexities.  The left-hand column indicates how many\nmilliseconds are required to process an input of size $n$.  The other entries\nshow the maximum size of $n$ that can be processed in the given time (one\nsecond, minute or hour).\n\nThe table illustrates how large an input can be processed as a function\nof time.  As we increase the computer time per input from one second to one\nminute and then to one hour, the size of the input increases accordingly.\n\nThe top two rows (complexities $n$ and $n\\lg n$) increase rapidly: for $n$, by\na factor of 60.  The bottom two start out close together, but $n^3$ (which\ngrows by a factor of 3.9) pulls well away from~$2^n$ (whose growth is only\nadditive).  If an algorithm's complexity is exponential then it can never\nhandle large inputs, even if it is given huge resources.  On the other hand,\nsuppose the complexity has the form~$n^c$, where $c$ is a constant.  (We say\nthe complexity is _polynomial_.)  Doubling the argument then increases the\ncost by a constant factor.  That is much better, though if $c>3$ the algorithm\nmay not be considered practical.\n\n\n"}]}