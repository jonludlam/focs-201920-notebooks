{"metadata":{"kernelspec":{"display_name":"OCaml 4.07.1","language":"OCaml","name":"ocaml-jupyter"},"language_info":{"name":"OCaml","version":"4.07.1","codemirror_mode":"text/x-ocaml","file_extension":".ml","mimetype":"text/x-ocaml","nbconverter_exporter":null,"pygments_lexer":"OCaml"}},"nbformat":4,"nbformat_minor":2,"cells":[{"cell_type":"markdown","metadata":{},"source":"# Foundations of Computer Science (OCaml version)\n\n\n\nThis course has two aims. The first is to teach programming. The second is to\npresent some fundamental principles of computer science, especially algorithm\ndesign. Most students will have some programming experience already, but there\nare few people whose programming cannot be improved through greater knowledge\nof basic principles. Please bear this point in mind if you have extensive\nexperience and find parts of the course rather slow.\n\nThe programming in this course is based on the language [OCaml](https://ocaml.org)\nand mostly concerns the functional programming style. Functional programs tend\nto be shorter and easier to understand than their counterparts in conventional\nlanguages such as C. In the space of a few weeks, we shall cover many\nfundamental data structures and learn basic methods for estimating efficiency.\n\n**this is a work-in-progress port of Lawrence C. Paulson's 1819 Cambridge\ncourse notes**\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 1: Introduction to Programming\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"### Basic Concepts in Computer Science\n\n\n\n- Computers: a child can use them; **nobody** can fully understand them!\n- We can master complexity through levels of abstraction.\n- Focus on 2 or 3 levels at most!\n\n**Recurring issues:**\n- what services to provide at each level\n- how to implement them using lower-level services\n- the interface: how the two levels should communicate\n\nA basic concept in computer science is that large systems can only be\nunderstood in levels, with each level further subdivided into functions or\nservices of some sort. The interface to the higher level should supply the\nadvertised services. Just as important, it should block access to the means by\nwhich those services are implemented. This _abstraction barrier_ allows one\nlevel to be changed without affecting levels above. For example, when a\nmanufacturer designs a faster version of a processor, it is essential that\nexisting programs continue to run on it. Any differences between the old and\nnew processors should be invisible to the program.\n\nModern processors have elaborate specifications, which still sometimes leave\nout important details. In the old days, you then had to consult the circuit\ndiagrams.\n"},{"cell_type":"markdown","metadata":{},"source":"### Example 1: Dates\n\n\n\n- Abstract level: dates over a certain interval\n- Concrete level: typically 6 characters: `YYMMDD` (where each character is represented by 8 bits)\n- Date crises caused by __inadequate__ internal formats:\n  * Digital’s PDP-10: using 12-bit dates (good for at most 11 years)\n  * 2000 crisis: 48 bits could be good for lifetime of universe!\n\nDigital Equipment Corporation’s date crisis occurred in 1975.  The\nPDP-10 was a 36-bit mainframe computer. It represented dates using a 12-bit\nformat designed for the tiny PDP-8. With 12 bits, one can distinguish\n$2^{12} = 4096$ days or 11 years.\n\nThe most common industry format for dates uses six characters: two for the\nyear, two for the month and two for the day. The most common \"solution\" to the\nyear 2000 crisis is to add two further characters, thereby altering file sizes.\nOthers have noticed that the existing six characters consist of 48 bits,\nalready sufficient to represent all dates over the projected lifetime of the\nuniverse: $2^{48}$ = $2.8 * 1014$ days = $7.7 * 1011$ years!\n\nMathematicians think in terms of unbounded ranges, but the representation we\nchoose for the computer usually imposes hard limits. A good programming\nlanguage like OCaml lets one easily change the representation used in the\nprogram.  But if files in the old representation exist all over the place,\nthere will still be conversion problems. The need for compatibility with older\nsystems causes problems across the computer industry.\n"},{"cell_type":"markdown","metadata":{},"source":"### Example II: Floating Point Numbers\n\n\n\nComputers have integers like `1066` and floats like $1.066 x 10^3$.\nA floating-point number is represented by two integers.\nThe concept of _data type_ involves:\n* how a value is represented inside the computer\n* the suite of operations given to programmers\n* valid and invalid (or exceptional) results, such as “infinity”\nComputer arithmetic can yield _incorrect answers_!\n\nIn science, numbers written with finite precision and a decimal exponent are\nsaid to be in _standard form_. The computational equivalent is the _floating\npoint number_. These are familiar to anybody who has used a scientific\ncalculator.  Internally, a float consists of two integers.\n\nBecause of its finite precision, floating-point computations are potentially\ninaccurate. To see an example, use your nearest electronic calculator to\ncompute $(2^{1/10000})10000$. I get $1.99999959$! With certain computations,\nthe errors spiral out of control. Many programming languages fail to check\nwhether even integer computations fall within the allowed range: you can add\ntwo positive integers and get a negative one!\n\nMost computers give us a choice of precisions. In 32-bit precision, integers\ntypically range from $2^{31} − 1$ (namely $2,147,483,647$) to $−2^{31}$; reals\nare accurate to about six decimal places and can get as large as 1035 or so.\nFor reals, 64-bit precision is often preferred. Early languages like Fortran\nrequired variables to be declared as `INTEGER`, `REAL` or `COMPLEX` and barred\nprogrammers from mixing numbers in a computation. Nowadays, programs handle\nmany different kinds of data, including text and symbols. The concept of a\n_data type_ can ensure that different types of data are not combined in a\nsenseless way.\n\nInside the computer, all data are stored as bits. In most programming\nlanguages, the compiler uses types to generate correct machine code, and types\nare not stored during program execution. In this course, we focus almost\nentirely on programming in a high-level language: OCaml.\n"},{"cell_type":"markdown","metadata":{},"source":"### Goals of Programming\n\n\n\n- to describe a computation so that it can be done _mechanically_:\n  * Expressions compute values.\n  * Commands cause effects.\n- to do so efficiently and **correctly**, giving the right answers quickly\n- to allow easy modification as needs change\n  * Through an orderly _structure_ based on abstraction principles\n  * Such as modules or (Java) classes\n\nProgramming _in-the-small_ concerns the writing of code to do simple, clearly\ndefined tasks. Programs provide expressions for describing mathematical\nformulae and so forth. (This was the original contribution of FORTRAN, the\nFORmula TRANslator.) Commands describe how control should flow from one part of\nthe program to the next.\n\nAs we code layer upon layer, we eventually find ourselves programming\n_in the large_ : joining large modules to solve some messy task. Programming\nlanguages have used various mechanisms to allow one part of the program to\nprovide interfaces to other parts. Modules encapsulate a body of code, allowing\noutside access only through a programmer-defined interface. _Abstract Data\nTypes_ are a simpler version of this concept, which implement a single concept\nsuch as dates or floating-point numbers.\n\n_Object-oriented programming_ is the most complicated approach to modularity.\n_Classes_ define concepts, and they can be built upon other classes. Operations\ncan be defined that work in appropriately specialized ways on a family of\nrelated classes. _Objects_ are instances of classes and hold the data that is\nbeing manipulated.\n\nThis course does not cover OCaml's sophisticated module system, which can do\nmany of the same things as classes. You will learn all about objects when you\nstudy Java.\n"},{"cell_type":"markdown","metadata":{},"source":"## Why Program in ML?\n\n\n\n* Why Program in ML?\n* It is interactive.\n* It has a flexible notion of _data type_.\n* It hides the underlying hardware: _no crashes_.\n* Programs can easily be understood mathematically.\n* It distinguishes naming something from _updating memory_.\n* It manages storage for us.\n\nStandard ML is the outcome of years of research into\nprogramming languages. It is unique, defined using a mathematical formalism (an\noperational semantics) that is both precise and comprehensible. Several\nsupported compilers are available, and thanks to the formal definition, there\nare remarkably few incompatibilities among them. _(TODO edit)_\n\nBecause of its connection to mathematics, ML programs can be designed and\nunderstood without thinking in detail about how the computer will run them.\nAlthough a program can abort, it cannot crash: it remains under the control of\nthe OCaml system. It still achieves respectable efficiency and provides\nlower-level primitives for those who need them. Most other languages allow\ndirect access to the underlying machine and even try to execute illegal\noperations, causing crashes.\n\nThe only way to learn programming is by writing and running programs. This web\nnotebook provides an interactive environment where you can modify the example\nfragments and see the results for yourself.  You should also consider\ninstalling OCaml on your own computer so that you try more advanced programs\nlocally.\n"},{"cell_type":"markdown","metadata":{},"source":"### A first session with OCaml\n\n\n"},{"cell_type":"code","metadata":{},"source":"let pi = 3.14159265358979","outputs":[],"execution_count":1},{"cell_type":"markdown","metadata":{},"source":"\nThe first line of this simple session is a _value declaration_. It makes the\nname `pi` stand for the floating point number `3.14159`. (Such names are called\n_identifiers_.)  OCaml echoes the name (`pi`) and type (`float`) of the\ndeclared identifier.\n"},{"cell_type":"code","metadata":{},"source":"pi *. 1.5 *. 1.5","outputs":[],"execution_count":2},{"cell_type":"markdown","metadata":{},"source":"\nThe second line computes the area of the circle with radius `1.5` using the\nformula $A = \\pi r^2$. We use `pi` as an abbreviation for `3.14159`.\nMultiplication is expressed using `*.`, which is called an _infix operator_\nbecause it is written between its two operands.\n\nOCaml replies with the computed value (about `7.07`) and its type (again `float`).\n"},{"cell_type":"code","metadata":{},"source":"let area r = pi *. r *. r","outputs":[],"execution_count":3},{"cell_type":"markdown","metadata":{},"source":"\nTo work abstractly, we should provide the service \"compute the area of a\ncircle,\" so that we no longer need to remember the formula. This sort of\nencapsulated computation is called a _function_. The third line declares the\nfunction `area`. Given any floating point number `r`, it returns another\nfloating point number computed using the `area` formula; note that the function\nhas type `float -> float`.\n"},{"cell_type":"code","metadata":{},"source":"area 2.0","outputs":[],"execution_count":4},{"cell_type":"markdown","metadata":{},"source":"\nThe fourth line calls the function `area` supplying `2.0` as the argument. A\ncircle of radius `2` has an area of about `12.6`. Note that brackets around a\nfunction argument are not necessary.\n\nThe function uses `pi` to stand for `3.14159`. Unlike what you may have seen in\nother programming languages, `pi` cannot be \"assigned to\" or otherwise updated.\nIts meaning within `area` will persist even if we issue a new `let` declaration\nfor `pi` afterwards.\n"},{"cell_type":"code","metadata":{},"source":"let rec npower x n =\n  if n = 0 then 1.0\n  else x *. npower x (n-1)","outputs":[],"execution_count":5},{"cell_type":"markdown","metadata":{},"source":"\nThe function `npower` raises its real argument `x` to the power `n`, a\nnon-negative integer. The function is _recursive_: it calls itself. This concept\nshould be familiar from mathematics, since exponentiation is defined by the\nrules shown above. You may also have seen recursion in the product rule for\ndifferentiation: $(u · v)′ = u · v′ + u′ · v.$.\n\nIn finding the derivative of $u.v$, we recursively find the derivatives of $u$\nand $v$, combining them to obtain the desired result. The recursion is\nmeaningful because it terminates: we reduce the problem to two smaller\nproblems, and this cannot go on forever. The ML programmer uses recursion\nheavily.  For $n>=0$, the equation $x^{n+1} = x * x^n$ yields an obvious\ncomputation:\n\n$$ x^3 = x\\times x^2 = x\\times x\\times x^1 = x\\times x\\times x\\times x^0 = x\\times x\\times x $$\n\nThe equation clearly holds even for negative $n$. However, the corresponding\ncomputation runs forever:\n\n$$ x^{-1} = x\\times x^{-2} = x\\times x\\times x^{-3}=\\cdots $$\n\nNote that the function `npower` contains both an integer constant (0) and a\nfloating point constant (1.0). The decimal point makes all the difference. The\nML system will notice and ascribe different meaning to each type of constant.\n"},{"cell_type":"code","metadata":{},"source":"let square x = x *. x;","outputs":[],"execution_count":6},{"cell_type":"markdown","metadata":{},"source":"\nNow for a tiresome but necessary aside. In most languages, the types of\narguments and results must always be specified. ML is unusual that it normally\ninfers the types itself. However, sometimes ML could use a hint; function\n`square` above has a type constraint to say its result is a float.\n\nML can still infer the type even if you don't specify them, but in some cases\nit will use a more inefficient function than a specialised one.  Some languages\nhave just one type of number, converting automatically between different\nformats; this is slow and could lead to unexpected rounding errors.  Type\nconstraints are allowed almost anywhere. We can put one on any occurrence of x\nin the function. We can constrain the function’s result:\n"},{"cell_type":"code","metadata":{},"source":"let square (x:float) = x *. x","outputs":[],"execution_count":7},{"cell_type":"code","metadata":{},"source":"let square x : float = x *. x","outputs":[],"execution_count":8},{"cell_type":"markdown","metadata":{},"source":"\nML treats the equality and comparison test specially. Expressions like `if x = y then ...`\nare fine provided `x` and `y` have the same type and equality testing is\npossible for that type. (We discuss equality further in a later lecture.)\nNote that `x <> y` is ML for `x  ̸= y`.\n\nA characteristic feature of the computer is its ability to test for conditions\nand act accordingly.  In the early days, a program might jump to a given\naddress depending on the sign of some number.  Later, John McCarthy defined\nthe _conditional expression_ to satisfy:\n\n$$if true then x else y = x$$\n$$if false then x else y = y$$\n\nML evaluates the expression $if B then E_1 else E_2$ by first evaluating $B$.\nIf the result is `true` then ML evaluates $E_1$ and otherwise $E_2$.  Only one\nof the two expressions $E_1$ and $E_2$ is evaluated!  If both were evaluated,\nthen recursive functions like `npower` above would run forever.\n\nThe _if-expression_ is governed by an expression of type `bool`, whose two\nvalues are `true` and `false`.  In modern programming languages, tests are not\nbuilt into \"conditional branch\" constructs but have an independent status.\nTests, or _Boolean expressions_, can be expressed using relational operators\nsuch as `<` and `=`. They can be combined using the Boolean operators for\nnegation (`not`), `and` (written as `&&`) and `or` (written as `||`).  New\nproperties can be declared as functions: here, to test whether an integer is\neven.\n\nFor large `n`, computing powers using $x^{n+1} = x\\times x^n$ is too slow to\nbe practical.  The equations above are much faster. Example:\n\n$$ 2^{12} = 4^6 = 16^3 = 16\\times 256^1 = 16\\times 256 = 4096. $$\n\nInstead of `n` multiplications, we need at most $2 lg n$ multiplications,\nwhere $lg n$ is the logarithm of $n$ to the base $2$.\n\nWe use the function `even`, declared previously, to test whether the\nexponent is even.  Integer division (`div`) truncates its result to an\ninteger: dividing $2n+1$ by 2 yields $n$.\n\nA recurrence is a useful computation rule only if it is bound to terminate.\nIf $n>0$ then $n$ is smaller than both $2n$ and $2n+1$.  After enough\nrecursive calls, the exponent will be reduced to $1$.  The equations also hold\nif $n\\leq0$, but the corresponding computation runs forever.\n\nOur reasoning assumes arithmetic to be _exact_. Fortunately, the calculation is\nwell-behaved using floating-point.\n\nTODO edit for OCaml. The negation of `x` is written `~x` rather than `-x`\nplease note.  Most languages use the same symbol for minus and subtraction,\nbut ML regards all operators, whether infix or not, as functions.  Subtraction\ntakes a pair of numbers, but minus takes a single number; they are distinct\nfunctions and must have distinct names.\n\nTODO edit for OCaml. Computer numbers have a finite range, which if exceeded gives rise to an\nOverflow error.  Some ML systems can represent integers of arbitrary size.\n\nIf integers and reals must be combined in a calculation, ML provides functions\nto convert between them:\n"},{"cell_type":"code","metadata":{},"source":"int_of_float ;;","outputs":[],"execution_count":9},{"cell_type":"code","metadata":{},"source":"int_of_float 3.14159 ;;","outputs":[],"execution_count":10},{"cell_type":"code","metadata":{},"source":"float_of_int ;;","outputs":[],"execution_count":11},{"cell_type":"code","metadata":{},"source":"float_of_int 3 ;;","outputs":[],"execution_count":12},{"cell_type":"markdown","metadata":{},"source":"\nML's libraries are organized using _modules_, so we many use compound\nidentifiers such as `Float.of_int` to refer to library functions.  In OCaml,\nlibrary units can also be loaded by commands such as `#require \"num\"`.  There\nare many thousands of library functions available in the OCaml ecosystem,\nincluding text-processing and operating systems functions in addition to the\nusual numerical ones.\n\nTODO summarise OCaml syntax.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 2: Recursion and Efficiency\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"### Expression Evaluation\n\n\n\nExpression evaluation concerns expressions and the values they return. This\nview of computation may seem to be too narrow. It is certainly far removed from\ncomputer hardware, but that can be seen as an advantage. For the traditional\nconcept of computing solutions to problems, expression evaluation is entirely\nadequate.\n\nStarting with $E_0$, the expression $E_i$ is reduced to $E_{i+1}$ until this\nprocess concludes with a value~$v$.  A _value_ is something like a number\nthat cannot be further reduced.\n\nWe write $E E'$ to say that $E$ is _reduced_ to $E'$.\nMathematically, they are equal: $E=E'$, but the computation goes from $E$ to\n$E'$ and never the other way around.\n\nComputers also interact with the outside world.  For a start, they need some\nmeans of accepting problems and delivering solutions.  Many computer systems\nmonitor and control industrial processes.  This role of computers is familiar\nnow, but was never envisaged in the early days. Computer pioneers focused on\nmathematical calculations.  Modelling interaction and control requires a notion\nof _states_ that can be observed and changed.  Then we can consider\nupdating the state by assigning to variables or performing input/output,\nfinally arriving at conventional programs as coded in C, for instance.\n\nFor now, we remain at the level of expressions, which is usually termed\n_functional programming_.\n\n"},{"cell_type":"markdown","metadata":{},"source":"### Summing the first n integers\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec nsum n =\n  if n = 0 then 0\n           else n + nsum (n-1)","outputs":[],"execution_count":13},{"cell_type":"markdown","metadata":{},"source":"\nThe function call `nsum n` computes the sum `1+...+nz` rather naively, hence the\ninitial `n` in its name.  The nesting of parentheses is not just an artifact of\nour notation; it indicates a real problem.  The function gathers up a\ncollection of numbers, but none of the additions can be performed until `nsum\n0` is reached.  Meanwhile, the computer must store the numbers in an internal\ndata structure, typically the _stack_.  For large `n`, say `nsum 10000`, the\ncomputation might fail due to stack overflow.\n\nWe all know that the additions can be performed as we go along.  How do we\nmake the computer do that?\n"},{"cell_type":"markdown","metadata":{},"source":"### Iteratively summing the first `n` integers\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec summing n total =\n  if n = 0 then total\n           else summing (n-1) (n + total)","outputs":[],"execution_count":14},{"cell_type":"markdown","metadata":{},"source":"\nFunction `summing` takes an additional argument: a running total.  If\n`n` is zero then it returns the running total; otherwise, `summing`\nadds to it and continues.  The recursive calls do not nest; the additions are\ndone immediately.\n\nA recursive function whose computation does not nest is called\n_iterative_ or _tail-recursive_. Many functions can be made iterative by\nintroducing an argument analogous to _total_, which is often called an\n_accumulator_.\n\nThe gain in efficiency is sometimes worthwhile and sometimes not.  The function\n`power` is not iterative because nesting occurs whenever the exponent is odd.\nAdding a third argument makes it iterative, but the change complicates the\nfunction and the gain in efficiency is minute; for 32-bit integers, the maximum\npossible nesting is 30 for the exponent $2^{31}-1$.\n\n\nTODO slide\n\nA [classic\nbook](https://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs)\nby Abelson and Sussman used _iterative_ to mean _tail-recursive_. It describes\nthe Lisp dialect known as Scheme. Iterative functions produce computations\nresembling those that can be done using while-loops in conventional languages.\n\nMany algorithms can be expressed naturally using recursion, but only awkwardly\nusing iteration. There is a story that Dijkstra sneaked recursion into Algol-60\nby inserting the words \"any other occurrence of the procedure name denotes\nexecution of the procedure\". By not using the word \"recursion\", he managed to\nslip this amendment past sceptical colleagues.\n\nObsession with tail recursion leads to a coding style in which functions\nhave many more arguments than necessary.  Write straightforward code first,\navoiding only gross inefficiency.  If the program turns out to be too slow,\ntools are available for pinpointing the cause.  Always remember KISS (Keep\nIt Simple, Stupid).\n\nI hope you have all noticed by now that the summation can be done even more\nefficiently using the arithmetic progression formula:\n\n$$ 1+\\cdots+n = n(n+1)/2 $$\n"},{"cell_type":"markdown","metadata":{},"source":"### Stupidly Summing the First `n` Integers\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec stupidSum n =\n  if n = 0 then 0\n           else n + (stupidSum (n-1) + stupidSum (n-1)) / 2","outputs":[],"execution_count":15},{"cell_type":"markdown","metadata":{},"source":"\nThe function calls itself $2^n$ times!  Bigger inputs mean higher costs---but\nwhat's the _growth rate_?\n\nNow let us consider how to estimate various costs associated with a program.\n_Asymptotic complexity_ refers to how costs---usually time or space---grow with\nincreasing inputs. Space complexity can never exceed time complexity, for it\ntakes time to do anything with the space.  Time complexity often greatly\nexceeds space complexity.\n\nThe function `stupidSum` calls itself twice in each recursive step.  This\nfunction is contrived, but many mathematical formulas refer to a particular\nquantity more than once.  In OCaml, we can create a local binding to a computed\nvalue using the _local declaration_ syntax:\n\nTODO make power a real function\n"},{"cell_type":"raw","metadata":{},"source":"# let y = power x 20 in\n  (f y) + (g x y)"},{"cell_type":"markdown","metadata":{},"source":"\nFast hardware does not make good algorithms unnecessary.  On the contrary,\nfaster hardware magnifies the superiority of better algorithms.  Typically, we\nwant to handle the largest inputs possible.  If we double our processing power,\nwhat do we gain?  How much can we increase $n$, the input to our function?\nWith `stupidSum`, we can only go from $n$ to $n+1$.  We are limited to this\nmodest increase because the function's running time is proportional to $2^n$.\nWith the function `npower` defined in the previous lecture, we can go from $n$\nto $2n$: we can handle problems twice as big.  With `power` we can do much\nbetter still, going from $n$ to $n^2$.\n\nTODO table\n\nThis table (excerpted from a 40-year-old book! TODO cite aho74) illustrates the\neffect of various time complexities.  The left-hand column indicates how many\nmilliseconds are required to process an input of size $n$.  The other entries\nshow the maximum size of $n$ that can be processed in the given time (one\nsecond, minute or hour).\n\nThe table illustrates how large an input can be processed as a function\nof time.  As we increase the computer time per input from one second to one\nminute and then to one hour, the size of the input increases accordingly.\n\nThe top two rows (complexities $n$ and $n \\lg n$) increase rapidly: for $n$, by\na factor of 60.  The bottom two start out close together, but $n^3$ (which\ngrows by a factor of 3.9) pulls well away from $2^n$ (whose growth is only\nadditive).  If an algorithm's complexity is exponential then it can never\nhandle large inputs, even if it is given huge resources.  On the other hand,\nsuppose the complexity has the form $n^c$, where $c$ is a constant.  (We say\nthe complexity is _polynomial_.)  Doubling the argument then increases the\ncost by a constant factor.  That is much better, though if $c>3$ the algorithm\nmay not be considered practical.\n\nThe cost of a program is usually a complicated formula.  Often we should\nconsider only the most significant term.  If the cost is $n^2 + 99n + 900$\nfor an input of size $n$, then the $n^2$ term will eventually dominate,\neven though $99n$ is bigger for $n<99$.\nThe constant term $900$ may look big, but it is soon dominated by $n^2$.\n\nConstant factors in costs can be ignored unless they are large.  For one thing,\nthey seldom make a difference: $100n^2$ will be better than $n^3$ in the long\nrun: or _asymptotically_ to use the jargon.  Moreover, constant factors are\nseldom stable.  They depend upon details such as which hardware, operating\nsystem or programming language is being used.  By ignoring constant factors, we\ncan make comparisons between algorithms that remain valid in a broad range of\ncircumstances.\n\nThe \"Big O\" notation is commonly used to describe efficiency---to be precise,\n_asymptotic complexity_.  It concerns the limit of a function as its\nargument tends to infinity.  It is an abstraction that meets the informal\ncriteria that we have just discussed.\nIn the definition, _sufficiently large_ means there is some constant $n_0$\nsuch that $|f(n)|\\leq c|g(n)|$ for all $n$ greater than $n_0$.  The\nrole of $n_0$ is to ignore finitely many exceptions to the bound, such as the\ncases when $99n$ exceeds~$n^2$.\n\nTODO onotation slide\n\n$O$ notation lets us reason about the costs of algorithms easily.\n- Constant factors such as the $2$ in $O(2g(n))$ drop out: we can use $O(g(n))$ with twice the value of~$c$ in the definition.\n- Because constant factors drop out, the base of logarithms is irrelevant.\n- Insignificant terms drop out.  To see that $O(n^2+50n+36)$ is the same as $O(n^2)$, consider that $n^2+50n+36/n^2$ converges to 1 for increasing $n$.  % In fact, $n^2+50n+36 \\le 2n^2$ for $n\\ge 51$, so can double the constant factor\n\nIf $c$ and $d$ are constants (that is, they are independent of $n$) with $0 < c < d$ then\n- $O(n^c)$ is contained in $O(n^d)$\n- $O(c^n)$ is contained in $O(d^n)$\n- $O(\\log n)$ is contained $in O(n^c)$\n\nTo say that $O(c^n)$ _is contained in_ $O(d^n)$ means that the former gives\na tighter bound than the latter.  For example, if $f(n)=O(2^n)$ then\n$f(n)=O(3^n)$ trivially, but the converse does not hold.\n"},{"cell_type":"markdown","metadata":{},"source":"### Common Complexity Classes\n\n\n\n- $O(1)$ is _constant_\n- $O(\\log n)$ is _logarithmic_\n- $O(n)$ is _linear_\n- $O(n\\log n)$ is _quasi-linear_\n- $O(n^2)$ is _quadratic_\n- $O(n^3)$ is _cubic_\n- $O(a^n)$ is _exponential_ (for fixed $a$)\n\nLogarithms grow very slowly, so $O(\\log n)$ complexity is excellent.  Because\n$O$ notation ignores constant factors, the base of the logarithm is\nirrelevant!\n\nUnder linear we might mention $O(n\\log n)$, which occasionally is called\n_quasilinear_ and which scales up well for large $n$.\n\nAn example of quadratic complexity is matrix addition: forming the sum of two\n$n\\times n$ matrices obviously takes $n^2$ additions.  Matrix\nmultiplication is of cubic complexity, which limits the size of matrices that\nwe can multiply in reasonable time.  An $O(n^{2.81})$ algorithm exists, but it\nis too complicated to be of much use, even though it is theoretically better.\n\nAn exponential growth rate such as $2^n$ restricts us to small values of~$n$.\nAlready with $n=20$ the cost exceeds one million.  However, the worst case\nmight not arise in normal practice.  OCaml type-checking is exponential in the\nworst case, but not for ordinary programs.\n\n### Sample costs in O notation\n\n| Function     | Time | Space |\n| ------------ | ---- | ----- |\n|  npower, nsum  | $O(n)$ | $O(n)$ |\n|  summing       | $O(n)$ | $O(1)$ |\n| $n(n+1)/2$ | $O(1)$ | $O(1)$ |\n|  power  | $O(\\log n)$ | $O(\\log n)$ |\n|  stupidSum  | $O(2^n)$ | $O(n)$ |\n\nRecall that `npower` computes $x^n$\nby repeated multiplication while `nsum` naively computes the sum\n$1+\\cdots+n$.  Each obviously performs $O(n)$ arithmetic operations.  Because\nthey are not tail recursive, their use of space is also $O(n)$.  The function\n`summing` is a version of `nsum` with an accumulating argument;\nits iterative behaviour lets it work in constant space.  $O$ notation spares\nus from having to specify the units used to measure space.\n\nEven ignoring constant factors, the units chosen can influence the result.\nMultiplication may be regarded as a single unit of cost.  However, the cost of\nmultiplying two $n$-digit numbers for large $n$ is itself an important\nquestion, especially now that public-key cryptography uses numbers hundreds of\ndigits long.\n\nFew things can _really_ be done in constant time or stored in constant\nspace.  Merely to store the number $n$ requires $O(\\log n)$ bits.  If a\nprogram cost is $O(1)$, then we have probably assumed that certain operations\nit performs are also $O(1)$---typically because we expect never to exceed the\ncapacity of the standard hardware arithmetic.\n\nWith `power`, the precise number of operations depends upon $n$ in a\ncomplicated way, depending on how many odd numbers arise, so it is convenient\nthat we can just write $O(\\log n)$.  An accumulating argument could reduce its\nspace cost to $O(1)$.\n\n### Some Simple Recurrence Relations\n\n\nConsider $T(n)$ has a cost we want to bound using $O$ notation.\nA typical _base case_ is $T(1)=1$.  Some _recurrences_ are:\n\n| Equation            | Complexity   |\n| ------------------- | ------------ |\n| $T(n+1) = T(n)+1$  | $O(n)$       |\n| $T(n+1) = T(n)+n$  | $O(n^2)$     |\n| $T(n) = T(n/2)+1$  | $O(\\log n)$  |\n| $T(n) = 2T(n/2)+n$ | $O(n\\log n)$ |\n\nTo analyse a function, inspect its OCaml declaration.  Recurrence equations for\nthe cost function $T(n)$ can usually be read off.  Since we ignore constant\nfactors, we can give the base case a cost of one unit.  Constant work done in\nthe recursive step can also be given unit cost; since we only need an upper\nbound, this unit represents the larger of the two actual costs.  We could use\nother constants if it simplifies the algebra.\n\nFor example, recall our function `nsum`:\n"},{"cell_type":"code","metadata":{},"source":"let rec nsum n =\n  if n = 0 then \n    0\n  else\n    n + nsum (n-1)","outputs":[],"execution_count":16},{"cell_type":"markdown","metadata":{},"source":"\nGiven $n+1$, it performs a constant amount of work (an addition and\nsubtraction) and calls itself recursively with argument $n$.  We get the\nrecurrence equations $T(0)=1$ and $T(n+1) = T(n)+1$.  The closed form is\nclearly $T(n)=n+1$, as we can easily verify by substitution.  The cost is\n_linear_.\n\nThis function, given $n+1$, calls `nsum`, performing $O(n)$ work.\nAgain ignoring constant factors, we can say that this call takes exactly $n$\nunits.\n"},{"cell_type":"code","metadata":{},"source":"let rec nsumsum n =\n  if n = 0 then\n    0\n  else\n    nsum n + nsumsum (n-1)","outputs":[],"execution_count":17},{"cell_type":"markdown","metadata":{},"source":"\nWe get the recurrence equations $T(0)=1$ and $T(n+1) = T(n)+n$.  It is easy to\nsee that $T(n)=(n-1)+\\cdots+1=n(n-1)/2=O(n^2)$.  The cost is\n_quadratic_.\n\nThe function `power` divides its input $n$ into two, with\nthe recurrence equation $T(n) = T(n/2)+1$.  Clearly $T(2^n)=n+1$, so\n$T(n)=O(\\log n)$.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 3: Lists\n\n\n"},{"cell_type":"code","metadata":{},"source":"let x = [3; 5; 9] ;;","outputs":[],"execution_count":18},{"cell_type":"code","metadata":{},"source":"let y = [ (1,\"one\"); (2,\"two\") ] ;;","outputs":[],"execution_count":19},{"cell_type":"markdown","metadata":{},"source":"\nA _list_ is an ordered series of elements; repetitions are significant.\nSo `[3;5;9]` differs from `[5;3;9]` and from `[3;3;5;9]`.  Elements in the\nlist are separated with `;` when constructed, as opposed to the `,` syntax\nused for fixed-length tuples.\n\nAll elements of a list must have the same type.  Above we see a list of\nintegers and a list of `(integer, string)` pairs.  One can also have lists of\nlists, such as `[[3]; []; [5,6]]`, which has type `int list list`.\n\nIn the general case, if $x_1$, \\ldots, $x_n$ all have the same type (say\n$\\tau$) then the list $[x_1,\\ldots,x_n]$ has type $(\\tau)\\texttt{list}$.\n\nLists are the simplest data structure that can be used to process collections\nof items.  Conventional languages use _arrays_, whose elements are\naccessed using subscripting: for example, $A[i]$ yields the $i$th element of\nthe array~$A$.  Subscripting errors are a known cause of programmer grief,\nhowever, so arrays should be replaced by higher-level data structures whenever\npossible.\n"},{"cell_type":"code","metadata":{},"source":"x @ [2; 10] ;;","outputs":[],"execution_count":20},{"cell_type":"code","metadata":{},"source":"List.rev [ (1,\"one\"); (2,\"two\") ] ;;","outputs":[],"execution_count":21},{"cell_type":"markdown","metadata":{},"source":"\nThe infix operator `@` (also called `List.append`) concatenates two lists.\nAlso built-in is `List.rev`, which reverses a list.  These are demonstrated\nin the session above.\n"},{"cell_type":"markdown","metadata":{},"source":"### The List Primitives\n\n\n\nThere are two kinds of lists:\n- `[]` represents the empty list\n- `x :: l` is the list with head $x$ and tail $l$\n"},{"cell_type":"code","metadata":{},"source":"let nil = [] ;;","outputs":[],"execution_count":22},{"cell_type":"code","metadata":{},"source":"1 :: nil ;;","outputs":[],"execution_count":23},{"cell_type":"code","metadata":{},"source":"1 :: 2 :: nil ;;","outputs":[],"execution_count":24},{"cell_type":"markdown","metadata":{},"source":"\nThe operator `::` (also called `List.cons` for \"construct\"), puts a new element on\nto the head of an existing list.  While we should not be too preoccupied with\nimplementation details, it is essential to know that `::` is an $O(1)$\noperation.  It uses constant time and space, regardless of the length of the\nresulting list.  Lists are represented internally with a linked structure;\nadding a new element to a list merely hooks the new element to the front of\nthe existing structure.  Moreover, that structure continues to denote the same\nlist as it did before; to see the new list, one must look at the new `::` node\n(or _cons cell_) just created.\n\n\nHere we see the element~1 being consed to the front of the list `[3;5;9]`:\n\n$$\n\\let\\down=\\downarrow\n\\begin{array}{*{10}{c@{\\,}}c}\n:: & \\to & \\cdots & :: & \\to &  :: & \\to &  :: & \\to & nil \\\\\n\\down &  &        & \\down &  & \\down &  & \\down  \\\\\n1     &  &        & 3     &  & 5     &  & 9\n\\end{array}\n$$\n\nGiven a list, taking its first element (its _head_) or its list of\nremaining elements (its _tail_) also takes constant time.  Each\noperation just follows a link.  In the diagram above, the first down arrow\nleads to the head and the leftmost right arrow leads to the tail.  Once we\nhave the tail, its head is the second element of the original list, etc.\n\nThe tail is _not_ the last element; it is the _list_ of all elements\nother than the head!\n"},{"cell_type":"markdown","metadata":{},"source":"### Getting at the Head and Tail\n\n\n"},{"cell_type":"code","metadata":{},"source":"let null = function\n  | [] -> true\n  | x :: l -> false ;;","outputs":[],"execution_count":25},{"cell_type":"code","metadata":{},"source":"null [] ;;","outputs":[],"execution_count":26},{"cell_type":"code","metadata":{},"source":"null [1;2;3] ;;","outputs":[],"execution_count":27},{"cell_type":"code","metadata":{},"source":"let hd (x::l) = x ;;","outputs":[],"execution_count":28},{"cell_type":"code","metadata":{},"source":"hd [1;2;3] ;;","outputs":[],"execution_count":29},{"cell_type":"code","metadata":{},"source":"let tl (x::l) = l ;;","outputs":[],"execution_count":30},{"cell_type":"code","metadata":{},"source":"tl [7;6;5] ;;","outputs":[],"execution_count":31},{"cell_type":"markdown","metadata":{},"source":"\nThe empty list has neither head nor tail.  Applying `List.hd` `List.tl` to `[]`\nis an error---strictly speaking, an _exception_.  The function `null` can\nbe used to check for the empty list beforehand.  Taking a list apart using\ncombinations of `hd` and `tl` is hard to get right.  Fortunately, it is seldom\nnecessary because of _pattern-matching_.\n\nThe declaration of `null`} above has two clauses: one for the empty list (for\nwhich it returns `true`) and one for non-empty lists (for which it returns\n`false`).\n\nThe declaration of `null` above has two clauses: one for the empty list\n(for which it returns `true`) and one for non-empty lists (for which it\nreturns `false`).\n\nThe declaration of `hd` above has only one clause, for non-empty lists.  They\nhave the form `x::l` and the function returns `x`, which is the head.  OCaml\nprints a warning to tell us that calling the function could raise an exception\ndue to all possible inputs not being handles, including a counter-example (in\nthis case, the empty list `[]`). The declaration of `tl` is similar to `hd`.\n\nThese three primitive functions are _polymorphic_ and allow flexibility in the\ntypes of their arguments and results. Note their types!\n"},{"cell_type":"code","metadata":{},"source":"null ;;","outputs":[],"execution_count":32},{"cell_type":"code","metadata":{},"source":"hd ;;","outputs":[],"execution_count":33},{"cell_type":"code","metadata":{},"source":"tl ;;","outputs":[],"execution_count":34},{"cell_type":"markdown","metadata":{},"source":"\nSymbols `'a` and `'b` are called _type variables_ and stand for any types. Code\nwritten using these functions is checked for type correctness at compile time.\nAnd this guarantees strong properties at run time, for example that the\nelements of any list all have the same type.\n"},{"cell_type":"markdown","metadata":{},"source":"### Computing the Length of a List\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec nlength = function\n | [] -> 0\n | x :: xs -> 1 + nlength xs ;;","outputs":[],"execution_count":35},{"cell_type":"code","metadata":{},"source":"nlength [] ;;","outputs":[],"execution_count":36},{"cell_type":"code","metadata":{},"source":"nlength [5; 6; 7] ;;","outputs":[],"execution_count":37},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nnlength[a,b,c] \\Rightarrow & 1 + nlength[b,c] \\\\\n   \\Rightarrow & 1 + (1 + nlength[c]) \\\\\n   \\Rightarrow & 1 + (1 + (1 + nlength[])) \\\\\n   \\Rightarrow & 1 + (1 + (1 + 0)) \\\\\n   \\Rightarrow & \\ldots \\;\\; 3\n\\end{align*}\n$$\n\nMost list processing involves recursion.  This is a simple example; patterns\ncan be more complex.  Observe the use of a vertical bar `|` to separate the function's\nclauses.  We have _one_ function declaration that handles two cases.\nTo understand its role, consider the following faulty code:\n"},{"cell_type":"code","metadata":{},"source":"let rec nlength [] = 0 ;;","outputs":[],"execution_count":38},{"cell_type":"code","metadata":{},"source":"let rec nlength (x::xs) = 1 + nlength xs ;;","outputs":[],"execution_count":39},{"cell_type":"markdown","metadata":{},"source":"\nThese are two declarations, not one.  First we declare `nlength` to be a\nfunction that handles only empty lists.  Then we redeclare it to be a function\nthat handles only non-empty lists; it can never deliver a result.  We see that\na second `let` declaration replaces any previous one rather than extending it\nto cover new cases.\n\nNow, let us return to the declaration shown on the slide.  The length function\nis _polymorphic_ and applies to _all_ lists regardless of element\ntype!  Most programming languages lack such flexibility.\n\nUnfortunately, this length computation is naive and wasteful.  Like\n`nsum` earlier, it is not tail-recursive.  It\nuses $O(n)$ space, where $n$ is the length of its input.  As usual, the\nsolution is to add an accumulating argument.\n"},{"cell_type":"markdown","metadata":{},"source":"### Efficiently Computing the Length of a List\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec addlen = function\n  | (n, []) -> n\n  | (n, x::xs) -> addlen (n+1, xs) ;;","outputs":[],"execution_count":40},{"cell_type":"code","metadata":{},"source":"addlen (0, [5;6;7]) ;;","outputs":[],"execution_count":41},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\naddlen(0, [a,b,c]) \\Rightarrow &  addlen(1, [b,c]) \\\\\n  \\Rightarrow  & addlen(2, [c]) \\\\\n  \\Rightarrow  & addlen(3, []) \\\\\n  \\Rightarrow  & 3\n\\end{align*}\n$$\n\nPatterns can be as complicated as we like.  Here, the two patterns are\n`(n,[])` and `(n,x::xs)`.\n\nFunction `addlen` is again polymorphic.  Its type mentions the integer\naccumulator.\n\nNow we may declare an efficient length function.  It is simply a wrapper for\n`addlen`, supplying zero as the initial value of $n$.\n"},{"cell_type":"code","metadata":{},"source":"let length xs = addlen (0, xs) ;;","outputs":[],"execution_count":42},{"cell_type":"code","metadata":{},"source":"length [5;6;7;8] ;;","outputs":[],"execution_count":43},{"cell_type":"markdown","metadata":{},"source":"\nThe recursive calls do not nest: this version is iterative.  It takes $O(1)$\nspace.  Obviously its time requirement is $O(n)$ because it takes at least $n$\nsteps to find the length of an $n$-element list.\n"},{"cell_type":"markdown","metadata":{},"source":"### Append: List Concatenation\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec append = function\n  | ([], ys) -> ys\n  | (x::xs, ys) -> x :: append (xs,ys) ;;","outputs":[],"execution_count":44},{"cell_type":"code","metadata":{},"source":"append ([1;2;3], [4]) ;;","outputs":[],"execution_count":45},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nappend([1;2;3], [4]) \\Rightarrow & 1 :: append([2;3], [4]) \\\\\n  \\Rightarrow & 1 :: (2 :: append([3], [4])) \\\\\n  \\Rightarrow & 1 :: (2 :: (3 :: append([], [4]))) \\\\\n  \\Rightarrow & 1 :: (2 :: (3 :: [4])) \\;; [1;2;3;4]\n\\end{align*}\n$$\n\nHere is how append might be declared, ignoring the details of how `@` is made\nan infix operator.  This function is also not iterative.  It scans its first\nargument, sets up a string of `cons' operations (`::`) and finally does them.\n\nIt uses $O(n)$ space and time, where $n$ is the length of its first argument.\n_Its costs are independent of its second argument._\n\nAn accumulating argument could make it iterative, but with considerable\ncomplication.  The iterative version would still require $O(n)$ space and time\nbecause concatenation requires copying all the elements of the first list.\nTherefore, we cannot hope for asymptotic gains; at best we can decrease the\nconstant factor involved in $O(n)$, but complicating the code is likely to\nincrease that factor.  Never add an accumulator merely out of habit.\n\nNote append's polymorphic type. It tells us that two lists can be joined if\ntheir element types agree.\n"},{"cell_type":"markdown","metadata":{},"source":"### Reversing a List in O(n^2)\n\n\n\nnrev \n"},{"cell_type":"code","metadata":{},"source":"let rec nrev = function\n  | [] -> []\n  | x::xs -> (nrev xs) @ [x] ;;","outputs":[],"execution_count":46},{"cell_type":"code","metadata":{},"source":"nrev [1;2;3] ;;","outputs":[],"execution_count":47},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nnrev[a;b;c] \\Rightarrow & nrev[b,c] @ [a] \\\\\n  \\Rightarrow &  (nrev[c] @ [b]) @ [a] \\\\\n  \\Rightarrow &  ((nrev[] @ [c]) @ [b]) @ [a] \\\\\n  \\Rightarrow &  (([] @ [c]) @ [b]) @ [a] \\;; \\ldots \\;; [c,b,a]\n\\end{align*}\n$$\n\nThis reverse function is grossly inefficient due to poor usage of append,\nwhich copies its first argument.  If `nrev` is given a list of length\n$n>0$, then append makes $n-1$ conses to copy the reversed tail.  Constructing\nthe list `[x]` calls `cons` again, for a total of $n$ calls.  Reversing\nthe tail requires $n-1$ more conses, and so forth.  The total number of conses\nis:\n\n$$ 0 + 1 + 2 + \\cdots + n = {n(n+1)/2} $$\n\nThe time complexity is therefore $O(n^2)$.  Space complexity is only $O(n)$\nbecause the copies don't all exist at the same time.\n"},{"cell_type":"markdown","metadata":{},"source":"### Reversing a List in O(n)\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec revApp = function\n  | ([], ys) -> ys\n  | (x::xs, ys) -> revApp (xs, x::ys)","outputs":[],"execution_count":48},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nrevApp([a;b;c], []) \\Rightarrow & revApp([b,c], [a]) \\\\\n  \\Rightarrow & revApp([c], [b;a]) \\\\\n  \\Rightarrow & revApp([], [c;b;a]) \\\\\n  \\Rightarrow & [c;b;a]\n\\end{align*}\n$$\n\nCalling `revApp (xs,ys)` reverses the elements of `xs` and\nprepends them to `ys`.  Now we may declare\n"},{"cell_type":"code","metadata":{},"source":"let rev xs = revApp (xs, []) ;;","outputs":[],"execution_count":49},{"cell_type":"code","metadata":{},"source":"rev [1;2;3] ;;","outputs":[],"execution_count":50},{"cell_type":"markdown","metadata":{},"source":"\nIt is easy to see that this reverse function performs just $n$ conses, given\nan $n$-element list.  For both reverse functions, we could count the number of\nconses precisely---not just up to a constant factor.  $O$ notation is still\nuseful to describe the overall running time: the time taken by a cons\nvaries from one system to another.\n\nThe accumulator $y$ makes the function iterative.  But the gain in complexity\narises from the removal of `append`.  Replacing an expensive operation (append)\nby a series of cheap operations (cons) is called _reduction in strength_\nand is a common technique in computer science.  It originated when many\ncomputers did not have a hardware multiply instruction; the series of products\n$i\\times r$ for $i=0$, $\\ldots, n$ could more efficiently be computed by\nrepeated addition.  Reduction in strength can be done in various ways; we\nshall see many instances of removing append.\n\nConsing to an accumulator produces the result in reverse.  If\nthat forces the use of an extra list reversal then the iterative function\nmay be much slower than the recursive one.\n\nTODO strings\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 4: More on Lists\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"### List Utilities: take and drop\n\n\n\nRemoving the first $i$ elements of a list can be done as follows:\n"},{"cell_type":"code","metadata":{},"source":"let rec take = function\n  | ([], _) -> []\n  | (x::xs, i) ->\n      if i > 0 then\n        x :: take (xs, i-1)\n      else\n        [] ;;","outputs":[],"execution_count":51},{"cell_type":"code","metadata":{},"source":"let rec drop = function\n  | ([], _) -> []\n  | (x::xs, i) ->\n      if i > 0 then\n        drop (xs, i-1)\n      else\n        x::xs ;;","outputs":[],"execution_count":52},{"cell_type":"markdown","metadata":{},"source":"\nThis lecture examines more list utilities, illustrating more patterns of\nrecursion, and concludes with a small program for making change.\n\nThe functions `take` and `drop` divide a list\ninto parts, returning or discarding the first $i$ elements.\n\n$$\nxs = [\\underbrace{x_0,\\ldots,x_{i-1}}_{\\textstyle take(xs,i)},\n      \\underbrace{x_i,\\ldots,x_{n-1}}_{\\textstyle drop(xs,i)} ]\n$$\n\nApplications of `take` and `drop` will appear in future lectures.  Typically,\nthey divide a collection of items into equal parts for recursive processing.\n\nThe special pattern variable `_` appears in both functions.  This _wildcard\npattern_ matches anything.  We could have written `i` in both positions, but\nthe wildcard reminds us that the relevant clause ignores this argument.\n\nFunction `take` is not iterative, but making it so would not improve\nits efficiency.  The task requires copying up to $i$ list elements, which must\ntake $O(i)$ space and time.\n\nFunction `drop` simply skips over $i$ list elements.  This requires\n$O(i)$ time but only constant space.  It is iterative and much faster than\n\\texttt{take}.  Both functions use $O(i)$ time, but skipping elements is faster\nthan copying them:  \\texttt{drop}'s constant factor is smaller.\n\nBoth functions take a list and an integer, returning a list of the same type.\nSo their type is `'a list * int -> 'a list`.\n"},{"cell_type":"markdown","metadata":{},"source":"### Linear Search\n\n\n\nTODO slide\n\n_Linear search_ is the obvious way to find a desired item in a\ncollection: simply look through all the items, one at a time.  If $x$ is in\nthe list, then it will be found in $n/2$ steps on average, and even the worst\ncase is obviously $O(n)$.\n\nLarge collections of data are usually ordered or indexed so that items can be\nfound in $O(\\log n)$ time, which is exponentially better than $O(n)$.  Even\n$O(1)$ is achievable (using a _hash table_), though subject to the usual\nproviso that machine limits are not exceeded.\n\nEfficient indexing methods are of prime importance: consider Web\nsearch engines.  Nevertheless, linear search is often used to search small\ncollections because it is so simple and general, and it is the starting point\nfor better algorithms.\n"},{"cell_type":"markdown","metadata":{},"source":"### Equality Tests\n\n\n\nTODO describe OCaml runtime equality tests\n"},{"cell_type":"markdown","metadata":{},"source":"### Building a List of Pairs\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec zip = function\n  | (x::xs, y::ys) -> (x,y) :: zip (xs,ys)\n  | _ -> [] ;;","outputs":[],"execution_count":53},{"cell_type":"markdown","metadata":{},"source":"\n$$ \\left.[x_1,\\ldots,x_n]\\atop\n         [y_1,\\ldots,y_n]\\right\\}\\;\\longmapsto\\;[(x_1,y_1),\\ldots,(x_n,y_n)]\n$$\n\nThe _wildcard_ pattern `_` matches _anything_. The patterns are also tested\nin order of their definitions.\n\nA list of pairs of the form $[(x_1,y_1),\\ldots,(x_n,y_n)]$ associates each\n$x_i$ with $y_i$.  Conceptually, a telephone directory could be regarded as\nsuch a list, where $x_i$ ranges over names and $y_i$ over the corresponding\ntelephone number.  Linear search in such a list can find the $y_i$ associated\nwith a given $x_i$, or vice versa---very slowly.\n\nIn other cases, the $(x_i,y_i)$ pairs might have been generated by applying a\nfunction to the elements of another list $[z_1,\\ldots,z_n]$.\n"},{"cell_type":"code","metadata":{},"source":"let rec unzip = function\n | [] -> ([], [])\n | (x,y)::pairs ->\n     let xs,ys = unzip pairs in\n     (x::xs, y::ys) ;;","outputs":[],"execution_count":54},{"cell_type":"markdown","metadata":{},"source":"\nThe functions `zip` and `unzip` build and take apart lists of\npairs: `zip` pairs up corresponding list elements and `unzip`\ninverts this operation.  Their types reflect what they do:\n"},{"cell_type":"code","metadata":{},"source":"zip ;;","outputs":[],"execution_count":55},{"cell_type":"code","metadata":{},"source":"unzip ;;","outputs":[],"execution_count":56},{"cell_type":"markdown","metadata":{},"source":"\nIf the lists are of unequal length, `zip` discards surplus items at the\nend of the longer list.  Its first pattern only matches a pair of non-empty\nlists.  The second pattern is just a wildcard and could match anything.  ML\ntries the clauses in the order given, so the first pattern is tried first.\nThe second only gets arguments where at least one of the lists is empty.\n"},{"cell_type":"markdown","metadata":{},"source":"### Building a Pair of Results\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec unzip = function\n | [] -> ([], [])\n | (x,y)::pairs ->\n     let xs,ys = unzip pairs in\n     (x::xs, y::ys) ;;","outputs":[],"execution_count":57},{"cell_type":"code","metadata":{},"source":"let rec revUnzip = function\n  | ([], xs, ys) -> (xs, ys)\n  | ((x,y)::pairs, xs, ys) ->\n      revUnzip (pairs, x::xs, y::ys) ;;","outputs":[],"execution_count":58},{"cell_type":"markdown","metadata":{},"source":"\nGiven a list of pairs, `unzip` has to build _two_ lists of\nresults, which is awkward using recursion.  The version shown about uses the\n_local declaration_ `let D in E`,\nwhere $D$ consists of declarations and $E$ is the expression that can use\nthem. The let-construct counts as an expression and can be used\n(perhaps wrapped within parentheses) wherever an expression is expected.\n\nNote especially the declaration `let xs,ys = unzip pairs`\nwhich binds `xs` and `ys` to the results of the recursive call.\nIn general, the declaration `let P = E` matches the\npattern $P$ against the value of expression $E$.  It binds all the variables\nin $P$ to the corresponding values.\n\nHere is a version of `unzip` that replaces the local declaration by a\nfunction `conspair` for taking apart the pair of lists in the\nrecursive call.  It defines the same\ncomputation as the previous version of `unzip` and is possibly clearer,\nbut not every local declaration can be eliminated as easily.\n"},{"cell_type":"code","metadata":{},"source":"let conspair ((x,y), (xs,ys)) = (x::xs, y::ys) ;;","outputs":[],"execution_count":59},{"cell_type":"code","metadata":{},"source":"let rec unzip = function\n  | [] -> ([], [])\n  | xy :: pairs -> conspair (xy, unzip pairs) ;;","outputs":[],"execution_count":60},{"cell_type":"markdown","metadata":{},"source":"\nMaking the function iterative yields `revUnzip` above, which is\nvery simple.  Iteration can construct many results at once in different\nargument positions.  Both output lists are built in reverse order, which can\nbe corrected by reversing the input to `revUnzip`.  The total costs\nwill probably exceed those of `unzip` despite the advantages of\niteration.\n"},{"cell_type":"markdown","metadata":{},"source":"### An Application: Making Change\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec change = function\n  | till, 0 -> []\n  | c::till, amt ->\n      if amt < c then\n        change (till, amt)\n      else\n        c :: change (c::till, amt-c) ;;","outputs":[],"execution_count":61},{"cell_type":"markdown","metadata":{},"source":"\n- The recursion _terminates_ when `amt = 0`.\n- Tries the _largest coin first_ to use large coins.\n- The algorithm is _greedy_ and can fail!\n\nThe till has unlimited supplies of coins.  The largest coins should be tried\nfirst, to avoid giving change all in pennies.  The list of legal coin values,\ncalled `till`, is given in descending order, such as 50, 20, 10, 5,\n2 and 1.  (Recall that the head of a list is the element most easily reached.)\nThe code for `change` is based on simple observations:\n- Change for zero consists of no coins at all.  (Note the pattern of `0` in the first clause.)\n- For a nonzero amount, try the largest available coin.  If it is small enough, use it and decrease the amount accordingly.\n- Exclude from consideration any coins that are too large.\n\nAlthough nobody considers making change for zero, this is the simplest way to\nmake the algorithm terminate.  Most iterative procedures become simplest if,\nin their base case, they do nothing.  A base case of one instead of zero is\noften a sign of a novice programmer.\n\nThe function can terminate either with success or failure.  It fails by\nraising exception `Match_failure`, signifying that no pattern matches,\nnamely if `till` becomes empty while `amt` is still nonzero.\n(Exceptions will be discussed later.)\n\nUnfortunately, failure can occur even when change can be made.  The greedy\n`largest coin first' approach is to blame.  Suppose we have coins of values 5\nand 2, and must make change for 6; the only way is $6=2+2+2$, ignoring the 5.\n_Greedy algorithms_ are often effective, but not here.\n"},{"cell_type":"markdown","metadata":{},"source":"### All Ways of Making Change\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec change = function\n  | (till, 0) -> [ [] ]\n  | ([], amt) -> []\n  | (c::till, amt) ->\n      if amt < c then\n        change (till, amt)\n      else\n        let rec allc = function\n          | [] -> []\n          | cs :: css -> (c::cs) :: allc css\n        in\n        allc (change (c::till, amt-c)) @\n        change (till, amt)\n ;;","outputs":[],"execution_count":62},{"cell_type":"markdown","metadata":{},"source":"\nNow we generalize the problem to return the list of _all possible ways_ of making change.\nLook at the type: the result is now a list of lists.\n\nThe code will never raise exceptions.  It expresses failure by returning an\nempty list of solutions: it returns `[]` if the till is empty and the\namount is nonzero.\n\nIf the amount is zero, then there is only one way of making change;\nthe result should be `[[]]`.  This is success in the base case.\n\nIn nontrivial cases, there are two sources of solutions: to use a coin (if\npossible) and decrease the amount accordingly, or to remove the current coin\nvalue from consideration.\n\nThe function `allc` is declared locally in order to make use\nof `c`, the current coin.  It adds an extra `c` to all the\nsolutions returned by the recursive call to make change for `amt - c`.\n\nObserve the naming convention: `cs` is a list of coins, while\n`css` is a list of such lists.  The trailing `s' is suggestive of a\nplural.\n\nThis complicated program, and the even trickier one on the next slide, are\nincluded as challenges.  Are you enthusiastic enough to work them out?  We\nshall revisit the \"making change\" task later to illustrate exception-handling.\n"},{"cell_type":"markdown","metadata":{},"source":"### All Ways of Making Change --- Faster!\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec change = function\n  | till, 0, chg, chgs -> chg::chgs\n  | [], amt, chg, chgs -> chgs\n  | c::till, amt, chg, chgs ->\n      if amt < 0 then\n        chgs\n      else\n        change (c::till, amt-c, c::chg,\n                change (till, amt, chg, chgs))\n;;","outputs":[],"execution_count":63},{"cell_type":"markdown","metadata":{},"source":"We've added _another_ accumulating parameter!  Repeatedly improving simple code\nis called _stepwise refinement_.\n\nTwo extra arguments eliminate many `::` and append operations from the previous\nslide's change function.  The first, `chg`, accumulates the coins chosen so\nfar; one evaluation of c::chg} replaces many evaluations of \\texttt{allc}.  The\nsecond, `chgs`, accumulates the list of solutions so far; it avoids the need\nfor append.  This version runs several times faster than the previous one.\n\nMaking change is still extremely slow for an obvious reason: the number of\nsolutions grows rapidly in the amount being changed.  Using 50, 20, 10, 5,\n2 and 1, there are 4366 ways of expressing 99.\n \nOur three change functions illustrate a basic technique: program development\nby stepwise refinement.  Begin by writing a very simple program and add\nrequirements individually.  Add efficiency refinements last of all.\nEven if the simpler program cannot be included in the next version and has\nto be discarded, one has learned about the task by writing it.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 5: Sorting\n\n\n\n### Sorting: Arranging Items into Order\n\nA few applications:\n- search\n- merging\n- duplicates\n- inverting tables\n- graphics algorithms\n\nSorting is perhaps the most deeply studied aspect of algorithm design.\nKnuth's series _The Art of Computer Programming_ devotes an entire\nvolume to sorting and searching!  Sedgewick (TODO cite)\nalso covers sorting.  Sorting has countless applications.\n\nSorting a collection allows items to be found quickly.  Recall that linear\nsearch requires $O(n)$ steps to search among $n$ items.  A sorted collection\nadmits _binary search_ which requires only $O(\\log n)$ time.  The idea\nof binary search is to compare the item being sought with the middle item (in\nposition $n/2$) and then to discard either the left half or the right,\ndepending on the result of the comparison.  Binary search needs arrays or\ntrees, not lists; we shall come to binary search trees later.\n\nTwo sorted files can quickly be _merged_ to form a larger sorted file.  Other\napplications include finding _duplicates_ that, after sorting, are adjacent.\n\nA telephone directory is sorted alphabetically by name.  The same information\ncan instead be sorted by telephone number (useful to the police) or by street\naddress (useful to junk-mail firms).  Sorting information in different ways\ngives it different applications.\n\nCommon sorting algorithms include insertion sort, quicksort,\nmergesort and heapsort.  We shall consider the first three of\nthese.  Each algorithm has its advantages.\n\nAs a concrete basis for comparison, runtimes are quoted for DECstation\ncomputers.  (These were based on the MIPS chip, an early RISC design.)\n(TODO add benchmarks)\n"},{"cell_type":"markdown","metadata":{},"source":"### How Fast Can We Sort?\n\n\n\n- typically count _comparisons_ $C(n)$\n- there are $n!$ permutations of $n$ elements\n- each comparison eliminates _half_ of the permutations $2^{C(n)}\\geq n!$\n- therefore $C(n)\\geq \\log(n!)\\approx n\\log n-1.44n$\n\nThe usual measure of efficiency for sorting algorithms is the number of\ncomparison operations required.  Mergesort requires only $O(n\\log n)$\ncomparisons to sort an input of $n$ items.  It is straightforward to prove\nthat this complexity is the best possible (TODO cite pages 86--7  aho74).  There\nare $n!$ permutations of $n$ elements and each comparison distinguishes two\npermutations.  The lower bound on the number of comparisons, $C(n)$, is\nobtained by solving $2^{C(n)}\\geq n!$; therefore $C(n)\\geq \\log(n!)\\approx\nn\\log n-1.44n$.\n\nIn order to compare the sorting algorithms, we use the following source of\npseudo-random numbers (TODO cite park88). Never mind how this works: generating\nstatistically good random numbers is hard.  Much effort has gone into those few\nlines of code.\n"},{"cell_type":"code","metadata":{},"source":"let nextrandom seed =\n  let a = 16807.0 in\n  let m = 2147483647.0 in\n  let t = a *. seed in\n  t -. m *. (floor (t /. m)) ;;","outputs":[],"execution_count":64},{"cell_type":"code","metadata":{},"source":"let rec randlist (seed,seeds) = function\n  | 0 -> (seed, seeds)\n  | n -> randlist (nextrandom seed, seed::seeds) (n-1) ;;","outputs":[],"execution_count":65},{"cell_type":"markdown","metadata":{},"source":"\nWe can now bind the identifier `rs` to a list of 10,000 random numbers.\n"},{"cell_type":"code","metadata":{},"source":"let seed, rs = randlist (1.0, []) 10000 ;;","outputs":[],"execution_count":66},{"cell_type":"markdown","metadata":{},"source":"\n### Insertion Sort\n\nAn insert does does $n/2$ comparisons on average.\n"},{"cell_type":"code","metadata":{},"source":"let rec ins = function\n  | x, [] -> [x]\n  | x, y::ys ->\n      if x <= y then\n        x :: y :: ys\n      else\n        y :: ins (x,ys)","outputs":[],"execution_count":67},{"cell_type":"markdown","metadata":{},"source":"\n_Insertion sort_ takes $O(n^2)$ comparisons on average:\n"},{"cell_type":"code","metadata":{},"source":"let rec insort = function\n    | [] -> []\n    | x::xs -> ins (x, insort xs)","outputs":[],"execution_count":68},{"cell_type":"markdown","metadata":{},"source":"\nItems from the input are copied one at a time to the output.  Each new item is\ninserted into the right place so that the output is always in order.\n\nWe could easily write iterative versions of these functions, but to no purpose.\nInsertion sort is slow because it does $O(n^2)$ comparisons (and a lot of list\ncopying), not because it is recursive.  Its quadratic runtime makes it nearly\nuseless: it takes 174 seconds for our example while the next-worst figure is\n1.4 seconds.\n\nInsertion sort is worth considering because it is easy to code and illustrates\nthe concepts.  Two efficient sorting algorithms, mergesort and heapsort, can be\nregarded as refinements of insertion sort.\n\nTODO:\n> The notion of\n> sorting depends upon the form of comparison being done, which in turn\n> determines the type of the sorting function.\n\n### Quicksort: The Idea\n\n- choose a _pivot_ element, $a$\n- Divide to partition the input into two sublists:\n  * those _at most_ $a$ in value\n  * those _exceeding_ $a$\n- Conquer using recursive calls to sort the sublists\n- Combine the sorted lists by appending one to the other\n\nQuicksort was invented by C. A. R. Hoare, who now works at Microsoft Research,\nCambridge.  Quicksort works by _divide and conquer_, a basic algorithm design\nprinciple.  Quicksort chooses from the input some value $a$, called the\n_pivot_.  It partitions the remaining items into two parts: those $\\leq a$, and\nthose $>a$.  It sorts each part recursively, then puts the smaller part before\nthe greater.\n\nThe cleverest feature of Hoare's algorithm was that the partition could be done\n_in place_ by exchanging array elements.  Quicksort was invented before\nrecursion was well known, and people found it extremely hard to understand.  As\nusual, we shall consider a list version based on functional programming.\n"},{"cell_type":"markdown","metadata":{},"source":"### Quicksort: The Code\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec quick = function\n  | [] -> []\n  | [x] -> [x]\n  | a::bs ->\n      let rec part = function\n        | (l,r,[]) -> (quick l) @ (a :: quick r)\n        | (l,r,x::xs) ->\n            if (x <= a) then\n              part (x::l, r, xs)\n            else\n              part (l, x::r, xs)\n      in\n      part ([], [], bs)","outputs":[],"execution_count":69},{"cell_type":"markdown","metadata":{},"source":"\nOur ML quicksort copies the items.  It is still pretty fast, and it is much\neasier to understand.  It takes roughly 0.74 seconds to sort our list of random\nnumbers.\n\nThe function declaration consists of three clauses.  The first handles the\nempty list; the second handles singleton lists (those of the form `[x]`; the\nthird handles lists of two or more elements.  Often, lists of length up to five\nor so are treated as special cases to boost speed.\n\nThe locally declared function `part` partitions the input using `a` as the\npivot.  The arguments `l` and `r` accumulate items for the left ($\\leq a$) and\nright ($>a$) parts of the input, respectively.\n\nIt is not hard to prove that quicksort does $n\\log n$ comparisons, _in the average case_\n(TODO cite page 94 aho74).  With random data, the pivot\nusually has an average value that divides the input in two approximately equal\nparts.  We have the recurrence $T(1) = 1$ and $T(n) = 2T(n/2)+n$, which is\n$O(n\\log n)$.  In our example, it is about 235 times faster than insertion\nsort.\n\nIn the worst case, quicksort's running time is quadratic!  An example is when\nits input is almost sorted or reverse sorted.  Nearly all of the items end up\nin one partition; work is not divided evenly.  We have the recurrence\n$T(1) = 1$ and $T(n+1) = T(n)+n$, which is $O(n^2)$.  Randomizing the input\nmakes the worst case highly unlikely.\n"},{"cell_type":"markdown","metadata":{},"source":"### Append-Free Quicksort\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec quik = function\n  | ([], sorted) -> sorted\n  | ([x], sorted) -> x::sorted\n  | a::bs, sorted ->\n     let rec part = function\n       | l, r, [] -> quik (l, a :: quik (r,sorted))\n       | l, r, x::xs ->\n           if x <= a then\n             part (x::l, r, xs)\n           else\n             part (l, x::r, xs)\n     in\n     part ([], [], bs)","outputs":[],"execution_count":70},{"cell_type":"markdown","metadata":{},"source":"\nThe list `sorted` accumulates the result in the _combine_ stage of\nthe quicksort algorithm.  We have again used the standard technique for\neliminating append.  Calling `quik(xs,sorted)` reverses the elements of\n`xs` and prepends them to the list `sorted`.\n\nLooking closely at `part`, observe that `quik(r,sorted)` is\nperformed first.  Then `a` is consed to this sorted list.  Finally,\n`quik` is called again to sort the elements of `l`.\n\nThe speedup is significant.  An imperative quicksort coded in Pascal (taken\nfrom Sedgewick (TODO cite sedgewick88) is just slightly faster than function\n`quik`.  The near-agreement is surprising because the computational overheads\nof lists exceed those of arrays.  In realistic applications, comparisons are\nthe dominant cost and the overheads matter even less.\n\n### Merging Two Lists\n\nMerge joins two sorted lists.\n"},{"cell_type":"code","metadata":{},"source":"let rec merge = function\n  | [], ys -> ys\n  | xs, [] -> xs\n  | x::xs, y::ys ->\n      if x <= y then\n        x :: merge (xs, y::ys)\n      else \n        y :: merge (x::xs, ys)","outputs":[],"execution_count":71},{"cell_type":"markdown","metadata":{},"source":"\nGeneralises insert to two lists, and does at most $m+n-1$ comparisons.\n\n_Merging_ means combining two sorted lists to form a larger sorted list.\nIt does at most $m+n$ comparisons, where $m$ and $n$ are the lengths of the\ninput lists.  If $m$ and $n$ are roughly equal then we have a fast way of\nconstructing sorted lists; if $n=1$ then merging degenerates to insertion,\ndoing much work for little gain.\n\nMerging is the basis of several sorting algorithms; we look at a\ndivide-and-conquer one.  Mergesort is seldom found in conventional programming\nbecause it is hard to code for arrays; it works nicely with lists.  It divides\nthe input (if non-trivial) into two roughly equal parts, sorts them\nrecursively, then merges them.\n\nFunction `merge` is not iterative; the recursion is deep.  An iterative\nversion is of little benefit for the same reasons that apply to\n`append` in the earlier lecture on Lists.\n"},{"cell_type":"markdown","metadata":{},"source":"### Top-down Merge sort\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec tmergesort = function\n  | [] -> []\n  | [x] -> [x]\n  | xs ->\n      let k = List.length xs / 2 in\n      let l = tmergesort (take (xs, k)) in\n      let r = tmergesort (drop (xs,k)) in\n      merge (l,r)","outputs":[],"execution_count":72},{"cell_type":"markdown","metadata":{},"source":"\n$O(n\\log n)$ comparisons in worst case\n\nMergesort's _divide_ stage divides the input not by choosing a pivot (as\nin quicksort) but by simply counting out half of the elements.  The\n_conquer_ stage again involves recursive calls, and the _combine_\nstage involves merging.  Function `tmergesort` takes roughly 1.4\nseconds to sort the list `rs`.\n\nIn the worst case, mergesort does $O(n\\log n)$ comparisons, with the same\nrecurrence equation as in quicksort's average case.  Because `take` and\n`drop` divide the input in two equal parts (they differ at most by\none element), we always have $T(n) = 2T(n/2)+n$.\n\nQuicksort is nearly 3 times as fast in the example.  But it risks a\nquadratic worst case!  Merge sort is safe but slow.  So which algorithm is\nbest?\n\nWe have seen a _top-down_ mergesort.  _Bottom-up_ algorithms also\nexist.  They start with a list of one-element lists and repeatedly merge\nadjacent lists until only one is left.  A refinement, which exploits any\ninitial order among the input, is to start with a list of increasing or\ndecreasing runs of input items.\n"},{"cell_type":"markdown","metadata":{},"source":"### Summary of Sorting Algorithms\n\n\n\n- Optimal is $O(n\\log n)$ comparisons\n- Insertion sort: simple to code; too slow (_quadratic_) [174 secs]\n- Quicksort: fast on average; _quadratic_ in worst case [0.53 secs]\n- Mergesort: optimal in theory; often slower than quicksort [1.4 secs]\n- _Match the algorithm to the application_\n\nQuicksort's worst case cannot be ignored.  For large $n$, a complexity of\n$O(n^2)$ is catastrophic.  Mergesort has an $O(n\\log n)$ worst case running\ntime, which is optimal, but it is typically slower than quicksort for random\ndata.\n\nNon-comparison sorting deserves mentioning.  We can sort a large number of\nsmall integers using their radix representation in $O(n)$ time.  This result\ndoes not contradict the comparison-counting argument because comparisons are\nnot used at all.  Linear time is achievable only if the greatest integer is\nfixed in advance; as $n$ goes to infinity, increasingly many of the items\nare the same.  It is a simple special case.\n\nMany other sorting algorithms exist. A few are outlined in the exercises.\n\n## Lecture 6: Datatypes and Trees\n"},{"cell_type":"markdown","metadata":{},"source":"### An Enumeration Type\n\n\n\n"},{"cell_type":"code","metadata":{},"source":"type vehicle =   Bike\n               | Motorbike\n               | Car\n               | Lorry","outputs":[],"execution_count":73},{"cell_type":"markdown","metadata":{},"source":"\n- We have declared a _new type_ named `vehicle`.\n- $\\ldots$ along with four new constants.\n- They are the \\emph{constructors} of the datatype.\n\nThe `type` declaration adds a new type to our OCalm session.  Type\n`vehicle` is as good as any built-in type and even admits\npattern-matching.  The four new identifiers of type `vehicle` are\ncalled _constructors_.\n\nWe could represent the various vehicles by the numbers 0--3.  However, the code would be\nhard to read and even harder to maintain.  Consider adding `Tricycle`\nas a new vehicle. If we wanted to add it before `Bike`, then all the\nnumbers would have to be changed.  Using `type`, such additions are\ntrivial and the compiler can (at least sometimes) warn us when it encounters a\nfunction declaration that doesn't yet have a case for `Tricycle`.\n\nRepresenting vehicles by strings like `\"Bike\"`, `\"Car\"`, etc.,\nis also bad.  Comparing string values is slow and the compiler\ncan't warn us of misspellings like `\"MOtorbike\"`: they will make our\ncode fail.\n\nMost programming languages allow the declaration of types like\n`vehicle`.  Because they consist of a series of identifiers, they are\ncalled _enumeration types_.  Other common examples are days of the week\nor colours.  The compiler chooses the integers for us; type-checking prevents\nus from confusing `Bike` with `Red` or `Sunday`.\n"},{"cell_type":"markdown","metadata":{},"source":"### Declaring a Function on Vehicles\n\n\n"},{"cell_type":"code","metadata":{},"source":"let wheels = function\n  | Bike -> 2\n  | Motorbike -> 2\n  | Car -> 4\n  | Lorry -> 18","outputs":[],"execution_count":74},{"cell_type":"markdown","metadata":{},"source":"\n- Datatype constructors can be used in patterns.\n- Pattern-matching is fast, even complicated nested patterns.\n\nThe beauty of datatype declarations is that the new types behave as if they\nwere built into OCaml. Type-checking catches common errors, such as mixing up\ndifferent datatypes in a function like `wheels`, as well as missing\nand redundant patterns.\n"},{"cell_type":"markdown","metadata":{},"source":"### A Datatype with Constructor Functions\n\n\n"},{"cell_type":"code","metadata":{},"source":"type vehicle =   Bike\n               | Motorbike of int\n               | Car       of bool\n               | Lorry     of int","outputs":[],"execution_count":75},{"cell_type":"markdown","metadata":{},"source":"\n- Constructor functions (like `Lorry`) make _distinct values_.\n- Different kinds of `vehicle` can belong to one list: `[Bike, Car true, Motorbike 450]`\n\nOCaml generalizes the notion of enumeration type to allow data to be associated\nwith each constructor.  The constructor `Bike` is a vehicle all by itself, but\nthe other three constructors are functions for creating vehicles.\n\nSince we might find it hard to remember what the various `int` and\n`bool` components are for, it is wise to include _comments_ in\ncomplex declarations.  In ML, comments are enclosed in the brackets\n`(*` and `*)`.  Programmers should comment their code to explain\ndesign decisions and key features of the algorithms (sometimes by citing a\nreference work).\n"},{"cell_type":"code","metadata":{},"source":"type vehicle =   Bike\n               | Motorbike of int  (* engine size in CCs *)\n               | Car       of bool (* true if a Reliant Robin *)\n               | Lorry     of int  (* number of wheels *)","outputs":[],"execution_count":76},{"cell_type":"markdown","metadata":{},"source":"The list shown on the slide represents a bicycle, a Reliant Robin and a large\nmotorbike.  It can be almost seen as a mixed-type list containing integers and\nbooleans.  It is actually a list of vehicles; datatypes lessen the impact of\nthe restriction that all list elements must have the same type.\n"},{"cell_type":"markdown","metadata":{},"source":"### A Finer Wheel Computation\n\n\n"},{"cell_type":"code","metadata":{},"source":"let wheels = function\n  | Bike -> 2\n  | Motorbike _ -> 2\n  | Car robin -> if robin then 3 else 4\n  | Lorry w -> w","outputs":[],"execution_count":77},{"cell_type":"markdown","metadata":{},"source":"\nThis function consists of four clauses:\n- A Bike has two wheels.\n- A Motorbike has two wheels.\n- A Reliant Robin has three wheels; all other cars have four.\n- A Lorry has the number of wheels stored with its constructor.\n\nThere is no overlap between the `Motorbike` and `Lorry` cases.  Although\n`Motorbike` and `Lorry` both hold an integer, ML takes the\nconstructor into account. A Motorbike is distinct from any Lorry.\n\nVehicles are one example of a concept consisting of several varieties with\ndistinct features.  Most programming languages can represent such concepts\nusing something analogous to datatypes.  (They are sometimes called\n_union types_ or _variant records_ whose _tag fields_ play the\nrole of the constructors.)\n\n\nA pattern may be built from the constructors of several datatypes, including lists. A pattern may also contain integer and string constants. There is no limit to the size of patterns or the number of clauses in a function declaration. Most ML systems perform pattern-matching efficiently.\n"},{"cell_type":"markdown","metadata":{},"source":"### Error Handling: Exceptions\n\n\n\nDuring a computation, what happens if something goes _wrong_?\n- Division by zero\n- Pattern matching failure\n\n_Exception-handling_ lets us recover gracefully.\n- Raising an exception abandons the current computation.\n- Handling the exception attempts an alternative computation.\n- The raising and handling can be far apart in the code.\n- Errors of _different sorts_ can be handled separately.\n\nExceptions are necessary because it is not always possible to tell in advance\nwhether or not a search will lead to a dead end or whether a numerical\ncalculation will encounter errors such as overflow or divide by zero. Rather\nthan just crashing, programs should check whether things have gone wrong, and\nperhaps attempt an alternative computation (perhaps using a different algorithm\nor higher precision). A number of modern languages provide exception handling.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exceptions in ML\n\n\n"},{"cell_type":"code","metadata":{},"source":"exception Failure ;;","outputs":[],"execution_count":78},{"cell_type":"code","metadata":{},"source":"exception NoChange of int ;;","outputs":[],"execution_count":79},{"cell_type":"code","metadata":{},"source":"raise Failure ;;","outputs":[],"execution_count":80},{"cell_type":"code","metadata":{},"source":"try\n  print_endline \"pre exception\";\n  raise (NoChange 1);\n  print_endline \"post exception\";\nwith\n  | NoChange _ ->\n      print_endline \"handled a NoChange exception\"\n;;","outputs":[],"execution_count":81},{"cell_type":"markdown","metadata":{},"source":"\nEach `exception` declaration introduces a distinct sort of exception, which can\nbe handled separately from others. If $E$ raises an exception, then its\nevaluation has failed; _handling_ an exception means evaluating another\nexpression and returning its value instead. One exception handler can specify\nseparate expressions for different sorts of exceptions.\n\nException names are _constructors_ of the special datatype `exn`.  This is a\npeculiarity of ML that lets exception-handlers use pattern-matching. Note that\nexception `Failure` is just an error indication, while `NoChange n` carries\nfurther information: the integer $n$.\n\nThe effect of `raise <expr>` is to jump to the most recently-encountered\nhandler that matches `<expr>`.  The matching handler can only be found\n_dynamically_ (during execution); contrast with how ML associates occurrences\nof identifiers with their matching declarations, which does not require running\nthe program.\n\nOne criticism of OCaml's exceptions is that---unlike the Java language---nothing\nin a function declaration indicates which exceptions it might raise. One\nalternative to exceptions is to instead return a value of datatype `option`.\n"},{"cell_type":"code","metadata":{},"source":"let x = Some 1 ;;","outputs":[],"execution_count":82},{"cell_type":"code","metadata":{},"source":"let y = None ;;","outputs":[],"execution_count":83},{"cell_type":"code","metadata":{},"source":"type 'a option = None | Some of 'a ;;","outputs":[],"execution_count":84},{"cell_type":"markdown","metadata":{},"source":"\n`None` signifies an error, while `Some x` returns the solution $x$.  This\napproach looks clean, but the drawback is that many places in the code would\nhave to check for `None`.  Despite this, there is a builtin `option` type\nin OCaml as it is so useful.\n"},{"cell_type":"markdown","metadata":{},"source":"### Making Change with Exceptions\n\n\n"},{"cell_type":"code","metadata":{},"source":"exception Change\nlet rec change = function\n  | till, 0 -> []\n  | [], amt -> raise Change\n  | c::till, amt ->\n      if amt < 0 then\n        raise Change\n      else begin\n        try\n           c :: change (c::till, amt-c)\n         with\n           Change -> change (till,amt)\n      end\n ;;","outputs":[],"execution_count":85},{"cell_type":"markdown","metadata":{},"source":"\nIn the Lists lectures, we considered the problem of making change.  The greedy\nalgorithm presented there could not express \"6 using 5 and 2\" because it always\ntook the largest coin.  Returning the list of all possible solutions avoids\nthat problem rather expensively: we only need one solution.\n\nUsing exceptions, we can code a _backtracking_ algorithm: one that can undo\npast decisions if it comes to a dead end.  The exception `Change` is raised if\nwe run out of coins (with a non-zero amount) or if the amount goes negative.\nWe always try the largest coin, but enclose the recursive call in an exception\nhandler, which undoes the choice if it goes wrong.\n\nCarefully observe how exceptions interact with recursion.  The exception\nhandler always undoes the \\emph{most recent} choice, leaving others possibly to\nbe undone later.  If making change really is impossible, then eventually\nexception \\texttt{Change} will be raised with no handler to catch it, and it\nwill be reported at top level.\n\n### Making Change: A Trace\n"},{"cell_type":"raw","metadata":{},"source":"change([5,2],6)\n5::change([5,2],1) handle C => change([2],6)\n5::(5::change([5,2],~4) handle C => change([2],1))\n    handle C => change([2],6)\n5::change([2],1) handle C => change([2],6)\n5::(2::change([2],~1) handle C => change([],1))\n    handle C => change([2],6)\n5::(change([],1)) handle C => change([2],6)\nchange([2],6)"},{"cell_type":"markdown","metadata":{},"source":"\nHere is the full execution. Observe how the exception handlers nest and how\nthey drop away once the given expression has returned a value.\n"},{"cell_type":"raw","metadata":{},"source":"change([5,2],6)\n5::change([5,2],1) handle C => change([2],6)\n5::(5::change([5,2],~4) handle C => change([2],1))\n    handle C => change([2],6)\n5::change([2],1) handle C => change([2],6)\n5::(2::change([2],~1) handle C => change([],1))\n    handle C => change([2],6)\n5::(change([],1)) handle C => change([2],6)\nchange([2],6)\n2::change([2],4) handle C => change([],6)\n2::(2::change([2],2) handle C => change([],4)) handle ...\n2::(2::(2::change([2],0) handle C => change([],2)) handle C => ...)\n2::(2::[2] handle C => change([],4)) handle C => change([],6)\n2::[2,2] handle C => change([],6)\n[2,2,2]"},{"cell_type":"markdown","metadata":{},"source":"\n### Binary Trees, a Recursive Datatype\n"},{"cell_type":"code","metadata":{},"source":"type 'a tree =\n    Lf\n  | Br of 'a * 'a tree * 'a tree","outputs":[],"execution_count":86},{"cell_type":"markdown","metadata":{},"source":"\nTODO includegraphics(bintree)\n"},{"cell_type":"code","metadata":{},"source":"Br(1, Br(2, Br(4, Lf, Lf),\n              Br(5, Lf, Lf)),\n                Br(3, Lf, Lf))","outputs":[],"execution_count":87},{"cell_type":"markdown","metadata":{},"source":"\nA data structure with multiple branching is called a \"tree\".  Trees can\nrepresent mathematical expressions, logical formulae, computer programs, the\nphrase structure of English sentences, etc.\n\n_Binary trees_ are nearly as fundamental as lists.  They can provide\nefficient storage and retrieval of information.  In a binary tree, each node\nis empty ($Lf$), or is a branch ($Br$) with a label and two subtrees.\n\nOCaml lists are a datatype and could be declared as follows:\n"},{"cell_type":"code","metadata":{},"source":"type 'a list =\n | Nil\n | Cons of 'a * 'a list","outputs":[],"execution_count":88},{"cell_type":"markdown","metadata":{},"source":"\nWe could even declare `::` as an infix constructor.  The only\nthing we could not define is the `[...]` notation, which is\npart of the OCaml grammar (although there does exist a mechanism\nto use a _similar_ syntax for custom indexed datatypes).\n\nA recursive type does not have to be polymorphic.\nFor example, here is a simple datatype of tree shapes with no attached data\nthat is recursive but not polymorphic.\n"},{"cell_type":"code","metadata":{},"source":"type shape =\n    Null\n  | Join of shape * shape","outputs":[],"execution_count":89},{"cell_type":"markdown","metadata":{},"source":"\nThe datatype `'a option` (mentioned above) is the opposite -- it is\npolymorphic, but not recursive.\n"},{"cell_type":"markdown","metadata":{},"source":"### Basic Properties of Binary Trees\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec count = function\n  | Lf -> 0  (* number of branch nodes *)\n  | Br (v, t1, t2) -> 1 + count t1 + count t2","outputs":[],"execution_count":90},{"cell_type":"code","metadata":{},"source":"let rec depth = function\n  | Lf -> 0  (* length of longest path *)\n  | Br (v, t1, t2) -> 1 + max (depth t1) (depth t2)","outputs":[],"execution_count":91},{"cell_type":"markdown","metadata":{},"source":"\n"}]}