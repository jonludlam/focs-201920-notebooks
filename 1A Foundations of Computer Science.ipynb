{"metadata":{"kernelspec":{"display_name":"OCaml 4.07.1","language":"OCaml","name":"ocaml-jupyter"},"language_info":{"name":"OCaml","version":"4.07.1","codemirror_mode":"text/x-ocaml","file_extension":".ml","mimetype":"text/x-ocaml","nbconverter_exporter":null,"pygments_lexer":"OCaml"}},"nbformat":4,"nbformat_minor":2,"cells":[{"cell_type":"markdown","metadata":{},"source":"# Foundations of Computer Science (OCaml version)\n\n\n\nThis course has two aims. The first is to teach programming. The second is to\npresent some fundamental principles of computer science, especially algorithm\ndesign. Most students will have some programming experience already, but there\nare few people whose programming cannot be improved through greater knowledge\nof basic principles. Please bear this point in mind if you have extensive\nexperience and find parts of the course rather slow.\n\nThe programming in this course is based on the language [OCaml](https://ocaml.org)\nand mostly concerns the functional programming style. Functional programs tend\nto be shorter and easier to understand than their counterparts in conventional\nlanguages such as C. In the space of a few weeks, we shall cover many\nfundamental data structures and learn basic methods for estimating efficiency.\n\n**this is a work-in-progress port of Lawrence C. Paulson's 1819 Cambridge\ncourse notes**\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 1: Introduction to Programming\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"### Basic Concepts in Computer Science\n\n\n\n- Computers: a child can use them; **nobody** can fully understand them!\n- We can master complexity through levels of abstraction.\n- Focus on 2 or 3 levels at most!\n\n**Recurring issues:**\n- what services to provide at each level\n- how to implement them using lower-level services\n- the interface: how the two levels should communicate\n\nA basic concept in computer science is that large systems can only be\nunderstood in levels, with each level further subdivided into functions or\nservices of some sort. The interface to the higher level should supply the\nadvertised services. Just as important, it should block access to the means by\nwhich those services are implemented. This _abstraction barrier_ allows one\nlevel to be changed without affecting levels above. For example, when a\nmanufacturer designs a faster version of a processor, it is essential that\nexisting programs continue to run on it. Any differences between the old and\nnew processors should be invisible to the program.\n\nModern processors have elaborate specifications, which still sometimes leave\nout important details. In the old days, you then had to consult the circuit\ndiagrams.\n"},{"cell_type":"markdown","metadata":{},"source":"### Example 1: Dates\n\n\n\n- Abstract level: dates over a certain interval\n- Concrete level: typically 6 characters: `YYMMDD` (where each character is represented by 8 bits)\n- Date crises caused by __inadequate__ internal formats:\n  * Digital’s PDP-10: using 12-bit dates (good for at most 11 years)\n  * 2000 crisis: 48 bits could be good for lifetime of universe!\n\nDigital Equipment Corporation’s date crisis occurred in 1975.  The\nPDP-10 was a 36-bit mainframe computer. It represented dates using a 12-bit\nformat designed for the tiny PDP-8. With 12 bits, one can distinguish\n$2^{12} = 4096$ days or 11 years.\n\nThe most common industry format for dates uses six characters: two for the\nyear, two for the month and two for the day. The most common \"solution\" to the\nyear 2000 crisis is to add two further characters, thereby altering file sizes.\nOthers have noticed that the existing six characters consist of 48 bits,\nalready sufficient to represent all dates over the projected lifetime of the\nuniverse: $2^{48}$ = $2.8 * 1014$ days = $7.7 * 1011$ years!\n\nMathematicians think in terms of unbounded ranges, but the representation we\nchoose for the computer usually imposes hard limits. A good programming\nlanguage like OCaml lets one easily change the representation used in the\nprogram.  But if files in the old representation exist all over the place,\nthere will still be conversion problems. The need for compatibility with older\nsystems causes problems across the computer industry.\n"},{"cell_type":"markdown","metadata":{},"source":"### Example II: Floating Point Numbers\n\n\n\nComputers have integers like `1066` and floats like $1.066 x 10^3$.\nA floating-point number is represented by two integers.\nThe concept of _data type_ involves:\n* how a value is represented inside the computer\n* the suite of operations given to programmers\n* valid and invalid (or exceptional) results, such as “infinity”\nComputer arithmetic can yield _incorrect answers_!\n\nIn science, numbers written with finite precision and a decimal exponent are\nsaid to be in _standard form_. The computational equivalent is the _floating\npoint number_. These are familiar to anybody who has used a scientific\ncalculator.  Internally, a float consists of two integers.\n\nBecause of its finite precision, floating-point computations are potentially\ninaccurate. To see an example, use your nearest electronic calculator to\ncompute $(2^{1/10000})10000$. I get $1.99999959$! With certain computations,\nthe errors spiral out of control. Many programming languages fail to check\nwhether even integer computations fall within the allowed range: you can add\ntwo positive integers and get a negative one!\n\nMost computers give us a choice of precisions. In 32-bit precision, integers\ntypically range from $2^{31} − 1$ (namely $2,147,483,647$) to $−2^{31}$; reals\nare accurate to about six decimal places and can get as large as 1035 or so.\nFor reals, 64-bit precision is often preferred. Early languages like Fortran\nrequired variables to be declared as `INTEGER`, `REAL` or `COMPLEX` and barred\nprogrammers from mixing numbers in a computation. Nowadays, programs handle\nmany different kinds of data, including text and symbols. The concept of a\n_data type_ can ensure that different types of data are not combined in a\nsenseless way.\n\nInside the computer, all data are stored as bits. In most programming\nlanguages, the compiler uses types to generate correct machine code, and types\nare not stored during program execution. In this course, we focus almost\nentirely on programming in a high-level language: OCaml.\n"},{"cell_type":"markdown","metadata":{},"source":"### Goals of Programming\n\n\n\n- to describe a computation so that it can be done _mechanically_:\n  * Expressions compute values.\n  * Commands cause effects.\n- to do so efficiently and **correctly**, giving the right answers quickly\n- to allow easy modification as needs change\n  * Through an orderly _structure_ based on abstraction principles\n  * Such as modules or (Java) classes\n\nProgramming _in-the-small_ concerns the writing of code to do simple, clearly\ndefined tasks. Programs provide expressions for describing mathematical\nformulae and so forth. (This was the original contribution of FORTRAN, the\nFORmula TRANslator.) Commands describe how control should flow from one part of\nthe program to the next.\n\nAs we code layer upon layer, we eventually find ourselves programming\n_in the large_ : joining large modules to solve some messy task. Programming\nlanguages have used various mechanisms to allow one part of the program to\nprovide interfaces to other parts. Modules encapsulate a body of code, allowing\noutside access only through a programmer-defined interface. _Abstract Data\nTypes_ are a simpler version of this concept, which implement a single concept\nsuch as dates or floating-point numbers.\n\n_Object-oriented programming_ is the most complicated approach to modularity.\n_Classes_ define concepts, and they can be built upon other classes. Operations\ncan be defined that work in appropriately specialized ways on a family of\nrelated classes. _Objects_ are instances of classes and hold the data that is\nbeing manipulated.\n\nThis course does not cover OCaml's sophisticated module system, which can do\nmany of the same things as classes. You will learn all about objects when you\nstudy Java.\n"},{"cell_type":"markdown","metadata":{},"source":"## Why Program in ML?\n\n\n\n* Why Program in ML?\n* It is interactive.\n* It has a flexible notion of _data type_.\n* It hides the underlying hardware: _no crashes_.\n* Programs can easily be understood mathematically.\n* It distinguishes naming something from _updating memory_.\n* It manages storage for us.\n\nStandard ML is the outcome of years of research into\nprogramming languages. It is unique, defined using a mathematical formalism (an\noperational semantics) that is both precise and comprehensible. Several\nsupported compilers are available, and thanks to the formal definition, there\nare remarkably few incompatibilities among them. _(TODO edit)_\n\nBecause of its connection to mathematics, ML programs can be designed and\nunderstood without thinking in detail about how the computer will run them.\nAlthough a program can abort, it cannot crash: it remains under the control of\nthe OCaml system. It still achieves respectable efficiency and provides\nlower-level primitives for those who need them. Most other languages allow\ndirect access to the underlying machine and even try to execute illegal\noperations, causing crashes.\n\nThe only way to learn programming is by writing and running programs. This web\nnotebook provides an interactive environment where you can modify the example\nfragments and see the results for yourself.  You should also consider\ninstalling OCaml on your own computer so that you try more advanced programs\nlocally.\n"},{"cell_type":"markdown","metadata":{},"source":"### A first session with OCaml\n\n\n"},{"cell_type":"code","metadata":{},"source":"let pi = 3.14159265358979","outputs":[],"execution_count":1},{"cell_type":"markdown","metadata":{},"source":"\nThe first line of this simple session is a _value declaration_. It makes the\nname `pi` stand for the floating point number `3.14159`. (Such names are called\n_identifiers_.)  OCaml echoes the name (`pi`) and type (`float`) of the\ndeclared identifier.\n"},{"cell_type":"code","metadata":{},"source":"pi *. 1.5 *. 1.5","outputs":[],"execution_count":2},{"cell_type":"markdown","metadata":{},"source":"\nThe second line computes the area of the circle with radius `1.5` using the\nformula $A = \\pi r^2$. We use `pi` as an abbreviation for `3.14159`.\nMultiplication is expressed using `*.`, which is called an _infix operator_\nbecause it is written between its two operands.\n\nOCaml replies with the computed value (about `7.07`) and its type (again `float`).\n"},{"cell_type":"code","metadata":{},"source":"let area r = pi *. r *. r","outputs":[],"execution_count":3},{"cell_type":"markdown","metadata":{},"source":"\nTo work abstractly, we should provide the service \"compute the area of a\ncircle,\" so that we no longer need to remember the formula. This sort of\nencapsulated computation is called a _function_. The third line declares the\nfunction `area`. Given any floating point number `r`, it returns another\nfloating point number computed using the `area` formula; note that the function\nhas type `float -> float`.\n"},{"cell_type":"code","metadata":{},"source":"area 2.0","outputs":[],"execution_count":4},{"cell_type":"markdown","metadata":{},"source":"\nThe fourth line calls the function `area` supplying `2.0` as the argument. A\ncircle of radius `2` has an area of about `12.6`. Note that brackets around a\nfunction argument are not necessary.\n\nThe function uses `pi` to stand for `3.14159`. Unlike what you may have seen in\nother programming languages, `pi` cannot be \"assigned to\" or otherwise updated.\nIts meaning within `area` will persist even if we issue a new `let` declaration\nfor `pi` afterwards.\n"},{"cell_type":"code","metadata":{},"source":"let rec npower x n =\n  if n = 0 then 1.0\n  else x *. npower x (n-1)","outputs":[],"execution_count":5},{"cell_type":"markdown","metadata":{},"source":"\nThe function `npower` raises its real argument `x` to the power `n`, a\nnon-negative integer. The function is _recursive_: it calls itself. This concept\nshould be familiar from mathematics, since exponentiation is defined by the\nrules shown above. You may also have seen recursion in the product rule for\ndifferentiation: $(u · v)′ = u · v′ + u′ · v.$.\n\nIn finding the derivative of $u.v$, we recursively find the derivatives of $u$\nand $v$, combining them to obtain the desired result. The recursion is\nmeaningful because it terminates: we reduce the problem to two smaller\nproblems, and this cannot go on forever. The ML programmer uses recursion\nheavily.  For $n>=0$, the equation $x^{n+1} = x * x^n$ yields an obvious\ncomputation:\n\n$$ x^3 = x\\times x^2 = x\\times x\\times x^1 = x\\times x\\times x\\times x^0 = x\\times x\\times x $$\n\nThe equation clearly holds even for negative $n$. However, the corresponding\ncomputation runs forever:\n\n$$ x^{-1} = x\\times x^{-2} = x\\times x\\times x^{-3}=\\cdots $$\n\nNote that the function `npower` contains both an integer constant (0) and a\nfloating point constant (1.0). The decimal point makes all the difference. The\nML system will notice and ascribe different meaning to each type of constant.\n"},{"cell_type":"code","metadata":{},"source":"let square x = x *. x;","outputs":[],"execution_count":6},{"cell_type":"markdown","metadata":{},"source":"\nNow for a tiresome but necessary aside. In most languages, the types of\narguments and results must always be specified. ML is unusual that it normally\ninfers the types itself. However, sometimes ML could use a hint; function\n`square` above has a type constraint to say its result is a float.\n\nML can still infer the type even if you don't specify them, but in some cases\nit will use a more inefficient function than a specialised one.  Some languages\nhave just one type of number, converting automatically between different\nformats; this is slow and could lead to unexpected rounding errors.  Type\nconstraints are allowed almost anywhere. We can put one on any occurrence of x\nin the function. We can constrain the function’s result:\n"},{"cell_type":"code","metadata":{},"source":"let square (x:float) = x *. x","outputs":[],"execution_count":7},{"cell_type":"code","metadata":{},"source":"let square x : float = x *. x","outputs":[],"execution_count":8},{"cell_type":"markdown","metadata":{},"source":"\nML treats the equality and comparison test specially. Expressions like `if x = y then ...`\nare fine provided `x` and `y` have the same type and equality testing is\npossible for that type. (We discuss equality further in a later lecture.)\nNote that `x <> y` is ML for `x  ̸= y`.\n\nA characteristic feature of the computer is its ability to test for conditions\nand act accordingly.  In the early days, a program might jump to a given\naddress depending on the sign of some number.  Later, John McCarthy defined\nthe _conditional expression_ to satisfy:\n\n$$if true then x else y = x$$\n$$if false then x else y = y$$\n\nML evaluates the expression $if B then E_1 else E_2$ by first evaluating $B$.\nIf the result is `true` then ML evaluates $E_1$ and otherwise $E_2$.  Only one\nof the two expressions $E_1$ and $E_2$ is evaluated!  If both were evaluated,\nthen recursive functions like `npower` above would run forever.\n\nThe _if-expression_ is governed by an expression of type `bool`, whose two\nvalues are `true` and `false`.  In modern programming languages, tests are not\nbuilt into \"conditional branch\" constructs but have an independent status.\nTests, or _Boolean expressions_, can be expressed using relational operators\nsuch as `<` and `=`. They can be combined using the Boolean operators for\nnegation (`not`), `and` (written as `&&`) and `or` (written as `||`).  New\nproperties can be declared as functions: here, to test whether an integer is\neven.\n\nFor large `n`, computing powers using $x^{n+1} = x\\times x^n$ is too slow to\nbe practical.  The equations above are much faster. Example:\n\n$$ 2^{12} = 4^6 = 16^3 = 16\\times 256^1 = 16\\times 256 = 4096. $$\n\nInstead of `n` multiplications, we need at most $2 lg n$ multiplications,\nwhere $lg n$ is the logarithm of $n$ to the base $2$.\n\nWe use the function `even`, declared previously, to test whether the\nexponent is even.  Integer division (`div`) truncates its result to an\ninteger: dividing $2n+1$ by 2 yields $n$.\n\nA recurrence is a useful computation rule only if it is bound to terminate.\nIf $n>0$ then $n$ is smaller than both $2n$ and $2n+1$.  After enough\nrecursive calls, the exponent will be reduced to $1$.  The equations also hold\nif $n\\leq0$, but the corresponding computation runs forever.\n\nOur reasoning assumes arithmetic to be _exact_. Fortunately, the calculation is\nwell-behaved using floating-point.\n\nTODO edit for OCaml. The negation of `x` is written `~x` rather than `-x`\nplease note.  Most languages use the same symbol for minus and subtraction,\nbut ML regards all operators, whether infix or not, as functions.  Subtraction\ntakes a pair of numbers, but minus takes a single number; they are distinct\nfunctions and must have distinct names.\n\nTODO edit for OCaml. Computer numbers have a finite range, which if exceeded gives rise to an\nOverflow error.  Some ML systems can represent integers of arbitrary size.\n\nIf integers and reals must be combined in a calculation, ML provides functions\nto convert between them:\n"},{"cell_type":"code","metadata":{},"source":"int_of_float ;;","outputs":[],"execution_count":9},{"cell_type":"code","metadata":{},"source":"int_of_float 3.14159 ;;","outputs":[],"execution_count":10},{"cell_type":"code","metadata":{},"source":"float_of_int ;;","outputs":[],"execution_count":11},{"cell_type":"code","metadata":{},"source":"float_of_int 3 ;;","outputs":[],"execution_count":12},{"cell_type":"markdown","metadata":{},"source":"\nML's libraries are organized using _modules_, so we many use compound\nidentifiers such as `Float.of_int` to refer to library functions.  In OCaml,\nlibrary units can also be loaded by commands such as `#require \"num\"`.  There\nare many thousands of library functions available in the OCaml ecosystem,\nincluding text-processing and operating systems functions in addition to the\nusual numerical ones.\n\nTODO summarise OCaml syntax.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 2: Recursion and Efficiency\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"### Expression Evaluation\n\n\n\nExpression evaluation concerns expressions and the values they return. This\nview of computation may seem to be too narrow. It is certainly far removed from\ncomputer hardware, but that can be seen as an advantage. For the traditional\nconcept of computing solutions to problems, expression evaluation is entirely\nadequate.\n\nStarting with $E_0$, the expression $E_i$ is reduced to $E_{i+1}$ until this\nprocess concludes with a value~$v$.  A _value_ is something like a number\nthat cannot be further reduced.\n\nWe write $E E'$ to say that $E$ is _reduced_ to $E'$.\nMathematically, they are equal: $E=E'$, but the computation goes from $E$ to\n$E'$ and never the other way around.\n\nComputers also interact with the outside world.  For a start, they need some\nmeans of accepting problems and delivering solutions.  Many computer systems\nmonitor and control industrial processes.  This role of computers is familiar\nnow, but was never envisaged in the early days. Computer pioneers focused on\nmathematical calculations.  Modelling interaction and control requires a notion\nof _states_ that can be observed and changed.  Then we can consider\nupdating the state by assigning to variables or performing input/output,\nfinally arriving at conventional programs as coded in C, for instance.\n\nFor now, we remain at the level of expressions, which is usually termed\n_functional programming_.\n\n"},{"cell_type":"markdown","metadata":{},"source":"### Summing the first n integers\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec nsum n =\n  if n = 0 then 0\n           else n + nsum (n-1)","outputs":[],"execution_count":13},{"cell_type":"markdown","metadata":{},"source":"\nThe function call `nsum n` computes the sum `1+...+nz` rather naively, hence the\ninitial `n` in its name.  The nesting of parentheses is not just an artifact of\nour notation; it indicates a real problem.  The function gathers up a\ncollection of numbers, but none of the additions can be performed until `nsum\n0` is reached.  Meanwhile, the computer must store the numbers in an internal\ndata structure, typically the _stack_.  For large `n`, say `nsum 10000`, the\ncomputation might fail due to stack overflow.\n\nWe all know that the additions can be performed as we go along.  How do we\nmake the computer do that?\n"},{"cell_type":"markdown","metadata":{},"source":"### Iteratively summing the first `n` integers\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec summing n total =\n  if n = 0 then total\n           else summing (n-1) (n + total)","outputs":[],"execution_count":14},{"cell_type":"markdown","metadata":{},"source":"\nFunction `summing` takes an additional argument: a running total.  If\n`n` is zero then it returns the running total; otherwise, `summing`\nadds to it and continues.  The recursive calls do not nest; the additions are\ndone immediately.\n\nA recursive function whose computation does not nest is called\n_iterative_ or _tail-recursive_. Many functions can be made iterative by\nintroducing an argument analogous to _total_, which is often called an\n_accumulator_.\n\nThe gain in efficiency is sometimes worthwhile and sometimes not.  The function\n`power` is not iterative because nesting occurs whenever the exponent is odd.\nAdding a third argument makes it iterative, but the change complicates the\nfunction and the gain in efficiency is minute; for 32-bit integers, the maximum\npossible nesting is 30 for the exponent $2^{31}-1$.\n\n\nTODO slide\n\nA [classic\nbook](https://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs)\nby Abelson and Sussman used _iterative_ to mean _tail-recursive_. It describes\nthe Lisp dialect known as Scheme. Iterative functions produce computations\nresembling those that can be done using while-loops in conventional languages.\n\nMany algorithms can be expressed naturally using recursion, but only awkwardly\nusing iteration. There is a story that Dijkstra sneaked recursion into Algol-60\nby inserting the words \"any other occurrence of the procedure name denotes\nexecution of the procedure\". By not using the word \"recursion\", he managed to\nslip this amendment past sceptical colleagues.\n\nObsession with tail recursion leads to a coding style in which functions\nhave many more arguments than necessary.  Write straightforward code first,\navoiding only gross inefficiency.  If the program turns out to be too slow,\ntools are available for pinpointing the cause.  Always remember KISS (Keep\nIt Simple, Stupid).\n\nI hope you have all noticed by now that the summation can be done even more\nefficiently using the arithmetic progression formula:\n\n$$ 1+\\cdots+n = n(n+1)/2 $$\n"},{"cell_type":"markdown","metadata":{},"source":"### Stupidly Summing the First `n` Integers\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec stupidSum n =\n  if n = 0 then 0\n           else n + (stupidSum (n-1) + stupidSum (n-1)) / 2","outputs":[],"execution_count":15},{"cell_type":"markdown","metadata":{},"source":"\nThe function calls itself $2^n$ times!  Bigger inputs mean higher costs---but\nwhat's the _growth rate_?\n\nNow let us consider how to estimate various costs associated with a program.\n_Asymptotic complexity_ refers to how costs---usually time or space---grow with\nincreasing inputs. Space complexity can never exceed time complexity, for it\ntakes time to do anything with the space.  Time complexity often greatly\nexceeds space complexity.\n\nThe function `stupidSum` calls itself twice in each recursive step.  This\nfunction is contrived, but many mathematical formulas refer to a particular\nquantity more than once.  In OCaml, we can create a local binding to a computed\nvalue using the _local declaration_ syntax:\n\nTODO make power a real function\n"},{"cell_type":"raw","metadata":{},"source":"# let y = power x 20 in\n  (f y) + (g x y)"},{"cell_type":"markdown","metadata":{},"source":"\nFast hardware does not make good algorithms unnecessary.  On the contrary,\nfaster hardware magnifies the superiority of better algorithms.  Typically, we\nwant to handle the largest inputs possible.  If we double our processing power,\nwhat do we gain?  How much can we increase $n$, the input to our function?\nWith `stupidSum`, we can only go from $n$ to $n+1$.  We are limited to this\nmodest increase because the function's running time is proportional to $2^n$.\nWith the function `npower` defined in the previous lecture, we can go from $n$\nto $2n$: we can handle problems twice as big.  With `power` we can do much\nbetter still, going from $n$ to $n^2$.\n\nTODO table\n\nThis table (excerpted from a 40-year-old book! TODO cite aho74) illustrates the\neffect of various time complexities.  The left-hand column indicates how many\nmilliseconds are required to process an input of size $n$.  The other entries\nshow the maximum size of $n$ that can be processed in the given time (one\nsecond, minute or hour).\n\nThe table illustrates how large an input can be processed as a function\nof time.  As we increase the computer time per input from one second to one\nminute and then to one hour, the size of the input increases accordingly.\n\nThe top two rows (complexities $n$ and $n \\lg n$) increase rapidly: for $n$, by\na factor of 60.  The bottom two start out close together, but $n^3$ (which\ngrows by a factor of 3.9) pulls well away from $2^n$ (whose growth is only\nadditive).  If an algorithm's complexity is exponential then it can never\nhandle large inputs, even if it is given huge resources.  On the other hand,\nsuppose the complexity has the form $n^c$, where $c$ is a constant.  (We say\nthe complexity is _polynomial_.)  Doubling the argument then increases the\ncost by a constant factor.  That is much better, though if $c>3$ the algorithm\nmay not be considered practical.\n\nThe cost of a program is usually a complicated formula.  Often we should\nconsider only the most significant term.  If the cost is $n^2 + 99n + 900$\nfor an input of size $n$, then the $n^2$ term will eventually dominate,\neven though $99n$ is bigger for $n<99$.\nThe constant term $900$ may look big, but it is soon dominated by $n^2$.\n\nConstant factors in costs can be ignored unless they are large.  For one thing,\nthey seldom make a difference: $100n^2$ will be better than $n^3$ in the long\nrun: or _asymptotically_ to use the jargon.  Moreover, constant factors are\nseldom stable.  They depend upon details such as which hardware, operating\nsystem or programming language is being used.  By ignoring constant factors, we\ncan make comparisons between algorithms that remain valid in a broad range of\ncircumstances.\n\nThe \"Big O\" notation is commonly used to describe efficiency---to be precise,\n_asymptotic complexity_.  It concerns the limit of a function as its\nargument tends to infinity.  It is an abstraction that meets the informal\ncriteria that we have just discussed.\nIn the definition, _sufficiently large_ means there is some constant $n_0$\nsuch that $|f(n)|\\leq c|g(n)|$ for all $n$ greater than $n_0$.  The\nrole of $n_0$ is to ignore finitely many exceptions to the bound, such as the\ncases when $99n$ exceeds~$n^2$.\n\nTODO onotation slide\n\n$O$ notation lets us reason about the costs of algorithms easily.\n- Constant factors such as the $2$ in $O(2g(n))$ drop out: we can use $O(g(n))$ with twice the value of~$c$ in the definition.\n- Because constant factors drop out, the base of logarithms is irrelevant.\n- Insignificant terms drop out.  To see that $O(n^2+50n+36)$ is the same as $O(n^2)$, consider that $n^2+50n+36/n^2$ converges to 1 for increasing $n$.  % In fact, $n^2+50n+36 \\le 2n^2$ for $n\\ge 51$, so can double the constant factor\n\nIf $c$ and $d$ are constants (that is, they are independent of $n$) with $0 < c < d$ then\n- $O(n^c)$ is contained in $O(n^d)$\n- $O(c^n)$ is contained in $O(d^n)$\n- $O(\\log n)$ is contained $in O(n^c)$\n\nTo say that $O(c^n)$ _is contained in_ $O(d^n)$ means that the former gives\na tighter bound than the latter.  For example, if $f(n)=O(2^n)$ then\n$f(n)=O(3^n)$ trivially, but the converse does not hold.\n"},{"cell_type":"markdown","metadata":{},"source":"### Common Complexity Classes\n\n\n\n- $O(1)$ is _constant_\n- $O(\\log n)$ is _logarithmic_\n- $O(n)$ is _linear_\n- $O(n\\log n)$ is _quasi-linear_\n- $O(n^2)$ is _quadratic_\n- $O(n^3)$ is _cubic_\n- $O(a^n)$ is _exponential_ (for fixed $a$)\n\nLogarithms grow very slowly, so $O(\\log n)$ complexity is excellent.  Because\n$O$ notation ignores constant factors, the base of the logarithm is\nirrelevant!\n\nUnder linear we might mention $O(n\\log n)$, which occasionally is called\n_quasilinear_ and which scales up well for large $n$.\n\nAn example of quadratic complexity is matrix addition: forming the sum of two\n$n\\times n$ matrices obviously takes $n^2$ additions.  Matrix\nmultiplication is of cubic complexity, which limits the size of matrices that\nwe can multiply in reasonable time.  An $O(n^{2.81})$ algorithm exists, but it\nis too complicated to be of much use, even though it is theoretically better.\n\nAn exponential growth rate such as $2^n$ restricts us to small values of~$n$.\nAlready with $n=20$ the cost exceeds one million.  However, the worst case\nmight not arise in normal practice.  OCaml type-checking is exponential in the\nworst case, but not for ordinary programs.\n\n### Sample costs in O notation\n\n| Function     | Time | Space |\n| ------------ | ---- | ----- |\n|  npower, nsum  | $O(n)$ | $O(n)$ |\n|  summing       | $O(n)$ | $O(1)$ |\n| $n(n+1)/2$ | $O(1)$ | $O(1)$ |\n|  power  | $O(\\log n)$ | $O(\\log n)$ |\n|  stupidSum  | $O(2^n)$ | $O(n)$ |\n\nRecall that `npower` computes $x^n$\nby repeated multiplication while `nsum` naively computes the sum\n$1+\\cdots+n$.  Each obviously performs $O(n)$ arithmetic operations.  Because\nthey are not tail recursive, their use of space is also $O(n)$.  The function\n`summing` is a version of `nsum` with an accumulating argument;\nits iterative behaviour lets it work in constant space.  $O$ notation spares\nus from having to specify the units used to measure space.\n\nEven ignoring constant factors, the units chosen can influence the result.\nMultiplication may be regarded as a single unit of cost.  However, the cost of\nmultiplying two $n$-digit numbers for large $n$ is itself an important\nquestion, especially now that public-key cryptography uses numbers hundreds of\ndigits long.\n\nFew things can _really_ be done in constant time or stored in constant\nspace.  Merely to store the number $n$ requires $O(\\log n)$ bits.  If a\nprogram cost is $O(1)$, then we have probably assumed that certain operations\nit performs are also $O(1)$---typically because we expect never to exceed the\ncapacity of the standard hardware arithmetic.\n\nWith `power`, the precise number of operations depends upon $n$ in a\ncomplicated way, depending on how many odd numbers arise, so it is convenient\nthat we can just write $O(\\log n)$.  An accumulating argument could reduce its\nspace cost to $O(1)$.\n\n### Some Simple Recurrence Relations\n\n\nConsider $T(n)$ has a cost we want to bound using $O$ notation.\nA typical _base case_ is $T(1)=1$.  Some _recurrences_ are:\n\n| Equation            | Complexity   |\n| ------------------- | ------------ |\n| $T(n+1) = T(n)+1$  | $O(n)$       |\n| $T(n+1) = T(n)+n$  | $O(n^2)$     |\n| $T(n) = T(n/2)+1$  | $O(\\log n)$  |\n| $T(n) = 2T(n/2)+n$ | $O(n\\log n)$ |\n\nTo analyse a function, inspect its OCaml declaration.  Recurrence equations for\nthe cost function $T(n)$ can usually be read off.  Since we ignore constant\nfactors, we can give the base case a cost of one unit.  Constant work done in\nthe recursive step can also be given unit cost; since we only need an upper\nbound, this unit represents the larger of the two actual costs.  We could use\nother constants if it simplifies the algebra.\n\nFor example, recall our function `nsum`:\n"},{"cell_type":"code","metadata":{},"source":"let rec nsum n =\n  if n = 0 then \n    0\n  else\n    n + nsum (n-1)","outputs":[],"execution_count":16},{"cell_type":"markdown","metadata":{},"source":"\nGiven $n+1$, it performs a constant amount of work (an addition and\nsubtraction) and calls itself recursively with argument $n$.  We get the\nrecurrence equations $T(0)=1$ and $T(n+1) = T(n)+1$.  The closed form is\nclearly $T(n)=n+1$, as we can easily verify by substitution.  The cost is\n_linear_.\n\nThis function, given $n+1$, calls `nsum`, performing $O(n)$ work.\nAgain ignoring constant factors, we can say that this call takes exactly $n$\nunits.\n"},{"cell_type":"code","metadata":{},"source":"let rec nsumsum n =\n  if n = 0 then\n    0\n  else\n    nsum n + nsumsum (n-1)","outputs":[],"execution_count":17},{"cell_type":"markdown","metadata":{},"source":"\nWe get the recurrence equations $T(0)=1$ and $T(n+1) = T(n)+n$.  It is easy to\nsee that $T(n)=(n-1)+\\cdots+1=n(n-1)/2=O(n^2)$.  The cost is\n_quadratic_.\n\nThe function `power` divides its input $n$ into two, with\nthe recurrence equation $T(n) = T(n/2)+1$.  Clearly $T(2^n)=n+1$, so\n$T(n)=O(\\log n)$.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 3: Lists\n\n\n"},{"cell_type":"code","metadata":{},"source":"let x = [3; 5; 9] ;;","outputs":[],"execution_count":18},{"cell_type":"code","metadata":{},"source":"let y = [ (1,\"one\"); (2,\"two\") ] ;;","outputs":[],"execution_count":19},{"cell_type":"markdown","metadata":{},"source":"\nA _list_ is an ordered series of elements; repetitions are significant.\nSo `[3;5;9]` differs from `[5;3;9]` and from `[3;3;5;9]`.  Elements in the\nlist are separated with `;` when constructed, as opposed to the `,` syntax\nused for fixed-length tuples.\n\nAll elements of a list must have the same type.  Above we see a list of\nintegers and a list of `(integer, string)` pairs.  One can also have lists of\nlists, such as `[[3]; []; [5,6]]`, which has type `int list list`.\n\nIn the general case, if $x_1$, \\ldots, $x_n$ all have the same type (say\n$\\tau$) then the list $[x_1,\\ldots,x_n]$ has type $(\\tau)\\texttt{list}$.\n\nLists are the simplest data structure that can be used to process collections\nof items.  Conventional languages use _arrays_, whose elements are\naccessed using subscripting: for example, $A[i]$ yields the $i$th element of\nthe array~$A$.  Subscripting errors are a known cause of programmer grief,\nhowever, so arrays should be replaced by higher-level data structures whenever\npossible.\n"},{"cell_type":"code","metadata":{},"source":"x @ [2; 10] ;;","outputs":[],"execution_count":20},{"cell_type":"code","metadata":{},"source":"List.rev [ (1,\"one\"); (2,\"two\") ] ;;","outputs":[],"execution_count":21},{"cell_type":"markdown","metadata":{},"source":"\nThe infix operator `@` (also called `List.append`) concatenates two lists.\nAlso built-in is `List.rev`, which reverses a list.  These are demonstrated\nin the session above.\n"},{"cell_type":"markdown","metadata":{},"source":"### The List Primitives\n\n\n\nThere are two kinds of lists:\n- `[]` represents the empty list\n- `x :: l` is the list with head $x$ and tail $l$\n"},{"cell_type":"code","metadata":{},"source":"let nil = [] ;;","outputs":[],"execution_count":22},{"cell_type":"code","metadata":{},"source":"1 :: nil ;;","outputs":[],"execution_count":23},{"cell_type":"code","metadata":{},"source":"1 :: 2 :: nil ;;","outputs":[],"execution_count":24},{"cell_type":"markdown","metadata":{},"source":"\nThe operator `::` (also called `List.cons` for \"construct\"), puts a new element on\nto the head of an existing list.  While we should not be too preoccupied with\nimplementation details, it is essential to know that `::` is an $O(1)$\noperation.  It uses constant time and space, regardless of the length of the\nresulting list.  Lists are represented internally with a linked structure;\nadding a new element to a list merely hooks the new element to the front of\nthe existing structure.  Moreover, that structure continues to denote the same\nlist as it did before; to see the new list, one must look at the new `::` node\n(or _cons cell_) just created.\n\n\nHere we see the element~1 being consed to the front of the list `[3;5;9]`:\n\n$$\n\\let\\down=\\downarrow\n\\begin{array}{*{10}{c@{\\,}}c}\n:: & \\to & \\cdots & :: & \\to &  :: & \\to &  :: & \\to & nil \\\\\n\\down &  &        & \\down &  & \\down &  & \\down  \\\\\n1     &  &        & 3     &  & 5     &  & 9\n\\end{array}\n$$\n\nGiven a list, taking its first element (its _head_) or its list of\nremaining elements (its _tail_) also takes constant time.  Each\noperation just follows a link.  In the diagram above, the first down arrow\nleads to the head and the leftmost right arrow leads to the tail.  Once we\nhave the tail, its head is the second element of the original list, etc.\n\nThe tail is _not_ the last element; it is the _list_ of all elements\nother than the head!\n"},{"cell_type":"markdown","metadata":{},"source":"### Getting at the Head and Tail\n\n\n"},{"cell_type":"code","metadata":{},"source":"let null = function\n  | [] -> true\n  | x :: l -> false ;;","outputs":[],"execution_count":25},{"cell_type":"code","metadata":{},"source":"null [] ;;","outputs":[],"execution_count":26},{"cell_type":"code","metadata":{},"source":"null [1;2;3] ;;","outputs":[],"execution_count":27},{"cell_type":"code","metadata":{},"source":"let hd (x::l) = x ;;","outputs":[],"execution_count":28},{"cell_type":"code","metadata":{},"source":"hd [1;2;3] ;;","outputs":[],"execution_count":29},{"cell_type":"code","metadata":{},"source":"let tl (x::l) = l ;;","outputs":[],"execution_count":30},{"cell_type":"code","metadata":{},"source":"tl [7;6;5] ;;","outputs":[],"execution_count":31},{"cell_type":"markdown","metadata":{},"source":"\nThe empty list has neither head nor tail.  Applying `List.hd` `List.tl` to `[]`\nis an error---strictly speaking, an _exception_.  The function `null` can\nbe used to check for the empty list beforehand.  Taking a list apart using\ncombinations of `hd` and `tl` is hard to get right.  Fortunately, it is seldom\nnecessary because of _pattern-matching_.\n\nThe declaration of `null`} above has two clauses: one for the empty list (for\nwhich it returns `true`) and one for non-empty lists (for which it returns\n`false`).\n\nThe declaration of `null` above has two clauses: one for the empty list\n(for which it returns `true`) and one for non-empty lists (for which it\nreturns `false`).\n\nThe declaration of `hd` above has only one clause, for non-empty lists.  They\nhave the form `x::l` and the function returns `x`, which is the head.  OCaml\nprints a warning to tell us that calling the function could raise an exception\ndue to all possible inputs not being handles, including a counter-example (in\nthis case, the empty list `[]`). The declaration of `tl` is similar to `hd`.\n\nThese three primitive functions are _polymorphic_ and allow flexibility in the\ntypes of their arguments and results. Note their types!\n"},{"cell_type":"code","metadata":{},"source":"null ;;","outputs":[],"execution_count":32},{"cell_type":"code","metadata":{},"source":"hd ;;","outputs":[],"execution_count":33},{"cell_type":"code","metadata":{},"source":"tl ;;","outputs":[],"execution_count":34},{"cell_type":"markdown","metadata":{},"source":"\nSymbols `'a` and `'b` are called _type variables_ and stand for any types. Code\nwritten using these functions is checked for type correctness at compile time.\nAnd this guarantees strong properties at run time, for example that the\nelements of any list all have the same type.\n"},{"cell_type":"markdown","metadata":{},"source":"### Computing the Length of a List\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec nlength = function\n | [] -> 0\n | x :: xs -> 1 + nlength xs ;;","outputs":[],"execution_count":35},{"cell_type":"code","metadata":{},"source":"nlength [] ;;","outputs":[],"execution_count":36},{"cell_type":"code","metadata":{},"source":"nlength [5; 6; 7] ;;","outputs":[],"execution_count":37},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nnlength[a,b,c] \\Rightarrow & 1 + nlength[b,c] \\\\\n   \\Rightarrow & 1 + (1 + nlength[c]) \\\\\n   \\Rightarrow & 1 + (1 + (1 + nlength[])) \\\\\n   \\Rightarrow & 1 + (1 + (1 + 0)) \\\\\n   \\Rightarrow & \\ldots \\;\\; 3\n\\end{align*}\n$$\n\nMost list processing involves recursion.  This is a simple example; patterns\ncan be more complex.  Observe the use of a vertical bar `|` to separate the function's\nclauses.  We have _one_ function declaration that handles two cases.\nTo understand its role, consider the following faulty code:\n"},{"cell_type":"code","metadata":{},"source":"let rec nlength [] = 0 ;;","outputs":[],"execution_count":38},{"cell_type":"code","metadata":{},"source":"let rec nlength (x::xs) = 1 + nlength xs ;;","outputs":[],"execution_count":39},{"cell_type":"markdown","metadata":{},"source":"\nThese are two declarations, not one.  First we declare `nlength` to be a\nfunction that handles only empty lists.  Then we redeclare it to be a function\nthat handles only non-empty lists; it can never deliver a result.  We see that\na second `let` declaration replaces any previous one rather than extending it\nto cover new cases.\n\nNow, let us return to the declaration shown on the slide.  The length function\nis _polymorphic_ and applies to _all_ lists regardless of element\ntype!  Most programming languages lack such flexibility.\n\nUnfortunately, this length computation is naive and wasteful.  Like\n`nsum` earlier, it is not tail-recursive.  It\nuses $O(n)$ space, where $n$ is the length of its input.  As usual, the\nsolution is to add an accumulating argument.\n"},{"cell_type":"markdown","metadata":{},"source":"### Efficiently Computing the Length of a List\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec addlen = function\n  | (n, []) -> n\n  | (n, x::xs) -> addlen (n+1, xs) ;;","outputs":[],"execution_count":40},{"cell_type":"code","metadata":{},"source":"addlen (0, [5;6;7]) ;;","outputs":[],"execution_count":41},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\naddlen(0, [a,b,c]) \\Rightarrow &  addlen(1, [b,c]) \\\\\n  \\Rightarrow  & addlen(2, [c]) \\\\\n  \\Rightarrow  & addlen(3, []) \\\\\n  \\Rightarrow  & 3\n\\end{align*}\n$$\n\nPatterns can be as complicated as we like.  Here, the two patterns are\n`(n,[])` and `(n,x::xs)`.\n\nFunction `addlen` is again polymorphic.  Its type mentions the integer\naccumulator.\n\nNow we may declare an efficient length function.  It is simply a wrapper for\n`addlen`, supplying zero as the initial value of $n$.\n"},{"cell_type":"code","metadata":{},"source":"let length xs = addlen (0, xs) ;;","outputs":[],"execution_count":42},{"cell_type":"code","metadata":{},"source":"length [5;6;7;8] ;;","outputs":[],"execution_count":43},{"cell_type":"markdown","metadata":{},"source":"\nThe recursive calls do not nest: this version is iterative.  It takes $O(1)$\nspace.  Obviously its time requirement is $O(n)$ because it takes at least $n$\nsteps to find the length of an $n$-element list.\n"},{"cell_type":"markdown","metadata":{},"source":"### Append: List Concatenation\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec append = function\n  | ([], ys) -> ys\n  | (x::xs, ys) -> x :: append (xs,ys) ;;","outputs":[],"execution_count":44},{"cell_type":"code","metadata":{},"source":"append ([1;2;3], [4]) ;;","outputs":[],"execution_count":45},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nappend([1;2;3], [4]) \\Rightarrow & 1 :: append([2;3], [4]) \\\\\n  \\Rightarrow & 1 :: (2 :: append([3], [4])) \\\\\n  \\Rightarrow & 1 :: (2 :: (3 :: append([], [4]))) \\\\\n  \\Rightarrow & 1 :: (2 :: (3 :: [4])) \\;; [1;2;3;4]\n\\end{align*}\n$$\n\nHere is how append might be declared, ignoring the details of how `@` is made\nan infix operator.  This function is also not iterative.  It scans its first\nargument, sets up a string of `cons' operations (`::`) and finally does them.\n\nIt uses $O(n)$ space and time, where $n$ is the length of its first argument.\n_Its costs are independent of its second argument._\n\nAn accumulating argument could make it iterative, but with considerable\ncomplication.  The iterative version would still require $O(n)$ space and time\nbecause concatenation requires copying all the elements of the first list.\nTherefore, we cannot hope for asymptotic gains; at best we can decrease the\nconstant factor involved in $O(n)$, but complicating the code is likely to\nincrease that factor.  Never add an accumulator merely out of habit.\n\nNote append's polymorphic type. It tells us that two lists can be joined if\ntheir element types agree.\n"},{"cell_type":"markdown","metadata":{},"source":"### Reversing a List in O(n^2)\n\n\n\nnrev \n"},{"cell_type":"code","metadata":{},"source":"let rec nrev = function\n  | [] -> []\n  | x::xs -> (nrev xs) @ [x] ;;","outputs":[],"execution_count":46},{"cell_type":"code","metadata":{},"source":"nrev [1;2;3] ;;","outputs":[],"execution_count":47},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nnrev[a;b;c] \\Rightarrow & nrev[b,c] @ [a] \\\\\n  \\Rightarrow &  (nrev[c] @ [b]) @ [a] \\\\\n  \\Rightarrow &  ((nrev[] @ [c]) @ [b]) @ [a] \\\\\n  \\Rightarrow &  (([] @ [c]) @ [b]) @ [a] \\;; \\ldots \\;; [c,b,a]\n\\end{align*}\n$$\n\nThis reverse function is grossly inefficient due to poor usage of append,\nwhich copies its first argument.  If `nrev` is given a list of length\n$n>0$, then append makes $n-1$ conses to copy the reversed tail.  Constructing\nthe list `[x]` calls `cons` again, for a total of $n$ calls.  Reversing\nthe tail requires $n-1$ more conses, and so forth.  The total number of conses\nis:\n\n$$ 0 + 1 + 2 + \\cdots + n = {n(n+1)/2} $$\n\nThe time complexity is therefore $O(n^2)$.  Space complexity is only $O(n)$\nbecause the copies don't all exist at the same time.\n"},{"cell_type":"markdown","metadata":{},"source":"### Reversing a List in O(n)\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec revApp = function\n  | ([], ys) -> ys\n  | (x::xs, ys) -> revApp (xs, x::ys)","outputs":[],"execution_count":48},{"cell_type":"markdown","metadata":{},"source":"\n$$\n\\begin{align*}\nrevApp([a;b;c], []) \\Rightarrow & revApp([b,c], [a]) \\\\\n  \\Rightarrow & revApp([c], [b;a]) \\\\\n  \\Rightarrow & revApp([], [c;b;a]) \\\\\n  \\Rightarrow & [c;b;a]\n\\end{align*}\n$$\n\nCalling `revApp (xs,ys)` reverses the elements of `xs` and\nprepends them to `ys`.  Now we may declare\n"},{"cell_type":"code","metadata":{},"source":"let rev xs = revApp (xs, []) ;;","outputs":[],"execution_count":49},{"cell_type":"code","metadata":{},"source":"rev [1;2;3] ;;","outputs":[],"execution_count":50},{"cell_type":"markdown","metadata":{},"source":"\nIt is easy to see that this reverse function performs just $n$ conses, given\nan $n$-element list.  For both reverse functions, we could count the number of\nconses precisely---not just up to a constant factor.  $O$ notation is still\nuseful to describe the overall running time: the time taken by a cons\nvaries from one system to another.\n\nThe accumulator $y$ makes the function iterative.  But the gain in complexity\narises from the removal of `append`.  Replacing an expensive operation (append)\nby a series of cheap operations (cons) is called _reduction in strength_\nand is a common technique in computer science.  It originated when many\ncomputers did not have a hardware multiply instruction; the series of products\n$i\\times r$ for $i=0$, $\\ldots, n$ could more efficiently be computed by\nrepeated addition.  Reduction in strength can be done in various ways; we\nshall see many instances of removing append.\n\nConsing to an accumulator produces the result in reverse.  If\nthat forces the use of an extra list reversal then the iterative function\nmay be much slower than the recursive one.\n\nTODO strings\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 4: More on Lists\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"### List Utilities: take and drop\n\n\n\nRemoving the first $i$ elements of a list can be done as follows:\n"},{"cell_type":"code","metadata":{},"source":"let rec take = function\n  | ([], _) -> []\n  | (x::xs, i) ->\n      if i > 0 then\n        x :: take (xs, i-1)\n      else\n        [] ;;","outputs":[],"execution_count":51},{"cell_type":"code","metadata":{},"source":"let rec drop = function\n  | ([], _) -> []\n  | (x::xs, i) ->\n      if i > 0 then\n        drop (xs, i-1)\n      else\n        x::xs ;;","outputs":[],"execution_count":52},{"cell_type":"markdown","metadata":{},"source":"\nThis lecture examines more list utilities, illustrating more patterns of\nrecursion, and concludes with a small program for making change.\n\nThe functions `take` and `drop` divide a list\ninto parts, returning or discarding the first $i$ elements.\n\n$$\nxs = [\\underbrace{x_0,\\ldots,x_{i-1}}_{\\textstyle take(xs,i)},\n      \\underbrace{x_i,\\ldots,x_{n-1}}_{\\textstyle drop(xs,i)} ]\n$$\n\nApplications of `take` and `drop` will appear in future lectures.  Typically,\nthey divide a collection of items into equal parts for recursive processing.\n\nThe special pattern variable `_` appears in both functions.  This _wildcard\npattern_ matches anything.  We could have written `i` in both positions, but\nthe wildcard reminds us that the relevant clause ignores this argument.\n\nFunction `take` is not iterative, but making it so would not improve\nits efficiency.  The task requires copying up to $i$ list elements, which must\ntake $O(i)$ space and time.\n\nFunction `drop` simply skips over $i$ list elements.  This requires\n$O(i)$ time but only constant space.  It is iterative and much faster than\n\\texttt{take}.  Both functions use $O(i)$ time, but skipping elements is faster\nthan copying them:  \\texttt{drop}'s constant factor is smaller.\n\nBoth functions take a list and an integer, returning a list of the same type.\nSo their type is `'a list * int -> 'a list`.\n"},{"cell_type":"markdown","metadata":{},"source":"### Linear Search\n\n\n\nTODO slide\n\n_Linear search_ is the obvious way to find a desired item in a\ncollection: simply look through all the items, one at a time.  If $x$ is in\nthe list, then it will be found in $n/2$ steps on average, and even the worst\ncase is obviously $O(n)$.\n\nLarge collections of data are usually ordered or indexed so that items can be\nfound in $O(\\log n)$ time, which is exponentially better than $O(n)$.  Even\n$O(1)$ is achievable (using a _hash table_), though subject to the usual\nproviso that machine limits are not exceeded.\n\nEfficient indexing methods are of prime importance: consider Web\nsearch engines.  Nevertheless, linear search is often used to search small\ncollections because it is so simple and general, and it is the starting point\nfor better algorithms.\n"},{"cell_type":"markdown","metadata":{},"source":"### Equality Tests\n\n\n\nTODO describe OCaml runtime equality tests\n"},{"cell_type":"markdown","metadata":{},"source":"### Building a List of Pairs\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec zip = function\n  | (x::xs, y::ys) -> (x,y) :: zip (xs,ys)\n  | _ -> [] ;;","outputs":[],"execution_count":53},{"cell_type":"markdown","metadata":{},"source":"\n$$ \\left.[x_1,\\ldots,x_n]\\atop\n         [y_1,\\ldots,y_n]\\right\\}\\;\\longmapsto\\;[(x_1,y_1),\\ldots,(x_n,y_n)]\n$$\n\nThe _wildcard_ pattern `_` matches _anything_. The patterns are also tested\nin order of their definitions.\n\nA list of pairs of the form $[(x_1,y_1),\\ldots,(x_n,y_n)]$ associates each\n$x_i$ with $y_i$.  Conceptually, a telephone directory could be regarded as\nsuch a list, where $x_i$ ranges over names and $y_i$ over the corresponding\ntelephone number.  Linear search in such a list can find the $y_i$ associated\nwith a given $x_i$, or vice versa---very slowly.\n\nIn other cases, the $(x_i,y_i)$ pairs might have been generated by applying a\nfunction to the elements of another list $[z_1,\\ldots,z_n]$.\n"},{"cell_type":"code","metadata":{},"source":"let rec unzip = function\n | [] -> ([], [])\n | (x,y)::pairs ->\n     let xs,ys = unzip pairs in\n     (x::xs, y::ys) ;;","outputs":[],"execution_count":54},{"cell_type":"markdown","metadata":{},"source":"\nThe functions `zip` and `unzip` build and take apart lists of\npairs: `zip` pairs up corresponding list elements and `unzip`\ninverts this operation.  Their types reflect what they do:\n"},{"cell_type":"code","metadata":{},"source":"zip ;;","outputs":[],"execution_count":55},{"cell_type":"code","metadata":{},"source":"unzip ;;","outputs":[],"execution_count":56},{"cell_type":"markdown","metadata":{},"source":"\nIf the lists are of unequal length, `zip` discards surplus items at the\nend of the longer list.  Its first pattern only matches a pair of non-empty\nlists.  The second pattern is just a wildcard and could match anything.  ML\ntries the clauses in the order given, so the first pattern is tried first.\nThe second only gets arguments where at least one of the lists is empty.\n"},{"cell_type":"markdown","metadata":{},"source":"### Building a Pair of Results\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec unzip = function\n | [] -> ([], [])\n | (x,y)::pairs ->\n     let xs,ys = unzip pairs in\n     (x::xs, y::ys) ;;","outputs":[],"execution_count":57},{"cell_type":"code","metadata":{},"source":"let rec revUnzip = function\n  | ([], xs, ys) -> (xs, ys)\n  | ((x,y)::pairs, xs, ys) ->\n      revUnzip (pairs, x::xs, y::ys) ;;","outputs":[],"execution_count":58},{"cell_type":"markdown","metadata":{},"source":"\nGiven a list of pairs, `unzip` has to build _two_ lists of\nresults, which is awkward using recursion.  The version shown about uses the\n_local declaration_ `let D in E`,\nwhere $D$ consists of declarations and $E$ is the expression that can use\nthem. The let-construct counts as an expression and can be used\n(perhaps wrapped within parentheses) wherever an expression is expected.\n\nNote especially the declaration `let xs,ys = unzip pairs`\nwhich binds `xs` and `ys` to the results of the recursive call.\nIn general, the declaration `let P = E` matches the\npattern $P$ against the value of expression $E$.  It binds all the variables\nin $P$ to the corresponding values.\n\nHere is a version of `unzip` that replaces the local declaration by a\nfunction `conspair` for taking apart the pair of lists in the\nrecursive call.  It defines the same\ncomputation as the previous version of `unzip` and is possibly clearer,\nbut not every local declaration can be eliminated as easily.\n"},{"cell_type":"code","metadata":{},"source":"let conspair ((x,y), (xs,ys)) = (x::xs, y::ys) ;;","outputs":[],"execution_count":59},{"cell_type":"code","metadata":{},"source":"let rec unzip = function\n  | [] -> ([], [])\n  | xy :: pairs -> conspair (xy, unzip pairs) ;;","outputs":[],"execution_count":60},{"cell_type":"markdown","metadata":{},"source":"\nMaking the function iterative yields `revUnzip` above, which is\nvery simple.  Iteration can construct many results at once in different\nargument positions.  Both output lists are built in reverse order, which can\nbe corrected by reversing the input to `revUnzip`.  The total costs\nwill probably exceed those of `unzip` despite the advantages of\niteration.\n"},{"cell_type":"markdown","metadata":{},"source":"### An Application: Making Change\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec change = function\n  | till, 0 -> []\n  | c::till, amt ->\n      if amt < c then\n        change (till, amt)\n      else\n        c :: change (c::till, amt-c) ;;","outputs":[],"execution_count":61},{"cell_type":"markdown","metadata":{},"source":"\n- The recursion _terminates_ when `amt = 0`.\n- Tries the _largest coin first_ to use large coins.\n- The algorithm is _greedy_ and can fail!\n\nThe till has unlimited supplies of coins.  The largest coins should be tried\nfirst, to avoid giving change all in pennies.  The list of legal coin values,\ncalled `till`, is given in descending order, such as 50, 20, 10, 5,\n2 and 1.  (Recall that the head of a list is the element most easily reached.)\nThe code for `change` is based on simple observations:\n- Change for zero consists of no coins at all.  (Note the pattern of `0` in the first clause.)\n- For a nonzero amount, try the largest available coin.  If it is small enough, use it and decrease the amount accordingly.\n- Exclude from consideration any coins that are too large.\n\nAlthough nobody considers making change for zero, this is the simplest way to\nmake the algorithm terminate.  Most iterative procedures become simplest if,\nin their base case, they do nothing.  A base case of one instead of zero is\noften a sign of a novice programmer.\n\nThe function can terminate either with success or failure.  It fails by\nraising exception `Match_failure`, signifying that no pattern matches,\nnamely if `till` becomes empty while `amt` is still nonzero.\n(Exceptions will be discussed later.)\n\nUnfortunately, failure can occur even when change can be made.  The greedy\n`largest coin first' approach is to blame.  Suppose we have coins of values 5\nand 2, and must make change for 6; the only way is $6=2+2+2$, ignoring the 5.\n_Greedy algorithms_ are often effective, but not here.\n"},{"cell_type":"markdown","metadata":{},"source":"### All Ways of Making Change\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec change = function\n  | (till, 0) -> [ [] ]\n  | ([], amt) -> []\n  | (c::till, amt) ->\n      if amt < c then\n        change (till, amt)\n      else\n        let rec allc = function\n          | [] -> []\n          | cs :: css -> (c::cs) :: allc css\n        in\n        allc (change (c::till, amt-c)) @\n        change (till, amt)\n ;;","outputs":[],"execution_count":62},{"cell_type":"markdown","metadata":{},"source":"\nNow we generalize the problem to return the list of _all possible ways_ of making change.\nLook at the type: the result is now a list of lists.\n\nThe code will never raise exceptions.  It expresses failure by returning an\nempty list of solutions: it returns `[]` if the till is empty and the\namount is nonzero.\n\nIf the amount is zero, then there is only one way of making change;\nthe result should be `[[]]`.  This is success in the base case.\n\nIn nontrivial cases, there are two sources of solutions: to use a coin (if\npossible) and decrease the amount accordingly, or to remove the current coin\nvalue from consideration.\n\nThe function `allc` is declared locally in order to make use\nof `c`, the current coin.  It adds an extra `c` to all the\nsolutions returned by the recursive call to make change for `amt - c`.\n\nObserve the naming convention: `cs` is a list of coins, while\n`css` is a list of such lists.  The trailing `s' is suggestive of a\nplural.\n\nThis complicated program, and the even trickier one on the next slide, are\nincluded as challenges.  Are you enthusiastic enough to work them out?  We\nshall revisit the \"making change\" task later to illustrate exception-handling.\n"},{"cell_type":"markdown","metadata":{},"source":"### All Ways of Making Change --- Faster!\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec change = function\n  | till, 0, chg, chgs -> chg::chgs\n  | [], amt, chg, chgs -> chgs\n  | c::till, amt, chg, chgs ->\n      if amt < 0 then\n        chgs\n      else\n        change (c::till, amt-c, c::chg,\n                change (till, amt, chg, chgs))\n;;","outputs":[],"execution_count":63},{"cell_type":"markdown","metadata":{},"source":"We've added _another_ accumulating parameter!  Repeatedly improving simple code\nis called _stepwise refinement_.\n\nTwo extra arguments eliminate many `::` and append operations from the previous\nslide's change function.  The first, `chg`, accumulates the coins chosen so\nfar; one evaluation of c::chg} replaces many evaluations of \\texttt{allc}.  The\nsecond, `chgs`, accumulates the list of solutions so far; it avoids the need\nfor append.  This version runs several times faster than the previous one.\n\nMaking change is still extremely slow for an obvious reason: the number of\nsolutions grows rapidly in the amount being changed.  Using 50, 20, 10, 5,\n2 and 1, there are 4366 ways of expressing 99.\n \nOur three change functions illustrate a basic technique: program development\nby stepwise refinement.  Begin by writing a very simple program and add\nrequirements individually.  Add efficiency refinements last of all.\nEven if the simpler program cannot be included in the next version and has\nto be discarded, one has learned about the task by writing it.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 5: Sorting\n\n\n\n### Sorting: Arranging Items into Order\n\nA few applications:\n- search\n- merging\n- duplicates\n- inverting tables\n- graphics algorithms\n\nSorting is perhaps the most deeply studied aspect of algorithm design.\nKnuth's series _The Art of Computer Programming_ devotes an entire\nvolume to sorting and searching!  Sedgewick (TODO cite)\nalso covers sorting.  Sorting has countless applications.\n\nSorting a collection allows items to be found quickly.  Recall that linear\nsearch requires $O(n)$ steps to search among $n$ items.  A sorted collection\nadmits _binary search_ which requires only $O(\\log n)$ time.  The idea\nof binary search is to compare the item being sought with the middle item (in\nposition $n/2$) and then to discard either the left half or the right,\ndepending on the result of the comparison.  Binary search needs arrays or\ntrees, not lists; we shall come to binary search trees later.\n\nTwo sorted files can quickly be _merged_ to form a larger sorted file.  Other\napplications include finding _duplicates_ that, after sorting, are adjacent.\n\nA telephone directory is sorted alphabetically by name.  The same information\ncan instead be sorted by telephone number (useful to the police) or by street\naddress (useful to junk-mail firms).  Sorting information in different ways\ngives it different applications.\n\nCommon sorting algorithms include insertion sort, quicksort,\nmergesort and heapsort.  We shall consider the first three of\nthese.  Each algorithm has its advantages.\n\nAs a concrete basis for comparison, runtimes are quoted for DECstation\ncomputers.  (These were based on the MIPS chip, an early RISC design.)\n(TODO add benchmarks)\n"},{"cell_type":"markdown","metadata":{},"source":"### How Fast Can We Sort?\n\n\n\n- typically count _comparisons_ $C(n)$\n- there are $n!$ permutations of $n$ elements\n- each comparison eliminates _half_ of the permutations $2^{C(n)}\\geq n!$\n- therefore $C(n)\\geq \\log(n!)\\approx n\\log n-1.44n$\n\nThe usual measure of efficiency for sorting algorithms is the number of\ncomparison operations required.  Mergesort requires only $O(n\\log n)$\ncomparisons to sort an input of $n$ items.  It is straightforward to prove\nthat this complexity is the best possible (TODO cite pages 86--7  aho74).  There\nare $n!$ permutations of $n$ elements and each comparison distinguishes two\npermutations.  The lower bound on the number of comparisons, $C(n)$, is\nobtained by solving $2^{C(n)}\\geq n!$; therefore $C(n)\\geq \\log(n!)\\approx\nn\\log n-1.44n$.\n\nIn order to compare the sorting algorithms, we use the following source of\npseudo-random numbers (TODO cite park88). Never mind how this works: generating\nstatistically good random numbers is hard.  Much effort has gone into those few\nlines of code.\n"},{"cell_type":"code","metadata":{},"source":"let nextrandom seed =\n  let a = 16807.0 in\n  let m = 2147483647.0 in\n  let t = a *. seed in\n  t -. m *. (floor (t /. m)) ;;","outputs":[],"execution_count":64},{"cell_type":"code","metadata":{},"source":"let rec randlist (seed,seeds) = function\n  | 0 -> (seed, seeds)\n  | n -> randlist (nextrandom seed, seed::seeds) (n-1) ;;","outputs":[],"execution_count":65},{"cell_type":"markdown","metadata":{},"source":"\nWe can now bind the identifier `rs` to a list of 10,000 random numbers.\n"},{"cell_type":"code","metadata":{},"source":"let seed, rs = randlist (1.0, []) 10000 ;;","outputs":[],"execution_count":66},{"cell_type":"markdown","metadata":{},"source":"\n### Insertion Sort\n\nAn insert does does $n/2$ comparisons on average.\n"},{"cell_type":"code","metadata":{},"source":"let rec ins = function\n  | x, [] -> [x]\n  | x, y::ys ->\n      if x <= y then\n        x :: y :: ys\n      else\n        y :: ins (x,ys)","outputs":[],"execution_count":67},{"cell_type":"markdown","metadata":{},"source":"\n_Insertion sort_ takes $O(n^2)$ comparisons on average:\n"},{"cell_type":"code","metadata":{},"source":"let rec insort = function\n    | [] -> []\n    | x::xs -> ins (x, insort xs)","outputs":[],"execution_count":68},{"cell_type":"markdown","metadata":{},"source":"\nItems from the input are copied one at a time to the output.  Each new item is\ninserted into the right place so that the output is always in order.\n\nWe could easily write iterative versions of these functions, but to no purpose.\nInsertion sort is slow because it does $O(n^2)$ comparisons (and a lot of list\ncopying), not because it is recursive.  Its quadratic runtime makes it nearly\nuseless: it takes 174 seconds for our example while the next-worst figure is\n1.4 seconds.\n\nInsertion sort is worth considering because it is easy to code and illustrates\nthe concepts.  Two efficient sorting algorithms, mergesort and heapsort, can be\nregarded as refinements of insertion sort.\n\nTODO:\n> The notion of\n> sorting depends upon the form of comparison being done, which in turn\n> determines the type of the sorting function.\n\n### Quicksort: The Idea\n\n- choose a _pivot_ element, $a$\n- Divide to partition the input into two sublists:\n  * those _at most_ $a$ in value\n  * those _exceeding_ $a$\n- Conquer using recursive calls to sort the sublists\n- Combine the sorted lists by appending one to the other\n\nQuicksort was invented by C. A. R. Hoare, who now works at Microsoft Research,\nCambridge.  Quicksort works by _divide and conquer_, a basic algorithm design\nprinciple.  Quicksort chooses from the input some value $a$, called the\n_pivot_.  It partitions the remaining items into two parts: those $\\leq a$, and\nthose $>a$.  It sorts each part recursively, then puts the smaller part before\nthe greater.\n\nThe cleverest feature of Hoare's algorithm was that the partition could be done\n_in place_ by exchanging array elements.  Quicksort was invented before\nrecursion was well known, and people found it extremely hard to understand.  As\nusual, we shall consider a list version based on functional programming.\n"},{"cell_type":"markdown","metadata":{},"source":"### Quicksort: The Code\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec quick = function\n  | [] -> []\n  | [x] -> [x]\n  | a::bs ->\n      let rec part = function\n        | (l,r,[]) -> (quick l) @ (a :: quick r)\n        | (l,r,x::xs) ->\n            if (x <= a) then\n              part (x::l, r, xs)\n            else\n              part (l, x::r, xs)\n      in\n      part ([], [], bs)","outputs":[],"execution_count":69},{"cell_type":"markdown","metadata":{},"source":"\nOur ML quicksort copies the items.  It is still pretty fast, and it is much\neasier to understand.  It takes roughly 0.74 seconds to sort our list of random\nnumbers.\n\nThe function declaration consists of three clauses.  The first handles the\nempty list; the second handles singleton lists (those of the form `[x]`; the\nthird handles lists of two or more elements.  Often, lists of length up to five\nor so are treated as special cases to boost speed.\n\nThe locally declared function `part` partitions the input using `a` as the\npivot.  The arguments `l` and `r` accumulate items for the left ($\\leq a$) and\nright ($>a$) parts of the input, respectively.\n\nIt is not hard to prove that quicksort does $n\\log n$ comparisons, _in the average case_\n(TODO cite page 94 aho74).  With random data, the pivot\nusually has an average value that divides the input in two approximately equal\nparts.  We have the recurrence $T(1) = 1$ and $T(n) = 2T(n/2)+n$, which is\n$O(n\\log n)$.  In our example, it is about 235 times faster than insertion\nsort.\n\nIn the worst case, quicksort's running time is quadratic!  An example is when\nits input is almost sorted or reverse sorted.  Nearly all of the items end up\nin one partition; work is not divided evenly.  We have the recurrence\n$T(1) = 1$ and $T(n+1) = T(n)+n$, which is $O(n^2)$.  Randomizing the input\nmakes the worst case highly unlikely.\n"},{"cell_type":"markdown","metadata":{},"source":"### Append-Free Quicksort\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec quik = function\n  | ([], sorted) -> sorted\n  | ([x], sorted) -> x::sorted\n  | a::bs, sorted ->\n     let rec part = function\n       | l, r, [] -> quik (l, a :: quik (r,sorted))\n       | l, r, x::xs ->\n           if x <= a then\n             part (x::l, r, xs)\n           else\n             part (l, x::r, xs)\n     in\n     part ([], [], bs)","outputs":[],"execution_count":70},{"cell_type":"markdown","metadata":{},"source":"\nThe list `sorted` accumulates the result in the _combine_ stage of\nthe quicksort algorithm.  We have again used the standard technique for\neliminating append.  Calling `quik(xs,sorted)` reverses the elements of\n`xs` and prepends them to the list `sorted`.\n\nLooking closely at `part`, observe that `quik(r,sorted)` is\nperformed first.  Then `a` is consed to this sorted list.  Finally,\n`quik` is called again to sort the elements of `l`.\n\nThe speedup is significant.  An imperative quicksort coded in Pascal (taken\nfrom Sedgewick (TODO cite sedgewick88) is just slightly faster than function\n`quik`.  The near-agreement is surprising because the computational overheads\nof lists exceed those of arrays.  In realistic applications, comparisons are\nthe dominant cost and the overheads matter even less.\n\n### Merging Two Lists\n\nMerge joins two sorted lists.\n"},{"cell_type":"code","metadata":{},"source":"let rec merge = function\n  | [], ys -> ys\n  | xs, [] -> xs\n  | x::xs, y::ys ->\n      if x <= y then\n        x :: merge (xs, y::ys)\n      else \n        y :: merge (x::xs, ys)","outputs":[],"execution_count":71},{"cell_type":"markdown","metadata":{},"source":"\nGeneralises insert to two lists, and does at most $m+n-1$ comparisons.\n\n_Merging_ means combining two sorted lists to form a larger sorted list.\nIt does at most $m+n$ comparisons, where $m$ and $n$ are the lengths of the\ninput lists.  If $m$ and $n$ are roughly equal then we have a fast way of\nconstructing sorted lists; if $n=1$ then merging degenerates to insertion,\ndoing much work for little gain.\n\nMerging is the basis of several sorting algorithms; we look at a\ndivide-and-conquer one.  Mergesort is seldom found in conventional programming\nbecause it is hard to code for arrays; it works nicely with lists.  It divides\nthe input (if non-trivial) into two roughly equal parts, sorts them\nrecursively, then merges them.\n\nFunction `merge` is not iterative; the recursion is deep.  An iterative\nversion is of little benefit for the same reasons that apply to\n`append` in the earlier lecture on Lists.\n"},{"cell_type":"markdown","metadata":{},"source":"### Top-down Merge sort\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec tmergesort = function\n  | [] -> []\n  | [x] -> [x]\n  | xs ->\n      let k = List.length xs / 2 in\n      let l = tmergesort (take (xs, k)) in\n      let r = tmergesort (drop (xs,k)) in\n      merge (l,r)","outputs":[],"execution_count":72},{"cell_type":"markdown","metadata":{},"source":"\n$O(n\\log n)$ comparisons in worst case\n\nMergesort's _divide_ stage divides the input not by choosing a pivot (as\nin quicksort) but by simply counting out half of the elements.  The\n_conquer_ stage again involves recursive calls, and the _combine_\nstage involves merging.  Function `tmergesort` takes roughly 1.4\nseconds to sort the list `rs`.\n\nIn the worst case, mergesort does $O(n\\log n)$ comparisons, with the same\nrecurrence equation as in quicksort's average case.  Because `take` and\n`drop` divide the input in two equal parts (they differ at most by\none element), we always have $T(n) = 2T(n/2)+n$.\n\nQuicksort is nearly 3 times as fast in the example.  But it risks a\nquadratic worst case!  Merge sort is safe but slow.  So which algorithm is\nbest?\n\nWe have seen a _top-down_ mergesort.  _Bottom-up_ algorithms also\nexist.  They start with a list of one-element lists and repeatedly merge\nadjacent lists until only one is left.  A refinement, which exploits any\ninitial order among the input, is to start with a list of increasing or\ndecreasing runs of input items.\n"},{"cell_type":"markdown","metadata":{},"source":"### Summary of Sorting Algorithms\n\n\n\n- Optimal is $O(n\\log n)$ comparisons\n- Insertion sort: simple to code; too slow (_quadratic_) [174 secs]\n- Quicksort: fast on average; _quadratic_ in worst case [0.53 secs]\n- Mergesort: optimal in theory; often slower than quicksort [1.4 secs]\n- _Match the algorithm to the application_\n\nQuicksort's worst case cannot be ignored.  For large $n$, a complexity of\n$O(n^2)$ is catastrophic.  Mergesort has an $O(n\\log n)$ worst case running\ntime, which is optimal, but it is typically slower than quicksort for random\ndata.\n\nNon-comparison sorting deserves mentioning.  We can sort a large number of\nsmall integers using their radix representation in $O(n)$ time.  This result\ndoes not contradict the comparison-counting argument because comparisons are\nnot used at all.  Linear time is achievable only if the greatest integer is\nfixed in advance; as $n$ goes to infinity, increasingly many of the items\nare the same.  It is a simple special case.\n\nMany other sorting algorithms exist. A few are outlined in the exercises.\n\n## Lecture 6: Datatypes and Trees\n"},{"cell_type":"markdown","metadata":{},"source":"### An Enumeration Type\n\n\n\n"},{"cell_type":"code","metadata":{},"source":"type vehicle =   Bike\n               | Motorbike\n               | Car\n               | Lorry","outputs":[],"execution_count":73},{"cell_type":"markdown","metadata":{},"source":"\n- We have declared a _new type_ named `vehicle`.\n- $\\ldots$ along with four new constants.\n- They are the \\emph{constructors} of the datatype.\n\nThe `type` declaration adds a new type to our OCalm session.  Type\n`vehicle` is as good as any built-in type and even admits\npattern-matching.  The four new identifiers of type `vehicle` are\ncalled _constructors_.\n\nWe could represent the various vehicles by the numbers 0--3.  However, the code would be\nhard to read and even harder to maintain.  Consider adding `Tricycle`\nas a new vehicle. If we wanted to add it before `Bike`, then all the\nnumbers would have to be changed.  Using `type`, such additions are\ntrivial and the compiler can (at least sometimes) warn us when it encounters a\nfunction declaration that doesn't yet have a case for `Tricycle`.\n\nRepresenting vehicles by strings like `\"Bike\"`, `\"Car\"`, etc.,\nis also bad.  Comparing string values is slow and the compiler\ncan't warn us of misspellings like `\"MOtorbike\"`: they will make our\ncode fail.\n\nMost programming languages allow the declaration of types like\n`vehicle`.  Because they consist of a series of identifiers, they are\ncalled _enumeration types_.  Other common examples are days of the week\nor colours.  The compiler chooses the integers for us; type-checking prevents\nus from confusing `Bike` with `Red` or `Sunday`.\n"},{"cell_type":"markdown","metadata":{},"source":"### Declaring a Function on Vehicles\n\n\n"},{"cell_type":"code","metadata":{},"source":"let wheels = function\n  | Bike -> 2\n  | Motorbike -> 2\n  | Car -> 4\n  | Lorry -> 18","outputs":[],"execution_count":74},{"cell_type":"markdown","metadata":{},"source":"\n- Datatype constructors can be used in patterns.\n- Pattern-matching is fast, even complicated nested patterns.\n\nThe beauty of datatype declarations is that the new types behave as if they\nwere built into OCaml. Type-checking catches common errors, such as mixing up\ndifferent datatypes in a function like `wheels`, as well as missing\nand redundant patterns.\n"},{"cell_type":"markdown","metadata":{},"source":"### A Datatype with Constructor Functions\n\n\n"},{"cell_type":"code","metadata":{},"source":"type vehicle =   Bike\n               | Motorbike of int\n               | Car       of bool\n               | Lorry     of int","outputs":[],"execution_count":75},{"cell_type":"markdown","metadata":{},"source":"\n- Constructor functions (like `Lorry`) make _distinct values_.\n- Different kinds of `vehicle` can belong to one list: `[Bike, Car true, Motorbike 450]`\n\nOCaml generalizes the notion of enumeration type to allow data to be associated\nwith each constructor.  The constructor `Bike` is a vehicle all by itself, but\nthe other three constructors are functions for creating vehicles.\n\nSince we might find it hard to remember what the various `int` and\n`bool` components are for, it is wise to include _comments_ in\ncomplex declarations.  In ML, comments are enclosed in the brackets\n`(*` and `*)`.  Programmers should comment their code to explain\ndesign decisions and key features of the algorithms (sometimes by citing a\nreference work).\n"},{"cell_type":"code","metadata":{},"source":"type vehicle =   Bike\n               | Motorbike of int  (* engine size in CCs *)\n               | Car       of bool (* true if a Reliant Robin *)\n               | Lorry     of int  (* number of wheels *)","outputs":[],"execution_count":76},{"cell_type":"markdown","metadata":{},"source":"The list shown on the slide represents a bicycle, a Reliant Robin and a large\nmotorbike.  It can be almost seen as a mixed-type list containing integers and\nbooleans.  It is actually a list of vehicles; datatypes lessen the impact of\nthe restriction that all list elements must have the same type.\n"},{"cell_type":"markdown","metadata":{},"source":"### A Finer Wheel Computation\n\n\n"},{"cell_type":"code","metadata":{},"source":"let wheels = function\n  | Bike -> 2\n  | Motorbike _ -> 2\n  | Car robin -> if robin then 3 else 4\n  | Lorry w -> w","outputs":[],"execution_count":77},{"cell_type":"markdown","metadata":{},"source":"\nThis function consists of four clauses:\n- A Bike has two wheels.\n- A Motorbike has two wheels.\n- A Reliant Robin has three wheels; all other cars have four.\n- A Lorry has the number of wheels stored with its constructor.\n\nThere is no overlap between the `Motorbike` and `Lorry` cases.  Although\n`Motorbike` and `Lorry` both hold an integer, ML takes the\nconstructor into account. A Motorbike is distinct from any Lorry.\n\nVehicles are one example of a concept consisting of several varieties with\ndistinct features.  Most programming languages can represent such concepts\nusing something analogous to datatypes.  (They are sometimes called\n_union types_ or _variant records_ whose _tag fields_ play the\nrole of the constructors.)\n\n\nA pattern may be built from the constructors of several datatypes, including lists. A pattern may also contain integer and string constants. There is no limit to the size of patterns or the number of clauses in a function declaration. Most ML systems perform pattern-matching efficiently.\n"},{"cell_type":"markdown","metadata":{},"source":"### Error Handling: Exceptions\n\n\n\nDuring a computation, what happens if something goes _wrong_?\n- Division by zero\n- Pattern matching failure\n\n_Exception-handling_ lets us recover gracefully.\n- Raising an exception abandons the current computation.\n- Handling the exception attempts an alternative computation.\n- The raising and handling can be far apart in the code.\n- Errors of _different sorts_ can be handled separately.\n\nExceptions are necessary because it is not always possible to tell in advance\nwhether or not a search will lead to a dead end or whether a numerical\ncalculation will encounter errors such as overflow or divide by zero. Rather\nthan just crashing, programs should check whether things have gone wrong, and\nperhaps attempt an alternative computation (perhaps using a different algorithm\nor higher precision). A number of modern languages provide exception handling.\n"},{"cell_type":"markdown","metadata":{},"source":"### Exceptions in ML\n\n\n"},{"cell_type":"code","metadata":{},"source":"exception Failure ;;","outputs":[],"execution_count":78},{"cell_type":"code","metadata":{},"source":"exception NoChange of int ;;","outputs":[],"execution_count":79},{"cell_type":"code","metadata":{},"source":"raise Failure ;;","outputs":[],"execution_count":80},{"cell_type":"code","metadata":{},"source":"try\n  print_endline \"pre exception\";\n  raise (NoChange 1);\n  print_endline \"post exception\";\nwith\n  | NoChange _ ->\n      print_endline \"handled a NoChange exception\"\n;;","outputs":[],"execution_count":81},{"cell_type":"markdown","metadata":{},"source":"\nEach `exception` declaration introduces a distinct sort of exception, which can\nbe handled separately from others. If $E$ raises an exception, then its\nevaluation has failed; _handling_ an exception means evaluating another\nexpression and returning its value instead. One exception handler can specify\nseparate expressions for different sorts of exceptions.\n\nException names are _constructors_ of the special datatype `exn`.  This is a\npeculiarity of ML that lets exception-handlers use pattern-matching. Note that\nexception `Failure` is just an error indication, while `NoChange n` carries\nfurther information: the integer $n$.\n\nThe effect of `raise <expr>` is to jump to the most recently-encountered\nhandler that matches `<expr>`.  The matching handler can only be found\n_dynamically_ (during execution); contrast with how ML associates occurrences\nof identifiers with their matching declarations, which does not require running\nthe program.\n\nOne criticism of OCaml's exceptions is that---unlike the Java language---nothing\nin a function declaration indicates which exceptions it might raise. One\nalternative to exceptions is to instead return a value of datatype `option`.\n"},{"cell_type":"code","metadata":{},"source":"let x = Some 1 ;;","outputs":[],"execution_count":82},{"cell_type":"code","metadata":{},"source":"let y = None ;;","outputs":[],"execution_count":83},{"cell_type":"code","metadata":{},"source":"type 'a option = None | Some of 'a ;;","outputs":[],"execution_count":84},{"cell_type":"markdown","metadata":{},"source":"\n`None` signifies an error, while `Some x` returns the solution $x$.  This\napproach looks clean, but the drawback is that many places in the code would\nhave to check for `None`.  Despite this, there is a builtin `option` type\nin OCaml as it is so useful.\n"},{"cell_type":"markdown","metadata":{},"source":"### Making Change with Exceptions\n\n\n"},{"cell_type":"code","metadata":{},"source":"exception Change\nlet rec change = function\n  | till, 0 -> []\n  | [], amt -> raise Change\n  | c::till, amt ->\n      if amt < 0 then\n        raise Change\n      else begin\n        try\n           c :: change (c::till, amt-c)\n         with\n           Change -> change (till,amt)\n      end\n ;;","outputs":[],"execution_count":85},{"cell_type":"markdown","metadata":{},"source":"\nIn the Lists lectures, we considered the problem of making change.  The greedy\nalgorithm presented there could not express \"6 using 5 and 2\" because it always\ntook the largest coin.  Returning the list of all possible solutions avoids\nthat problem rather expensively: we only need one solution.\n\nUsing exceptions, we can code a _backtracking_ algorithm: one that can undo\npast decisions if it comes to a dead end.  The exception `Change` is raised if\nwe run out of coins (with a non-zero amount) or if the amount goes negative.\nWe always try the largest coin, but enclose the recursive call in an exception\nhandler, which undoes the choice if it goes wrong.\n\nCarefully observe how exceptions interact with recursion.  The exception\nhandler always undoes the \\emph{most recent} choice, leaving others possibly to\nbe undone later.  If making change really is impossible, then eventually\nexception \\texttt{Change} will be raised with no handler to catch it, and it\nwill be reported at top level.\n\n### Making Change: A Trace\n"},{"cell_type":"raw","metadata":{},"source":"change([5,2],6)\n5::change([5,2],1) handle C => change([2],6)\n5::(5::change([5,2],~4) handle C => change([2],1))\n    handle C => change([2],6)\n5::change([2],1) handle C => change([2],6)\n5::(2::change([2],~1) handle C => change([],1))\n    handle C => change([2],6)\n5::(change([],1)) handle C => change([2],6)\nchange([2],6)"},{"cell_type":"markdown","metadata":{},"source":"\nHere is the full execution. Observe how the exception handlers nest and how\nthey drop away once the given expression has returned a value.\n"},{"cell_type":"raw","metadata":{},"source":"change([5,2],6)\n5::change([5,2],1) handle C => change([2],6)\n5::(5::change([5,2],~4) handle C => change([2],1))\n    handle C => change([2],6)\n5::change([2],1) handle C => change([2],6)\n5::(2::change([2],~1) handle C => change([],1))\n    handle C => change([2],6)\n5::(change([],1)) handle C => change([2],6)\nchange([2],6)\n2::change([2],4) handle C => change([],6)\n2::(2::change([2],2) handle C => change([],4)) handle ...\n2::(2::(2::change([2],0) handle C => change([],2)) handle C => ...)\n2::(2::[2] handle C => change([],4)) handle C => change([],6)\n2::[2,2] handle C => change([],6)\n[2,2,2]"},{"cell_type":"markdown","metadata":{},"source":"\n### Binary Trees, a Recursive Datatype\n"},{"cell_type":"code","metadata":{},"source":"type 'a tree =\n    Lf\n  | Br of 'a * 'a tree * 'a tree","outputs":[],"execution_count":86},{"cell_type":"markdown","metadata":{},"source":"\nTODO includegraphics(bintree)\n"},{"cell_type":"code","metadata":{},"source":"Br(1, Br(2, Br(4, Lf, Lf),\n              Br(5, Lf, Lf)),\n                Br(3, Lf, Lf))","outputs":[],"execution_count":87},{"cell_type":"markdown","metadata":{},"source":"\nA data structure with multiple branching is called a \"tree\".  Trees can\nrepresent mathematical expressions, logical formulae, computer programs, the\nphrase structure of English sentences, etc.\n\n_Binary trees_ are nearly as fundamental as lists.  They can provide\nefficient storage and retrieval of information.  In a binary tree, each node\nis empty ($Lf$), or is a branch ($Br$) with a label and two subtrees.\n\nOCaml lists are a datatype and could be declared as follows:\n"},{"cell_type":"code","metadata":{},"source":"type 'a list =\n | Nil\n | Cons of 'a * 'a list","outputs":[],"execution_count":88},{"cell_type":"markdown","metadata":{},"source":"\nWe could even declare `::` as an infix constructor.  The only\nthing we could not define is the `[...]` notation, which is\npart of the OCaml grammar (although there does exist a mechanism\nto use a _similar_ syntax for custom indexed datatypes).\n\nA recursive type does not have to be polymorphic.\nFor example, here is a simple datatype of tree shapes with no attached data\nthat is recursive but not polymorphic.\n"},{"cell_type":"code","metadata":{},"source":"type shape =\n    Null\n  | Join of shape * shape","outputs":[],"execution_count":89},{"cell_type":"markdown","metadata":{},"source":"\nThe datatype `'a option` (mentioned above) is the opposite -- it is\npolymorphic, but not recursive.\n"},{"cell_type":"markdown","metadata":{},"source":"### Basic Properties of Binary Trees\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec count = function\n  | Lf -> 0  (* number of branch nodes *)\n  | Br (v, t1, t2) -> 1 + count t1 + count t2","outputs":[],"execution_count":90},{"cell_type":"code","metadata":{},"source":"let rec depth = function\n  | Lf -> 0  (* length of longest path *)\n  | Br (v, t1, t2) -> 1 + max (depth t1) (depth t2)","outputs":[],"execution_count":91},{"cell_type":"markdown","metadata":{},"source":"\nThe invariant $\\texttt{count}(t)\\le 2^{\\texttt{depth}(t)} - 1$ holds in the functions above.\n\nFunctions on trees are expressed recursively using pattern-matching.  Both\nfunctions above are analogous to \\texttt{length} on lists.  Here is a third\nmeasure of a tree's size:\n"},{"cell_type":"code","metadata":{},"source":"let rec leaves = function\n  | Lf -> 1\n  | Br (v, t1, t2) -> leaves t1 + leaves t2","outputs":[],"execution_count":92},{"cell_type":"markdown","metadata":{},"source":"\nThis function is redundant because of a basic fact about trees, which can be\nproved by induction: for every tree $t$, we have $\\texttt{leaves}(t) =\n\\texttt{count}(t)+1$.  The inequality shown on the slide also has an elementary\nproof by induction.\n\nA tree of depth 20 can store $2^{20}-1$ or approximately one million elements.\nThe access paths to these elements are short, particularly when compared with\na million-element list!\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 8: Dictionaries and Functional Arrays\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"### Dictionaries\n\n\n\n- lookup: find an item in the dictionary\n- update (insert): replace (store) an item in the dictionary\n- delete: remove an item from the dictionary\n- empty: the null dictionary\n- Missing: exception for errors in `lookup` and `delete`\n\nIdeally, an _abstract type_ should provide these operations and hide the internal data structures.\n\nA dictionary attaches values to identifiers, called \"keys\".  Before choosing\nthe internal representation for a data structure, you need to specify the full\nset of operations. In fact, here we only consider `update` (associating a\nvalue with an identifier) and `lookup` (retrieving such a value). Deletion\nis more difficult and would limit our choices. Some applications may need\nadditional operations, such as `merge (combining two dictionaries). We\nshall see that update can be done efficiently in a functional style, without\nexcessive copying.\n\nAn _abstract type_ provides specified operations while hiding low-level\ndetails, such as the data structure used to represent dictionaries. Abstract\ntypes can be declared in any modern programming language. Java's _objects_\nserve this role, as do ML's modules. This course does not cover modules, and we\nsimply declare the dictionary operations individually.\n\nAn _association list_ (a list of pairs) is the simplest dictionary representation.\nLookup is by linear search, and therefore slow: $O(n)$. Association lists are\nonly usable if there are few keys in use. However, they are general in that the\nkeys do not need a concept of ordering, only equality.\n"},{"cell_type":"code","metadata":{},"source":"exception Missing","outputs":[],"execution_count":93},{"cell_type":"code","metadata":{},"source":"let rec lookup = function\n  | [], a -> raise Missing\n  | (x,y) :: pairs, a ->\n      if a = x then\n        y\n      else\n        lookup (pairs, a)","outputs":[],"execution_count":94},{"cell_type":"code","metadata":{},"source":"let update (l, b, y) = (b,y) :: l","outputs":[],"execution_count":95},{"cell_type":"markdown","metadata":{},"source":"\nTo enter a new `(key, value)` pair, simply \"cons\" it to the list with `update`.\nThis takes constant time, which is the best we could hope for.  But the space\nrequirement is huge: linear in the number of updates, not in the number of\ndistinct keys. Obsolete entries are never deleted: that would require first\nfinding them, increasing the update time from $O(1)$ to~$O(n)$.\n"},{"cell_type":"markdown","metadata":{},"source":"### Binary Search Trees\n\n\n\nA _dictionary_ associates _values_ (here, numbers) with _keys_.\nTODO includegraphics binsearch.\n\nBinary search trees are an important application of binary trees.  They work\nfor keys that have a total ordering, such as strings.  Each branch of the tree\ncarries a $(key,value)$ pair; its left subtree holds smaller keys; the right\nsubtree holds greater keys.  If the tree remains reasonably balanced, then\nupdate and lookup both take $O(\\log n)$ for a tree of size $n$.  These times\nhold in the average case; given random data, the tree is likely to remain\nbalanced.\n\nAt a given node, all keys in the left subtree are smaller (or equal) while all\ntrees in the right subtree are greater.\n\nAn unbalanced tree has a linear access time in the worst case.  Examples\ninclude building a tree by repeated insertions of elements in increasing or\ndecreasing order; there is a close resemblance to quicksort.  Building a binary\nsearch tree, then converting it to inorder, yields a sorting algorithm called\n_treesort_.\n\nSelf-balancing trees, such as Red-Black trees, attain $O(\\log n)$ in the worst\ncase.  They are complicated to implement.\n\n### Lookup: Seeks Left or Right\n"},{"cell_type":"code","metadata":{},"source":"exception Missing of string","outputs":[],"execution_count":96},{"cell_type":"code","metadata":{},"source":"let rec lookup = function\n | Br ((a,x), t1, t2), b ->\n     if b < a then\n       lookup (t1, b)\n     else if a < b then\n       lookup (t2, b)\n     else\n       x\n | Lf, b -> raise (Missing b)","outputs":[],"execution_count":97},{"cell_type":"markdown","metadata":{},"source":"\nGuaranteed $O(\\log n)$ access time _if_ the tree is balanced!\n\nLookup in the binary search tree goes to the left subtree if the desired\nkey is smaller than the current one and to the right if it is greater.\nIt raises `Missing` if it encounters an empty tree.\n\nSince an ordering is involved, we have to declare the functions for a specific\ntype, here `string`.  Now exception `Missing` mentions that type: if lookup\nfails, the exception returns the missing key.  The exception could be\neliminated using type `option` of our earlier Datatypes lecture, using the\nconstructor `None` for failure.\n\n### Update\n"},{"cell_type":"code","metadata":{},"source":"let rec update = function\n  | Lf, b, y -> Br ((b,y), Lf, Lf)\n  | Br ((a,x), t1, t2), b, y ->\n      if b < a then\n        Br ((a,x), update (t1, b, y), t2)\n      else if a < b then\n        Br ((a,x), t1, update (t2, b, y))\n      else (* a = b *)\n        Br ((a,y), t1, t2)","outputs":[],"execution_count":98},{"cell_type":"markdown","metadata":{},"source":"\nThis is also $O(\\log n)$ as it copies the path only, and _not whole subtrees_!.\n\nIf you are familiar with the usual update operation for this sort of tree, you\nmay wonder whether it can be implemented in ML, where there is no direct way to\nreplace part of a data structure by something else.\n\nThe update operation is a nice piece of functional programming.  It searches\nin the same manner as `lookup`, but the recursive calls reconstruct a\nnew tree around the result of the update.  One subtree is updated and the\nother left unchanged.  The internal representation of trees ensures that\nunchanged parts of the tree are not copied, but _shared_.\nTherefore, update copies only the path from the root to the new\nnode.  Its time and space requirements, for a reasonably balanced tree, are\nboth $O(\\log n)$.\n\nThe comparison between $b$ and $a$ allows three cases:\n- smaller: update the left subtree; share the right\n- greater: update the right subtree; share the left\n- equal: update the label and share both subtrees\n\nNote: in the function definition, `(* a = b*)` is a comment.  Comments\nin OCaml are enclosed in the brackets `(*` and `*)`.\n"},{"cell_type":"markdown","metadata":{},"source":"### Aside: Traversing Trees (3 Methods)\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec preorder = function\n  | Lf -> []\n  | Br (v, t1, t2) ->\n      [v] @ preorder t1 @ preorder t2","outputs":[],"execution_count":99},{"cell_type":"code","metadata":{},"source":"let rec inorder = function\n  | Lf -> []\n  | Br (v, t1, t2) ->\n      inorder t1 @ [v] @ inorder t2","outputs":[],"execution_count":100},{"cell_type":"code","metadata":{},"source":"let rec postorder = function\n  | Lf -> []\n  | Br (v, t1, t2) ->\n      postorder t1 @ postorder t2 @ [v]","outputs":[],"execution_count":101},{"cell_type":"markdown","metadata":{},"source":"\n_Tree traversal_ means examining each node of a tree in some order.  D. E.\nKnuth has identified three forms of tree traversal: preorder, inorder and\npostorder (TODO cite knuth).  We can code these \"visiting orders\" as functions\nthat convert trees into lists of labels.  Algorithms based on these notions\ntypically perform some action at each node; the functions above simply copy\nthe nodes into lists.  Consider the tree:\n\nTODO includegraphics bintree2\n\n- `preorder` visits the label first (\"Polish notation\"), yielding `ABDECFG`\n- `inorder` visits the label midway, yielding `DBEAFCG`\n- `postorder` visits the label last (\"Reverse Polish\"), yielding `DEBFGCA`. You will be familiar with this concept if you own an RPN calculator.\n\nWhat is the use of `inorder`? Consider applying it to a binary search tree: the\nresult is a sorted list of pairs. We could use this, for example, to merge two\nbinary search trees. It is not difficult to transform a sorted list of pairs\ninto a binary search tree.\n"},{"cell_type":"markdown","metadata":{},"source":"### Efficiently Traversing Trees\n\n\n\nUnfortunately, the functions shown on the previous slide are quadratic in the\nworst case: the appends in the recursive calls are inefficient.  To correct\nthat problem, we (as usual) add an accumulating argument.  Observe how\neach function constructs its result list and compare with how appends were\neliminated from `quicksort` in the Sorting lecture.\n"},{"cell_type":"code","metadata":{},"source":"let rec preord = function\n  | Lf, vs -> vs\n  | Br (v, t1, t2), vs ->\n      v :: preord (t1, preord (t2, vs))","outputs":[],"execution_count":102},{"cell_type":"code","metadata":{},"source":"let rec inord = function\n  | Lf , vs -> vs\n  | Br (v, t1, t2), vs ->\n      v :: preord (t1, preord (t2, vs))","outputs":[],"execution_count":103},{"cell_type":"code","metadata":{},"source":"let rec postord = function\n  | Lf, vs -> vs\n  | Br (v, t1, t2), vs ->\n      postord (t1, postord (t2, v::vs))","outputs":[],"execution_count":104},{"cell_type":"markdown","metadata":{},"source":"\nOne can prove equations relating each of these functions to its counterpart on\nthe previous slide.  For example:\n\n$$ \\texttt{inord}(t,vs) = \\texttt{inorder}(t) @ vs $$\n\nThese three types of tree traversal are related in that all are depth-first.\nThey each traverse the left subtree in full before traversing the right\nsubtree.  Breadth-first search (from the Queues lecture) is another\npossibility.  That involves going through the levels of a tree one at a time.\n\n### Arrays\n\n- A conventional array is an indexed storage area.\n  * It is updated _in place_ by the command `a.(k) <- x`\n  * The concept is inherently _imperative_.\n- A _functional array_ is a finite map from integers to data.\n  * Updating implies _copying_ to return `update(A,k,x)`\n  * The new array equals `A` except that `A[k] = x`.\n- Can we do updates efficiently?\n\nThe elements of a list can only be reached by counting from the front.\nElements of a tree are reached by following a path from the root.  An\n_array_ hides such structural matters; its elements are uniformly\ndesignated by number.  Immediate access to arbitrary parts of a data structure\nis called _random access_.\n\nArrays are the dominant data structure in conventional programming languages.\nThe ingenious use of arrays is the key to many of the great classical\nalgorithms, such as Hoare's original quicksort (the partition step) and\nWarshall's transitive-closure algorithm.\n\nThe drawback is that subscripting is a chief cause of programmer error.  That\nis why arrays play little role in this introductory course.\n\nFunctional arrays are described below in order to illustrate another way of\nusing trees to organize data.  Here is a summary of basic dictionary data\nstructures in order of decreasing generality and increasing efficiency:\n- Linear search: Most general, needing only equality on keys, but inefficient: linear time.\n- Binary search: Needs an ordering on keys.  Logarithmic access time in the average case, but our binary search trees are linear in the worst case.\n- Array subscripting: Least general, requiring keys to be integers, but even worst-case time is logarithmic.\n"},{"cell_type":"markdown","metadata":{},"source":"### Functional Arrays as Binary Trees\n\n\n\nThe path to element $i$ follows the _binary code_ for $i$ (its \"subscript\").\n\nTODO includegraphics array1\n\nThis simple representation (credited to W. Braun) ensures that the tree is\nbalanced.  Complexity of access is always $O(\\log n)$, which is optimal.  For\nactual running time, access to conventional arrays is much faster: it requires\nonly a few hardware instructions.  Array access is often taken to be $O(1)$,\nwhich (as always) presumes that hardware limits are never exceeded.\n\nThe lower bound for array indices is one.  The upper bound starts at zero\n(which signifies the empty array) and can grow without limit.  Inspection of\nthe diagram above should make it clear that these trees are always balanced:\nthe left subtree can have at most one node more than the right subtree,\nrecursively all the way down.  (This assumes that the array is defined for\nsubscripts $1\\ldots n$ with no gaps; an array defined only for odd numbers, for\nexample, would obviously be unbalanced.)\n\nThe numbers in the diagram above are not the labels of branch nodes, but\nindicate the positions of array elements. For example, the label corresponding\nto $A[2]$ is at the position shown. The nodes of a functional array are\nlabelled with the data we want to store, not with these integers.\n"},{"cell_type":"markdown","metadata":{},"source":"### The Lookup Function\n\n\n"},{"cell_type":"code","metadata":{},"source":"exception Subscript\nlet rec sub = function\n  | Lf, _ -> raise Subscript  (* Not found *)\n  | Br (v,t1,t2), k ->\n      if k = 1 then v\n      else if k mod 2 = 0 then\n        sub (t1, k / 2)\n      else\n        sub (t2, k / 2)","outputs":[],"execution_count":105},{"cell_type":"code","metadata":{},"source":"let rec sub = function (* TODO Alternative implementation *)\n  | Lf, _ -> raise Subscript\n  | Br (v,t1,t2), 1 -> v\n  | Br (v,t1,t2), k when k mod 2 = 0 -> sub (t1, k / 2)\n  | Br (v,t1,t2), k -> sub (t2, k / 2)","outputs":[],"execution_count":106},{"cell_type":"markdown","metadata":{},"source":"\nThe lookup function `sub`, divides the subscript by 2 until 1 is\nreached.  If the remainder is 0 then the function follows the left subtree,\notherwise the right.  If it reaches a leaf, it signals error by raising\nexception `Subscript`.\n\nArray access can also be understood in terms of the subscript's binary code.\nBecause the subscript must be a positive integer, in binary it has a leading\none.  Discard this one and reverse the remaining bits.  Interpreting zero\nas _left_ and one as _right_ yields the path from the root to the\nsubscript.\n\nPopular literature often explains the importance of binary as being led by\nhardware: because a circuit is either on or off.  The truth is almost the\nopposite.  Designers of digital electronics go to a lot of trouble to suppress\nthe continuous behaviour that would naturally arise.  The real reason why\nbinary is important is its role in algorithms: an `if-then-else` decision leads\nto binary branching.\n\nData structures, such as trees, and algorithms, such as mergesort, use binary\nbranching in order to reduce a cost from $O(n)$ to $O(\\log n)$.  Two is the\nsmallest integer divisor that achieves this reduction.  (Larger divisors are\nonly occasionally helpful, as in the case of B-trees, where they reduce the\nconstant factor.)  The simplicity of binary arithmetic compared with decimal\narithmetic is just another instance of the simplicity of algorithms based on\nbinary choices.\n"},{"cell_type":"markdown","metadata":{},"source":"### The Update Function\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec update = function\n  | Lf, k, w ->\n      if k = 1 then\n        Br (w, Lf, Lf)\n      else\n        raise Subscript  (* Gap in tree *)\n  | Br (v, t1, t2), k, w ->\n      if k = 1 then\n        Br (w, t1, t2)\n      else if k mod 2 = 0 then\n        Br (v, update (t1, k / 2, w), t2)\n      else\n        Br (v, t1, update (t2, k / 2, w))","outputs":[],"execution_count":107},{"cell_type":"markdown","metadata":{},"source":"\nThe `update` function also divides the subscript repeatedly by two.  When it\nreaches a value of one, it has identified the element position.  Then it\nreplaces the branch node by another branch with the new label.\n\nA leaf may be replaced by a branch, extending the array, provided no\nintervening nodes have to be generated.  This suffices for arrays without gaps\nin their subscripting.  (The data structure can be modified to allow _sparse_\narrays, where most subscript positions are undefined.) Exception `Subscript`\nindicates that the subscript position does not exist and cannot be created.\nThis use of exceptions is not easily replaced by `None` and `Some`.\n\nNote that there are two tests involving $k=1$.  If we have reached a leaf,\nit returns a branch, extending the array by one.  If we are still at a branch\nnode, then the effect is to update an existing array element.\n\nA similar function can _shrink_ an array by one.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 8: Functions as Value\n\n\n\nIn ML, functions can be\n- passed as arguments to other functions,\n- returned as results,\n- put into lists, trees, etc.,\n- but _not_ tested for equality.\n"},{"cell_type":"code","metadata":{},"source":"[ (fun n -> n * 2);\n  (fun n -> n * 3);\n  (fun n -> n + 1)  ]","outputs":[],"execution_count":108},{"cell_type":"markdown","metadata":{},"source":"\nProgress in programming languages can be measured by what abstractions they\nadmit.  Conditional expressions (descended from conditional jumps based\non the sign of some numeric variable) and parametric types such as\n$\\alpha\\,\\texttt{list}$ are examples.  The idea that functions could be used\nas values in a computation arose early, but it took some time before the idea\nwas fully realized.  Many programming languages let functions be passed as\narguments to other functions, but few take the trouble needed to allow\nfunctions to be returned as results.\n\nIn mathematics, a _functional_ or _higher-order function_ is a\nfunction that operates on other functions.  Many functionals are familiar from\nmathematics: for example, the differential operator maps functions to their\nderivatives, which are also functions.  To a mathematician, a function is\ntypically an infinite, uncomputable object.  We use ML functions to represent\nalgorithms.  Sometimes they represent infinite collections of data given by\ncomputation rules.\n\nFunctions cannot be compared for equality. We could compare the machine\naddresses of the compiled code, but that would merely be a test of identity: it\nwould regard any two separate functions as unequal even if they were compiled\nfrom identical pieces of source code.  Such a low-level feature has no place in\na principled language.\n"},{"cell_type":"markdown","metadata":{},"source":"### Functions Without Names\n\n\n\nIf functions are to be regarded as computational values, then we need a\nnotation for them.  The `fun` notation expresses a non-recursive function\nvalue without giving the function a name.\n\n$\\texttt{\\uline{fun} $x$ => $E$}$ is the function $f$ such that $f(x)=E$.\nThe function \\texttt{(\\uline{fun} n => n*2)} is a _doubling function_.\n"},{"cell_type":"code","metadata":{},"source":"fun n -> n * 2","outputs":[],"execution_count":109},{"cell_type":"code","metadata":{},"source":"(fun n -> n * 2) 17","outputs":[],"execution_count":110},{"cell_type":"markdown","metadata":{},"source":"\nThe main purpose of `fun`-notation is to package up small expressions that are to be\napplied repeatedly using some other function.\nThe expression `fun n -> n*2` has the same value as the identifier\n`double`, declared as follows:\n"},{"cell_type":"code","metadata":{},"source":"let double n = n * 2","outputs":[],"execution_count":111},{"cell_type":"markdown","metadata":{},"source":"\nThe `fun` notation can also do pattern matching, and the `function` keyword\nadds an anonymous variable name to pattern match against.  The following functions are all equivalent, with the latter definitions bound to the `is_zero` value and the earlier ones anonymous:\n"},{"cell_type":"code","metadata":{},"source":"fun x -> match x with 0 -> true | _ -> false","outputs":[],"execution_count":112},{"cell_type":"code","metadata":{},"source":"function 0 -> true | _ -> false","outputs":[],"execution_count":113},{"cell_type":"code","metadata":{},"source":"let is_zero = fun x -> match x with 0 -> true | _ -> false","outputs":[],"execution_count":114},{"cell_type":"code","metadata":{},"source":"let is_zero = function 0 -> true | _ -> false","outputs":[],"execution_count":115},{"cell_type":"markdown","metadata":{},"source":"### Curried Functions\n\n\n\nA _curried function_ returns another function as its result.    \n"},{"cell_type":"code","metadata":{},"source":"let prefix = fun a -> fun b -> a ^ b","outputs":[],"execution_count":116},{"cell_type":"code","metadata":{},"source":"let promote = prefix \"Senior \"","outputs":[],"execution_count":117},{"cell_type":"code","metadata":{},"source":"prefix \"Junior \" \"Professor\"","outputs":[],"execution_count":118},{"cell_type":"code","metadata":{},"source":"promote \"Professor\"","outputs":[],"execution_count":119},{"cell_type":"markdown","metadata":{},"source":"\nA short form for the definition of `prefix` is simply to pass multiple\narguments to the function definition.  The following two definitions\nare equivalent in OCaml:\n"},{"cell_type":"code","metadata":{},"source":"let prefix = fun a -> fun b -> a ^ b","outputs":[],"execution_count":120},{"cell_type":"code","metadata":{},"source":"let prefix a b = a ^ b","outputs":[],"execution_count":121},{"cell_type":"markdown","metadata":{},"source":"\nCurrying is the technique of expressing a function taking multiple arguments as nested functions, each taking a single argument.\nThe `fun`-notation lets us package `n*2` as the function\n`fun n -> n*2`, but what if there are several variables, as in\n`fun n -> n*2+k`?  A function of two arguments could be coded using\npattern-matching on pairs, writing `fun (n,k) -> n*2+k`.\n\nCurrying is an alternative, where we _nest_ the `fun`-notation:\n"},{"cell_type":"code","metadata":{},"source":"fun k -> fun n -> n*2+k","outputs":[],"execution_count":122},{"cell_type":"markdown","metadata":{},"source":"\nApplying this curried function to the argument 1 yields another function, in which `k` has been replaced by 1:\n"},{"cell_type":"code","metadata":{},"source":"let fn = fun k -> fun n -> n*2+k ;;","outputs":[],"execution_count":123},{"cell_type":"code","metadata":{},"source":"let fn' = fn 1 (* n*2+1 *)","outputs":[],"execution_count":124},{"cell_type":"code","metadata":{},"source":"fn' 3  (* 3*2+1 *)","outputs":[],"execution_count":125},{"cell_type":"markdown","metadata":{},"source":"\nAnd this function, when applied to 3, yields the result 7. The two arguments are supplied one after another.\n\nThe example on the slide is similar but refers to the expression `a^b`,\nwhere `^` is the infix operator for string concatenation. Function `promote` binds the first argument of `prefix` to\n`\"Professor\"`; the resulting function prefixes that title\nto any string to which it is applied.\n"},{"cell_type":"markdown","metadata":{},"source":"### Shorthand for Curried Functions\n\n\n\nA function-returning function is just a function of two arguments.\n\nThis curried function syntax is nicer than nested `fun` binders:\n"},{"cell_type":"code","metadata":{},"source":"let prefix a b = a ^ b","outputs":[],"execution_count":126},{"cell_type":"code","metadata":{},"source":"let dub = prefix \"Sir \"","outputs":[],"execution_count":127},{"cell_type":"markdown","metadata":{},"source":"\nCurried functions allows _partial application_ (to the first argument).\n\nIn OCaml, an $n$-argument curried function `f` can be declared using the syntax\n\n$$\\tt \\uline{let} f \\(x_1\\) \\ldots \\(x_{n}\\) = \\(E\\)$$\n\nand applied using the syntax $f \\(E_1\\) \\ldots \\(E_n\\)$.\nIf `f` is not recursive, then it is equivalent to the function expressed via nesting as follows:\n\n$$\n\\begin{quote}\\tt\n  \\uline{fn} \\(x_1\\) => \\(\\cdots\\) (\\uline{fn} \\(x_{n}\\) => \\(E\\))\n\\end{quote}\n$$\n\nWe now have two ways of expressing functions of multiple arguments: either by\npassing a pair of arguments or by currying.  Currying allows _partial application_\nwhich is useful when fixing the first argument yields a function\nthat is interesting in its own right.  An example from mathematics is the\nfunction $x^y$, where fixing $y=2$ yields a function in $x$ alone, namely\nsquaring. Similarly, $y=3$ yields cubing, while $y=1$ yields the identity\nfunction.\n\nThough the function `hd` (which returns the head of a list) is not\ncurried, it may be used with the curried application syntax in some\nexpressions:\n"},{"cell_type":"code","metadata":{},"source":"List.hd [dub; promote] \"Hamilton\" ;;","outputs":[],"execution_count":128},{"cell_type":"markdown","metadata":{},"source":"\nHere `List.hd` is applied to a list of functions, and the resulting function\n`dub` is then applied to the string `\"Hamilton\"`.  The idea of\nexecuting code stored in data structures reaches its full development in\n_object-oriented_ programming, like in Java.\n"},{"cell_type":"markdown","metadata":{},"source":"### Partial Application: A Curried Insertion Sort\n\n\n"},{"cell_type":"code","metadata":{},"source":"let insort lessequal =\n  let rec ins = function\n    | x, [] -> [x]\n    | x, y::ys ->\n        if lessequal x y then\n           x :: y :: ys\n        else\n           y :: ins (x,ys)\n  in\n  let rec sort = function\n    | [] -> []\n    | x::xs -> ins (x, sort xs)\n  in\n  sort","outputs":[],"execution_count":129},{"cell_type":"markdown","metadata":{},"source":"\nThe sorting functions we discussed in earlier lectures are coded to sort real\nnumbers.  They can be generalized to an arbitrary ordered type by passing the\nordering predicate `leq` as an argument.\n\nFunctions `ins` and `sort` are declared locally, referring to `lessequal`.\nThough it may not be obvious, `insort` is a curried function.  Given its first\nargument, a predicate for comparing some particular type of items, it returns\nthe function `sort` for sorting lists of that type of items.\n\nSome examples of its use:\n"},{"cell_type":"code","metadata":{},"source":"insort (<=) [5;3;9;8] ;;","outputs":[],"execution_count":130},{"cell_type":"code","metadata":{},"source":"insort (<=) [\"bitten\";\"on\";\"a\";\"bee\"] ;;","outputs":[],"execution_count":131},{"cell_type":"code","metadata":{},"source":"insort (>=) [5;3;9;8] ;;","outputs":[],"execution_count":132},{"cell_type":"markdown","metadata":{},"source":"\nAn obscure point: the syntax `(<=)` denotes the comparison operator as a\nfunction, which is then given to `insort`.  Passing the relation $\\geq$ for\n`lessequal` gives a decreasing sort.  This is no coding trick; it is justified\nin mathematics, since if $\\leq$ is a partial ordering then so is $\\geq$.\n"},{"cell_type":"markdown","metadata":{},"source":"### map: the \"Apply to All\" Function\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec map f = function\n  | [] -> []\n  | x::xs -> (f x) :: map f xs ;;","outputs":[],"execution_count":133},{"cell_type":"code","metadata":{},"source":"map (fun s -> s ^ \"ppy\") [\"Hi\"; \"Ho\"] ;;","outputs":[],"execution_count":134},{"cell_type":"code","metadata":{},"source":"map (map double) [[1]; [2;3]] ;;","outputs":[],"execution_count":135},{"cell_type":"markdown","metadata":{},"source":"\nThe functional `map` applies a function to every element of a list,\nreturning a list of the function's results.  \"Apply to all\" is a fundamental\noperation and we shall see several applications of it below.  We\nagain see the advantages of `fun`-notation, currying and\n`map`.  If we did not have them, the first example on the slide\nwould require a preliminary function declaration:\n"},{"cell_type":"code","metadata":{},"source":"let rec sillylist = function\n  | [] -> []\n  | s::ss -> (s^\"ppy\") :: sillylist ss","outputs":[],"execution_count":136},{"cell_type":"markdown","metadata":{},"source":"\nAn expression containing several applications of functionals---such as our\nsecond example---can abbreviate a long series of declarations.  Sometimes this\ncoding style is cryptic, but it can be clear as crystal.  Treating functions\nas values lets us capture common program structures once and for all.\n\nIn the second example, `double` is the obvious integer doubling function we\ndefined earlier.  Note that \\texttt{map} is a built-in OCaml function in the\nform of `List.map`.  OCaml's standard library includes, among much else, many\nlist functions.\n"},{"cell_type":"markdown","metadata":{},"source":"### Example: Matrix Transpose\n\n\n\n$$\n\\begin{pmatrix}\n           a   & b & c \\\\\n           d   & e & f\n   \\end{pmatrix}^T =\n   \\begin{pmatrix}\n        a & d     \\\\\n        b & e     \\\\\n        c & f\n\\end{pmatrix}\n$$\n"},{"cell_type":"code","metadata":{},"source":"let rec transp = function\n  | []::_ -> []\n  | rows -> (map List.hd rows) ::\n            (transp (map List.tl rows)) ;;","outputs":[],"execution_count":137},{"cell_type":"markdown","metadata":{},"source":"\nA matrix can be viewed as a list of rows, each row a list of matrix elements.\nThis representation is not especially efficient compared with the conventional\none (using arrays).  Lists of lists turn up often, though, and we can see how\nto deal with them by taking familiar matrix operations as examples.\n_ML for the Working Programmer_ goes as far as Gaussian elimination,\nwhich presents surprisingly few difficulties.\n\nThe transpose of the matrix\n$\\left(\\begin{smallmatrix} a & b & c \\\\\n                           d & e & f\\end{smallmatrix}\\right)$\nis\n$\\left(\\begin{smallmatrix}\n        a & d     \\\\\n        b & e     \\\\\n        c & f\n   \\end{smallmatrix}\\right)$,\nwhich in OCaml corresponds to the following transformation on lists of lists:\n"},{"cell_type":"raw","metadata":{},"source":"[[a,b,c], [d,e,f]] => [[a,d], [b,e], [c,f]]"},{"cell_type":"markdown","metadata":{},"source":"\nThe workings of function `transp` are simple.  If `rows` is the\nmatrix to be transposed, then `map hd` extracts its first column and\n`map tl` extracts its second column:\n"},{"cell_type":"raw","metadata":{},"source":"map hd rows => [a,d]\nmap tl rows => [[b,c], [e,f]]"},{"cell_type":"markdown","metadata":{},"source":"\nA recursive call transposes the latter matrix, which is then given the column\n`[a,d]` as its first row.\nThe two functions expressed using `map` would otherwise have to be declared\nseparately.\n"},{"cell_type":"markdown","metadata":{},"source":"### Review of Matrix Multiplication\n\n\n\n$$\n\\begin{pmatrix} A_1 & \\cdots & A_k \\end{pmatrix}  \\cdot\n   \\begin{pmatrix}\n        B_1 \\\\ \\vdots \\\\ B_k\n   \\end{pmatrix}   =\n   \\begin{pmatrix}\n        A_1 B_1 + \\cdots + A_k B_k\n   \\end{pmatrix}\n$$\n\nThe right side is the _vector dot product_ $\\vec{A}\\cdot \\vec{B}$.\nRepeat for each _row_ of $A$ and _column_ of $B$.\n\nThe _dot product_ of two vectors is\n\\[ (a_1,\\ldots,a_k) \\cdot (b_1,\\ldots,b_k) = a_1b_1 + \\cdots + a_kb_k. \\]\n\nA simple case of matrix multiplication is when $A$ consists of a single row\nand $B$ consists of a single column.  Provided $A$ and $B$ contain the same\nnumber $k$ of elements, multiplying them yields a $1\\times1$ matrix whose\nsingle element is the dot product shown above.\n\nIf $A$ is an $m\\times k$ matrix and $B$ is a $k\\times n$ matrix\nthen $A\\times B$ is an $m\\times n$ matrix.\nFor each $i$ and $j$, the $(i,j)$ element of $A\\times B$ is the dot\nproduct of row $i$ of $A$ with column $j$ of $B$.\n\n$$\n\\begin{pmatrix}\n        2 & 0 \\\\\n        3 &-1 \\\\\n        0 & 1 \\\\\n        1 & 1\n   \\end{pmatrix}\n   \\begin{pmatrix}\n        1 & 0 & 2 \\\\\n        4 &-1 & 0\n   \\end{pmatrix}   =\n   \\begin{pmatrix}\n        2 & 0 & 4 \\\\\n       -1 & 1 & 6 \\\\\n        4 &-1 & 0 \\\\\n        5 &-1 & 2\n\\end{pmatrix}\n$$\n\nThe (1,1) element above is computed by\n$$ (2,0)\\cdot(1,4) = 2\\times1 + 0\\times4 = 2. $$\n\nCoding matrix multiplication in a conventional programming language usually\ninvolves three nested loops.  It is hard to avoid mistakes in the subscripting,\nwhich often runs slowly due to redundant internal calculations.\n"},{"cell_type":"markdown","metadata":{},"source":"### Matrix Multiplication in ML\n\n\n\n_Dot product_ of two vectors---a _curried function_\n"},{"cell_type":"code","metadata":{},"source":"let rec dotprod xs ys =\n  match xs, ys with\n  | [], [] -> 0.0\n  | x::xs, y::ys ->  (x *. y) +. (dotprod xs ys)\n;;","outputs":[],"execution_count":138},{"cell_type":"markdown","metadata":{},"source":"\n_Matrix product_\n"},{"cell_type":"code","metadata":{},"source":"let rec matprod arows brows =\n  let cols = transp brows in\n  map (fun row -> map (dotprod row) cols) arows","outputs":[],"execution_count":139},{"cell_type":"markdown","metadata":{},"source":"\nThe `transp brows` converts $B$ into a list of columns.  It yields a\nlist, whose elements are the columns of $B$.  Each row of $A\\times B$ is\nobtained by multiplying a row of $A$ by the columns of $B$.\n\nBecause `dotprod` is curried, it can be applied to a row of $A$.  The\nresulting function is applied to all the columns of $B$.  We have another\nexample of currying and partial application.\n\nThe outer `map` applies `dotprod` to each row of $A$.  The inner\n`map`, using `fun`-notation, applies `dotprod row` to each\ncolumn of $B$.  Compare with the version in _ML for the Working\n  Programmer_ (page 89) which does not use `map` and requires two\nadditional function declarations.\n\nIn the dot product function, the two vectors must have the same length.\nOtherwise, exception `Match_failure` is raised.\n"},{"cell_type":"markdown","metadata":{},"source":"### List Functionals for Predicates\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec exists p = function\n  | [] -> false\n  | x::xs -> (p x) || (exists p xs) ;;","outputs":[],"execution_count":140},{"cell_type":"code","metadata":{},"source":"let rec filter p = function\n  | [] -> []\n  | x::xs ->\n      if p x then\n        x :: filter p xs\n      else\n        filter p xs ;;","outputs":[],"execution_count":141},{"cell_type":"markdown","metadata":{},"source":"\nA _predicate_ is a _boolean-valued_ function.\n\nThe functional `exists` transforms a predicate into a predicate over\nlists.  Given a list, `exists p` tests whether or not some list element\nsatisfies `p` (making it return `true`).  If it finds one, it stops\nsearching immediately, thanks to the behaviour of the lazy `||` operator.\n\nDually, we have a functional to test whether all list elements satisfy the\npredicate.  If it finds a counterexample then it, too, stops searching.\n"},{"cell_type":"code","metadata":{},"source":"let rec all p = function\n  | [] -> true\n  | x::xs -> (p x) && all p xs","outputs":[],"execution_count":142},{"cell_type":"markdown","metadata":{},"source":"\nThe `filter` functional is related to `map`.  It applies a predicate to all the\nlist elements, but instead of returning the resulting values (which could only\nbe `true` or `false`), it returns the list of elements satisfying the\npredicate.\n"},{"cell_type":"markdown","metadata":{},"source":"### Applications of the Predicate Functionals\n\n\n"},{"cell_type":"code","metadata":{},"source":"let member y xs =\n  exists (fun x -> x=y) xs","outputs":[],"execution_count":143},{"cell_type":"code","metadata":{},"source":"let inter xs ys =\n  filter (fun x -> member x ys) xs","outputs":[],"execution_count":144},{"cell_type":"markdown","metadata":{},"source":"\n_Testing whether two lists have no common elements_\n"},{"cell_type":"code","metadata":{},"source":"let disjoint xs ys =\n  all (fun x -> all (fun y -> x<>y) ys) xs","outputs":[],"execution_count":145},{"cell_type":"markdown","metadata":{},"source":"\nThe Lists lecture presented the function `member`, which tests whether a\nspecified value can be found as a list element, and `inter`, which returns the\n\"intersection\" of two lists: the list of elements they have in common.\n\nBut remember: the purpose of list functionals is not to replace the\ndeclarations of popular functions, which probably are available already.  It is\nto eliminate the need for separate declarations of ad-hoc functions.  When they\nare nested, like the calls to \\texttt{all} in \\texttt{disjoint} above, the\ninner functions are almost certainly one-offs, not worth declaring separately.\n\nOur primitives themselves can be seen as a programming language.  Part of the\ntask of programming is to extend our programming language with notation for\nsolving the problem at hand.  The levels of notation that we define should\ncorrespond to natural levels of abstraction in the problem domain.\n\nHistorical Note:\nAlonzo Church's $\\lambda$-calculus gave a simple syntax, $\\lambda$-notation,\nfor expressing functions.  It is the direct precursor of ML's\n`fun`-notation.  It was soon shown that his system was equivalent in\ncomputational power to Turing machines, and _Church's thesis_ states that\nthis defines precisely the set of functions that can be computed effectively.\n\nThe $\\lambda$-calculus had a tremendous influence on the design of functional\nprogramming languages.  McCarthy's Lisp was something of a false start; it\ninterpreted variable binding incorrectly, an error that stood for some 20\nyears.  But in 1966, Peter Landin (of Queen Mary College, University of London)\nsketched out the main features of functional languages.\n\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 9: Sequences, or Lazy Lists\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"### A Pipeline\n\n\n\n$$\n\\framebox{\\emph{Producer}} \\to \\framebox{\\emph{Filter}} \\to\\cdots\\to\n \\framebox{\\emph{Filter}} \\to \\framebox{\\emph{Consumer}}\n$$\n\n- Produce sequence of items\n- Filter sequence in stages\n- Consume results as needed\n- _Lazy lists_ join the stages together\n\nTwo types of program can be distinguished.  A sequential program\naccepts a problem to solve, processes for a while, and finally terminates\nwith its result.  A typical example is the huge numerical simulations that are\nrun on supercomputers.  Most of our ML functions also fit this model.\n\nAt the other extreme are _reactive_ programs, whose job is to interact\nwith the environment.  They communicate constantly during their operation and\nrun for as long as is necessary.  A typical example is the software that\ncontrols many modern aircraft.  Reactive programs often consist of\n_concurrent processes_ running at the same time and communicating with\none another.\n\nConcurrency is too difficult to consider in this course, but we can model\nsimple pipelines such as that shown above.  The _Producer_ represents one\nor more sources of data, which it outputs as a stream.  The _Filter_\nstages convert the input stream to an output stream, perhaps consuming several\ninput items to yield a single output item.  The _Consumer_ takes as many\nelements as necessary.\n\nThe Consumer drives the pipeline: nothing is computed except in response to\nits demand for an additional datum.  Execution of the Filter stages is\ninterleaved as required for the computation to go through.  The programmer\nsets up the data dependencies but has no clear idea of what happens when.  We\nhave the illusion of concurrent computation.\n\nThe Unix operating system provides similar ideas through its _pipes_ that\nlink processes together.  In ML, we can model pipelines using _lazy lists_.\n"},{"cell_type":"markdown","metadata":{},"source":"### Lazy Lists (or Streams)\n\n\n\n- Lists of possibly _infinite_ length\n- Elements _computed upon demand_\n- _Avoids _waste_ if there are many solutions\n- _Infinite_ values are a useful abstraction\n\nIn OCaml, we can implement laziness by _delaying evaluation_ of the tail of\nthe list.\n\nLazy lists have practical uses.  Some algorithms, like making change, can\nyield many solutions when only a few are required.  Sometimes the original\nproblem concerns infinite series: with lazy lists, we can pretend they really\nexist!\n\nWe are now dealing with _infinite_ (or at least unbounded) computations.\nA potentially infinite source of data is processed one element at a time, upon\ndemand.  Such programs are harder to understand than terminating ones and have\nmore ways of going wrong.\n\nSome purely functional languages, such as Haskell, use lazy evaluation\neverywhere.  Even the if-then-else construct can be a function, and all lists\nare lazy.  In ML, we can declare a type of lists such that evaluation of the\ntail does not occur until demanded.  _Delayed_ evaluation is weaker than\n_lazy_ evaluation, but it is good enough for our purposes and often the\nbest compromise for performance and memory usage.\n\nThe traditional word \"stream\" is reserved in ML parlance for\ninput/output channels.  Let us call lazy lists _sequences_ instead.\n"},{"cell_type":"markdown","metadata":{},"source":"### Lazy Lists in ML\n\n\n\n- The empty tuple `()` and its _type_ `unit`\n- Delayed version of $E$ is `fun () -> E`\n"},{"cell_type":"code","metadata":{},"source":"type 'a seq =\n  | Nil\n  | Cons of 'a * (unit -> 'a seq) ;;","outputs":[],"execution_count":146},{"cell_type":"code","metadata":{},"source":"let head (Cons (x,_)) = x","outputs":[],"execution_count":147},{"cell_type":"code","metadata":{},"source":"let tail (Cons (_,xf)) = xf ()","outputs":[],"execution_count":148},{"cell_type":"markdown","metadata":{},"source":"\n$\\texttt{Cons($x$,$xf$)}$ has _head_ $x$ and _tail function_ $xf$\n\nThe primitive ML type `unit` has one element, which is\nwritten `()`.  This element may be regarded as a 0-tuple, and\n`unit` as the nullary Cartesian product.  (Think of the connection\nbetween multiplication and the number 1.)\n\nThe empty tuple serves as a placeholder in situations where no information is\nrequired.  It may:\n- appear in a data structure.  For example, a `unit`-valued dictionary represents a set of keys.\n- be the argument of a function, where its effect is to \\emph{delay evaluation}.\n- be the argument or result of a procedure. (see the Procedural Programming section)\n\nThe empty tuple, like all tuples, is a constructor and is allowed in patterns:\n"},{"cell_type":"raw","metadata":{},"source":"let f () = ..."},{"cell_type":"markdown","metadata":{},"source":"\nIn particular $\\texttt{\\uline{fun}() => $E$}$ is the function that takes an argument of\ntype `unit` and returns the value of $E$ as its result.  Expression $E$\nis not evaluated until the function is called, even though the only possible\nargument is `()`.  The function simply delays the evaluation of $E$.\n"},{"cell_type":"markdown","metadata":{},"source":"### The Infinite Sequence: $k$, $k+1$, $k+2$, ...\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec from k = Cons (k, fun () -> from (k+1)) ;;","outputs":[],"execution_count":149},{"cell_type":"code","metadata":{},"source":"let it = from 1 ;;","outputs":[],"execution_count":150},{"cell_type":"code","metadata":{},"source":"let it = tail it ;;","outputs":[],"execution_count":151},{"cell_type":"code","metadata":{},"source":"let it = tail it ;;","outputs":[],"execution_count":152},{"cell_type":"markdown","metadata":{},"source":"\nFunction `from` constructs the infinite sequence of integers starting\nfrom $k$.  Execution terminates because of the `fun` enclosing the\nrecursive call.  OCaml displays the tail of a sequence as `fun`, which\nstands for some function value.  Each call to `tail` generates the next\nsequence element.  We could do this forever.\n\nThis example is of little practical value because the cost of computing a\nsequence element will be dominated by that of creating the dummy function.\nLazy lists tend to have high overheads.\n\n### Consuming a Sequence\n"},{"cell_type":"code","metadata":{},"source":"let rec get n s =\n  if n = 0 then\n    []\n  else\n    match s with\n    | Nil -> []\n    | Cons (x, xf) -> x :: get (n-1) (xf ())","outputs":[],"execution_count":153},{"cell_type":"markdown","metadata":{},"source":"\nThe above code gets the first $n$ elements as a list.\n`xf ()` _forces_ evaluation.\n\nThe function `get` converts a sequence to a list.  It takes the\nfirst $n$ elements; it takes all of them if $n<0$, which can terminate only if\nthe sequence is finite.\n\nIn the third line of `get`, the expression `xf()` calls the tail\nfunction, demanding evaluation of the next element.  This operation is called\n_forcing_ the list.\n"},{"cell_type":"markdown","metadata":{},"source":"### Sample Evaluation\n\n\n\n$$\n\\begin{alltt}\\small\nget(2, from 6)\n \\(\\) get(2, Cons(6, \\uline{fn}()=>from(6+1)))\n \\(\\) 6 :: get(1, from(6+1))\n \\(\\) 6 :: get(1, Cons(7, \\uline{fn}()=>from(7+1)))\n \\(\\) 6 :: 7 :: get(0, Cons(8, \\uline{fn}()=>from(8+1)))\n \\(\\) 6 :: 7 :: []\n \\(\\) [6,7]\n$$\n\nHere we ask for two elements of the infinite sequence.  In fact, three\nelements are computed: 6, 7 and 8.  Our implementation is slightly too eager.\nA more complicated `type` declaration could avoid this problem.\nAnother problem is that if one repeatedly examines some particular list\nelement using forcing, that element is repeatedly evaluated.  In a lazy\nprogramming language, the result of the first evaluation would be stored for\nlater reference.  To get the same effect in OCaml requires the use of\nreferences.\n\nWe should be grateful that the potentially infinite computation is kept\nfinite.  The tail of the original sequence even contains the unevaluated\nexpression 6+1.\n"},{"cell_type":"markdown","metadata":{},"source":"### Joining Two Sequences\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec appendq xq yq =\n  match xq with\n  | Nil -> r\n  | Cons (x,xf) ->\n      Cons(x, fun () -> appendq (xf (), yq))\n;;","outputs":[],"execution_count":154},{"cell_type":"markdown","metadata":{},"source":"\nA more fair alternative:\n"},{"cell_type":"code","metadata":{},"source":"let rec interleave xq yq =\n  match xq with\n  | Nil -> yq\n  | Cons (x,xf) ->\n      Cons (x, fun () -> interleave (yq, xf ()))\n;;","outputs":[],"execution_count":155},{"cell_type":"markdown","metadata":{},"source":"\nMost list functions and functionals have analogues on sequences, but strange\nthings can happen.  Can an infinite list be reversed?\n\nFunction `appendq` is precisely the same idea as `append`\nfrom the Lists lecture; it concatenates two sequences.  If the first\nargument is infinite, then `appendq` never gets to its second argument,\nwhich is lost.  Concatenation of infinite sequences is not terribly\ninteresting.\n\nThe function `interleave` avoids this problem by exchanging the two\narguments in each recursive call.  It combines the two lazy lists, losing no\nelements.  Interleaving is the right way to combine two potentially infinite\ninformation sources into one.\n\nIn both function declarations, observe that each `xf ()` is enclosed\nwithin a ${\\tt \\uline{fn}()=>\\ldots}$.  Each _force_ is enclosed within a\n_delay_.  This practice makes the functions lazy.  A force not enclosed\nin a delay, as in `get` above, runs the risk of evaluating the sequence\nin full.\n"},{"cell_type":"markdown","metadata":{},"source":"### Functionals for Lazy Lists\n\n\n\nFiltering lazy lists:\n"},{"cell_type":"code","metadata":{},"source":"let rec filterq p = function\n  | Nil -> Nil\n  | Cons (x,xf) ->\n     if p x then\n        Cons (x, fun () -> filterq p (xf ()))\n     else\n        filterq p (xf ())\n;;","outputs":[],"execution_count":156},{"cell_type":"markdown","metadata":{},"source":"\nThe infinite sequence $x$, $f(x)$, $f(f(x))$,...\n"},{"cell_type":"code","metadata":{},"source":"let rec iterates f x =\n  Cons (x, fun () -> iterates f (f x))","outputs":[],"execution_count":157},{"cell_type":"markdown","metadata":{},"source":"\nThe functional `filterq` demands elements of `xq` until it finds\none satisfying `p`.  (Recall `filter`, the analogous operation for ordinary lists.)  It\ncontains a _force_ not protected by a _delay_.  If `xq` is\ninfinite and contains no satisfactory element, then `filtering` runs\nforever.\n\nThe functional `iterates` generalizes `from`.  It creates the\nnext element not by adding one but by calling the function `f`.\n"},{"cell_type":"markdown","metadata":{},"source":"### Numerical Computations on Infinite Sequences\n\n\n"},{"cell_type":"code","metadata":{},"source":"let next a x = (a /. x +. x) /. 2.0 ","outputs":[],"execution_count":158},{"cell_type":"markdown","metadata":{},"source":"\nClose enough?\n"},{"cell_type":"code","metadata":{},"source":"let rec within eps = function\n  | Cons (x, xf) -> begin\n      match xf () with\n      | Cons (y, yf) ->\n          if abs_float (x -. y) <= eps then\n            y\n          else\n            within eps (Cons (y,yf))\n  end","outputs":[],"execution_count":159},{"cell_type":"markdown","metadata":{},"source":"\nSquare Roots:\n"},{"cell_type":"code","metadata":{},"source":"let root a = within 1e6 (iterates (next a) 1.0)","outputs":[],"execution_count":160},{"cell_type":"markdown","metadata":{},"source":"\nThe _Newton-Raphson method_ is widely used for computing square roots.\nThe infinite series $x_0$, $(a/x_0+x_0)/2$, \\ldots{} converges rapidly to $\\sqrt{a}$.\nThe initial approximation, $x_0$, is typically retrieved from a table, and is accurate enough\nthat only a few iterations of the method are necessary.\nCalling `iterates (next a) x0` generates the _infinite series_ of\napproximations to the square root of $a$ using the Newton-Raphson method.\nTo compute $\\sqrt2$, the resulting series begins 1, 1.5, 1.41667, 1.4142157, 1.414213562 ...,\nand this last figure is already accurate to 10 significant digits!\n\nFunction `within` searches down the lazy list for two points whose\ndifference is less than `eps`.  It tests their absolute difference.\nRelative difference and other \"close enough\" tests can be coded.  Such\ncomponents can be used to implement other numerical functions directly as\nfunctions over sequences.  The point is to build programs from small,\ninterchangeable parts.\n\nFunction `root` uses `within, `iterates` and `next` to\nto apply Newton-Raphson with a tolerance of $10^{-6}$\nand a (poor) initial approximation of 1.0.\n\nThis treatment of numerical computation has received some attention in the\nresearch literature; a recurring example is Richardson extrapolation.\n"},{"cell_type":"markdown","metadata":{},"source":"## Lecture 10: Queues and Search Strategies\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"### Breadth-First v Depth-First Tree Traversal\n\n\n\n- binary trees as _decision trees_\n- look for _solution nodes_\n  * Depth-first: search one subtree in full before moving on\n  * Breadth-first: search all nodes at level $k$ before moving to $k+1$\n- finds _all_ solutions --- nearest first!\n\nPreorder, inorder and postorder tree traversals all have something in common:\nthey are depth-first.  At each node, the left subtree is entirely\ntraversed before the right subtree.  Depth-first traversals are easy to code\nand can be efficient, but they are ill-suited for some problems.\n\nSuppose the tree represents the possible moves in a puzzle, and the purpose\nof the traversal is to search for a node containing a solution.  Then a\ndepth-first traversal may find one solution node deep in the left subtree,\nwhen another solution is at the very top of the right subtree.  Often we\nwant the shortest path to a solution.\n\nSuppose the tree is _infinite_, or simply extremely large.  Depth-first search\nis almost useless with such trees, for if the left subtree is infinite then the\nsearch will never reach the right subtree.  ML can represent infinite trees by\nthe means discussed in the lecture on laziness. Another tree representation (suitable\nfor solving solitaire, for example) is by a function `next : pos -> pos list`,\nwhich maps a board position to a list of the positions possible after\nthe next move.  For simplicity, the examples below use the ML datatype\n`tree`, which has only finite trees.\n\nA _breadth-first_ traversal explores the nodes horizontally rather than\nvertically.  When visiting a node, it does not traverse the subtrees until\nit has visited all other nodes at the current depth.  This is easily\nimplemented by keeping a list of trees to visit.  Initially, this list\nconsists of one element: the entire tree.  Each iteration removes a tree\nfrom the head of the list and adds its subtrees after the end of the\nlist.\n"},{"cell_type":"markdown","metadata":{},"source":"### Breadth-First Tree Traversal --- Using Append\n\n\n"},{"cell_type":"code","metadata":{},"source":"let rec nbreadth = function\n  | [] -> []\n  | Lf :: ts -> nbreadth ts\n  | Br (v,t,u) :: ts ->\n      v :: nbreadth (ts @ [t;u])","outputs":[],"execution_count":161},{"cell_type":"markdown","metadata":{},"source":"\nKeeps an _enormous queue_ of nodes of search, and is a wasteful use of `append`.\n\nBreadth-first search can be inefficient, this naive implementation especially\nso.  When the search is at depth $d$ of the tree, the list contains all the\nremaining trees at depth $d$, followed by the subtrees (all at depth $d+1$) of\nthe trees that have already been visited.  At depth 10, the list could already\ncontain 1024 elements.  It requires a lot of space, and aggravates this with a\ngross misuse of append.  Evaluating `ts@[t,u]` copies the long list\n`ts` just to insert two elements.\n"},{"cell_type":"markdown","metadata":{},"source":"### An Abstract Data Type: Queues\n\n\n\n- `qempty` is the _empty queue_\n- `qnull` _tests_ whether a queue is empty\n- `qhd` _returns_ the element at the _head_ of a queue\n- `deq` _discards_ the element at the _head_ of a queue\n- `enq` _adds_ an element at the _end_ of a queue\n\nBreadth-first search becomes much faster if we replace the lists by\n_queues_.  A queue represents a sequence, allowing elements to be taken\nfrom the head and added to the tail.  This is a First-In-First-Out (FIFO)\ndiscipline: the item next to be removed is the one that has been in the queue\nfor the longest time.  Lists can implement queues, but append is a poor means\nof adding elements to the tail.\n\nOur functional arrays are suitable, provided we\naugment them with a function to delete the first array element.  (See _ML\n  for the Working Programmer_ page 156.)  Each operation would take $O(\\log\nn)$ time for a queue of length $n$.\n\nWe shall describe a representation of queues that is purely functional, based\nupon lists, and efficient.  Operations take $O(1)$ time when \\emph{amortized}:\naveraged over the lifetime of a queue.\n\nA conventional programming technique is to represent a queue by an array.  Two\nindices point to the front and back of the queue, which may wrap around the\nend of the array.  The coding is somewhat tricky.  Worse, the length of the\nqueue must be given a fixed upper bound.\n"},{"cell_type":"markdown","metadata":{},"source":"### Efficient Functional Queues: Idea\n\n\n\n- Represent the queue $x_1\\; x_2\\; \\ldots\\; x_m\\; y_n\\; \\ldots\\; y_1$\nby any \\Emph{pair of lists}\n$$ ([x_1,x_2,\\ldots,x_m], \\; [y_1,y_2,\\ldots,y_n]) $$\n- Add new items to the _rear list_\n- Remove items from _front list_ and if empty move _rear_ to _front_\n- _Amortized_ time per operation is $O(1)$\n\nQueues require efficient access at both ends: at the front, for removal, and\nat the back, for insertion.  Ideally, access should take constant time,\n$O(1)$.  It may appear that lists cannot provide such access.  If\n`enq(q,x)` performs `q@[x]`, then this operation will be $O(n)$.  We\ncould represent queues by reversed lists, implementing `enq(q,x)` by\n`x::q`, but then the `deq` and `qhd` operations would be\n$O(n)$.  Linear time is intolerable: a series of $n$ queue operations\ncould then require $O(n^2)$ time.\n\nThe solution is to represent a queue by a pair of lists, where\n$$ ([x_1,x_2,\\ldots,x_m], \\, [y_1,y_2,\\ldots,y_n]) $$\nrepresents the queue $x_1 x_2 \\ldots x_m y_n \\ldots y_1$.\n\nThe front part of the queue is stored in order, and the rear part is stored in\nreverse order.  The `enq` operation adds elements to the rear part\nusing cons, since this list is reversed; thus, `enq` takes constant\ntime.  The `deq` and `qhd` operations look at the front part,\nwhich normally takes constant time, since this list is stored in order.  But\nsometimes `deq` removes the last element from the front part; when this\nhappens, it reverses the rear part, which becomes the new front part.\n\n_Amortized_ time refers to the cost per operation averaged over the\nlifetime of any complete execution.  Even for the worst possible execution,\nthe average cost per operation turns out to be constant; see the analysis\nbelow.\n"},{"cell_type":"markdown","metadata":{},"source":"### Efficient Functional Queues: Code\n\n\n"},{"cell_type":"code","metadata":{},"source":"type 'a queue =\n  | Q of 'a list * 'a list","outputs":[],"execution_count":162},{"cell_type":"code","metadata":{},"source":"let norm = function\n  | Q ([], tls) -> Q (rev tls, [])\n  | q -> q","outputs":[],"execution_count":163},{"cell_type":"code","metadata":{},"source":"let qnull = function\n  | Q ([],[]) -> true\n  | _ -> false","outputs":[],"execution_count":164},{"cell_type":"code","metadata":{},"source":"let enq q x =\n  match q with\n  | Q (hds, tls) -> norm (Q (hds, x::tls))","outputs":[],"execution_count":165},{"cell_type":"code","metadata":{},"source":"let deq = function\n  | Q (x::hds, tls) -> norm (Q (hds, tls))","outputs":[],"execution_count":166},{"cell_type":"code","metadata":{},"source":"let qempty = Q([],[])","outputs":[],"execution_count":167},{"cell_type":"code","metadata":{},"source":"let qhd = function\n  Q (x::_,_) -> x","outputs":[],"execution_count":168},{"cell_type":"markdown","metadata":{},"source":"\nThe datatype of queues prevents confusion with other pairs of lists.  The empty\nqueue has both parts empty.\n\nThe function `norm` puts a queue into normal form, ensuring that the front part\nis never empty unless the entire queue is empty.  Functions `deq` and `enq`\ncall `norm` to normalize their result.\n\nBecause queues are in normal form, their head is certain to be in their\nfront part, so `qhd` looks there.\n\nLet us analyse the cost of an execution comprising (in any possible order) $n$\n`enq` operations and $n$ `deq` operations, starting with an\nempty queue.  Each `enq` operation will perform one cons, adding an\nelement to the rear part.  Since the final queue must be empty, each element\nof the rear part gets transferred to the front part.  The corresponding\nreversals perform one cons per element.  Thus, the total cost of the series of\nqueue operations is $2n$ cons operations, an average of 2 per operation.  The\namortized time is $O(1)$.\n\nThere is a catch.  The conses need not be distributed evenly; reversing a long\nlist could take up to $n-1$ of them.  Unpredictable delays make the approach\nunsuitable for _real-time programming_ where deadlines must be met.\n"}]}